{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 21, 22, 23, 24, 25, 26, 50]\n"
     ]
    }
   ],
   "source": [
    "all_file='Input_data/Proteomic data/abundance/Intensity _all20230202.xlsx'\n",
    "raw_MS_data=pd.read_excel(all_file,header=0)\n",
    "col_length=raw_MS_data.shape[0]\n",
    "to_drop=[]\n",
    "for i in raw_MS_data.columns:\n",
    "    column=raw_MS_data[i]\n",
    "    zeros=(column==0).sum()\n",
    "    # print(zeros)\n",
    "    # print(zeros)\n",
    "    # print(raw_MS_data[i])\n",
    "    if zeros>int(col_length/2):\n",
    "        to_drop.append(i)\n",
    "        # print(raw_MS_data.shape[1])\n",
    "# print(raw_MS_data.shape[1])\n",
    "print(to_drop)\n",
    "raw_MS_data.drop(columns=to_drop,inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouls\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py:397: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Pull together Proteomic data\n",
    "in_dir=\"Input_data/Proteomic data/abundance/\"\n",
    "all_file='Input_data/Proteomic data/abundance/Intensity _all20230202.xlsx'\n",
    "#combine Mass Spec data input into one excel spreadsheet - Entry - Abundance labeled by NP Unique ID\n",
    "#Abundance as a percent\n",
    "#take files in_dir and combine then into one pandas df (raw_MS_data)\n",
    "# files = os.listdir(in_dir)\n",
    "# for i,f in enumerate(files):\n",
    "#     if i==0:\n",
    "#         raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "#     else:\n",
    "#         temp = pd.read_excel(in_dir+f,header=0)\n",
    "#         raw_MS_data=raw_MS_data.merge(temp,how='outer',on='Entry')\n",
    "raw_MS_data=pd.read_excel(all_file,header=0)\n",
    "#drop samples that are missing more than half of the proteins in their file\n",
    "col_length=raw_MS_data.shape[0]\n",
    "to_drop=[]\n",
    "for i in raw_MS_data.columns:\n",
    "    column=raw_MS_data[i]\n",
    "    zeros=(column==0).sum()\n",
    "    if zeros>int(col_length/2):\n",
    "        to_drop.append(i)\n",
    "raw_MS_data.drop(columns=to_drop,inplace=True)\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset\n",
    "raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "#remove prots that were added due to merge\n",
    "raw_MS_data=raw_MS_data.dropna()\n",
    "###Bring in controls (MS data for serums)##\n",
    "controls=pd.read_excel('Input_data/Proteomic data/controls_combined.xlsx',header=0)\n",
    "MS_data_controls = pd.merge(raw_MS_data,controls,how='left', on='Entry')\n",
    "###Bring in Uniprot_data,NSPdata and NP data##\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "uniprot_dat=pd.read_excel(uniprot_filepath,header=0)\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "NSP_data=pd.read_excel(NSPfilePath)\n",
    "###Bring in NP data and merge to get complete NP dataset###\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "NPUNdata=pd.read_excel(NP_filepath,header=0,sheet_name='NPUNID')\n",
    "NPprop=pd.read_excel(NP_filepath,header=0,sheet_name='NP_Props')\n",
    "NPdata=pd.merge(NPUNdata,NPprop,how=\"left\",on='NPID')\n",
    "NPdata.dropna(inplace=True)\n",
    "#calculate Enrichment\n",
    "#####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "#keep abundance Controls\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance'])\n",
    "raw_prop_data=pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left',on='Entry')\n",
    "Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']),how='left', on='Entry') #merges netsurfp features and biopython features\n",
    "Protein_data_complete.fillna(0,inplace=True)\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n",
    "\n",
    "data_complete= pd.merge(Protein_data_complete,NPdata,how='inner', on='Sample_num')\n",
    "data_complete.drop(columns=['notes','Notes','NPUNID'],inplace=True)\n",
    "data_complete.fillna(0,inplace=True)\n",
    "data_complete= data_complete.replace([-np.inf],'-12')\n",
    "data_complete=data_complete.replace([np.inf],'12')\n",
    "#create ordinal variables\n",
    "# data_complete2=pd.get_dummies(data_complete, columns=['Core Material','Surface_Ligand'])\n",
    "le=LabelEncoder()\n",
    "data_complete['Core Material']=le.fit_transform(data_complete['Core Material'])\n",
    "data_complete['Surface_Ligand']=le.fit_transform(data_complete['Surface_Ligand'])\n",
    "\n",
    "#set labels (what we are trying to predict) as Enrichment column\n",
    "# labels=data_complete['Enrichment'].copy()\n",
    "label_abund=np.ravel(data_complete['Abundance'].copy())\n",
    "label_enrich=np.ravel(data_complete['Enrichment'].copy())\n",
    "#make it one dimenisional\n",
    "#drop qualitative, not neccessary, and label columns\n",
    "#create df without bonus NSP columns (remove total_exposed)\n",
    "to_drop=data_complete.filter(like='total_exposed_')\n",
    "data_complete.drop(columns=to_drop,inplace=True)\n",
    "df=data_complete.drop(['Entry','Abundance','Sequence','NPID','Enrichment','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n",
    "# df_enrich=data_complete.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "#create normalized dataframe\n",
    "# df_norm=df.copy()\n",
    "# for col in df_norm.columns:\n",
    "#     if col == 'asa_sum_normalized':\n",
    "#         continue\n",
    "#     df_norm[col]=df_norm[col]/df_norm[col].max()\n",
    "#     print(col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df_rfe=feat_elim_rand(df,label_abund,'abund_15_Feats',15,150,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Abundance_Controls', 'Length', 'secondary_structure_fraction_sheet',\n       'fraction_exposed_exposed_G', 'fraction_exposed_exposed_H',\n       'fraction_exposed_exposed_I', 'fraction_exposed_exposed_P',\n       'nsp_secondary_structure_helix', 'BatchID', 'Zeta Potential',\n       'Surface_Ligand', 'Dh_core', 'Dh_functionalized',\n       'NP_incubation Concentration (mg/mL)',\n       'Incubation Concentration (mg/ml)'],\n      dtype='object')"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfe.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "# df_NSP_drop2=feat_elim_rand(df_NSP_drop,labels,'nspdrop',15,250,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "df_list=[df,df]\n",
    "labels=[label_enrich,label_abund]\n",
    "out_name=['enrich','abund']\n",
    "df_listnew=[]\n",
    "x=0\n",
    "for i in df_list:\n",
    "    df_listnew.append(feat_elim_rand(i,labels[x],out_name[x],15,150,3))\n",
    "    x+=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "df_enrich=df_listnew[0]\n",
    "df_abund=df_listnew[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Abundance_Controls', 'Length', 'frac_aa_N',\n       'secondary_structure_fraction_sheet', 'fraction_exposed_exposed_G',\n       'fraction_exposed_exposed_H', 'fraction_exposed_exposed_I',\n       'fraction_exposed_exposed_P', 'BatchID', 'Zeta Potential',\n       'Surface_Ligand', 'Dh_core', 'Dh_functionalized',\n       'NP_incubation Concentration (mg/mL)',\n       'Incubation Concentration (mg/ml)'],\n      dtype='object')"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_16344\\3070586185.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_abund['label_abund']=label_abund\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_16344\\3070586185.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_enrich['label_enrich']=label_enrich\n"
     ]
    }
   ],
   "source": [
    "df_abund['label_abund']=label_abund\n",
    "df_enrich['label_enrich']=label_enrich\n",
    "df_abund.to_excel(\"Output_data/df_abund.xlsx\")\n",
    "df_enrich.to_excel(\"Output_data/df_enrich.xlsx\")\n",
    "# df_listnew[2].to_excel(\"Output_data/df_nspdrop.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_16344\\1477147535.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_abund.drop(columns=['label_abund'],inplace=True)\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_16344\\1477147535.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_enrich.drop(columns=['label_enrich'],inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_abund.drop(columns=['label_abund'],inplace=True)\n",
    "df_enrich.drop(columns=['label_enrich'],inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "# for i in df_listnew:\n",
    "#     i.drop(columns=['labels'],inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enrich.isna().sum().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'Output_data/estimators_score_abund.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Input \u001B[1;32mIn [167]\u001B[0m, in \u001B[0;36m<cell line: 16>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     14\u001B[0m     scores\u001B[38;5;241m.\u001B[39mappend(score)\n\u001B[0;32m     15\u001B[0m a\u001B[38;5;241m=\u001B[39mpd\u001B[38;5;241m.\u001B[39mDataFrame(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(estimators,scores)), columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumber of estimators\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 16\u001B[0m \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mOutput_data/estimators_score_abund.xlsx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2345\u001B[0m, in \u001B[0;36mNDFrame.to_excel\u001B[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001B[0m\n\u001B[0;32m   2332\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mformats\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexcel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExcelFormatter\n\u001B[0;32m   2334\u001B[0m formatter \u001B[38;5;241m=\u001B[39m ExcelFormatter(\n\u001B[0;32m   2335\u001B[0m     df,\n\u001B[0;32m   2336\u001B[0m     na_rep\u001B[38;5;241m=\u001B[39mna_rep,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2343\u001B[0m     inf_rep\u001B[38;5;241m=\u001B[39minf_rep,\n\u001B[0;32m   2344\u001B[0m )\n\u001B[1;32m-> 2345\u001B[0m \u001B[43mformatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexcel_writer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2347\u001B[0m \u001B[43m    \u001B[49m\u001B[43msheet_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msheet_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstartrow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstartrow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstartcol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstartcol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfreeze_panes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfreeze_panes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2352\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2353\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py:888\u001B[0m, in \u001B[0;36mExcelFormatter.write\u001B[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001B[0m\n\u001B[0;32m    884\u001B[0m     need_save \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    885\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    886\u001B[0m     \u001B[38;5;66;03m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001B[39;00m\n\u001B[0;32m    887\u001B[0m     \u001B[38;5;66;03m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001B[39;00m\n\u001B[1;32m--> 888\u001B[0m     writer \u001B[38;5;241m=\u001B[39m \u001B[43mExcelWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[abstract]\u001B[39;49;00m\n\u001B[0;32m    889\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    891\u001B[0m     need_save \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    893\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py:191\u001B[0m, in \u001B[0;36mXlsxWriter.__init__\u001B[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAppend mode is not supported with xlsxwriter!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatetime_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatetime_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mif_sheet_exists\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mif_sheet_exists\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbook \u001B[38;5;241m=\u001B[39m Workbook(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mengine_kwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1106\u001B[0m, in \u001B[0;36mExcelWriter.__init__\u001B[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1102\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m IOHandles(\n\u001B[0;32m   1103\u001B[0m     cast(IO[\u001B[38;5;28mbytes\u001B[39m], path), compression\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mNone\u001B[39;00m}\n\u001B[0;32m   1104\u001B[0m )\n\u001B[0;32m   1105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, ExcelWriter):\n\u001B[1;32m-> 1106\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1107\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m   1108\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1109\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msheets: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m   1110\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcur_sheet \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:798\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    789\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    790\u001B[0m             handle,\n\u001B[0;32m    791\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    794\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    795\u001B[0m         )\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    797\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m--> 798\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[0;32m    801\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: 'Output_data/estimators_score_abund.xlsx'"
     ]
    }
   ],
   "source": [
    "#test estimator to see what is the ideal number of estimators\n",
    "estimators=np.arange(5,500,5)\n",
    "out_name=np.arange(5,500,5)\n",
    "# print(out_name)\n",
    "scores=[]\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_abund, label_abund,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "for i in range(len(estimators)):\n",
    "    rfg = RandomForestRegressor(n_estimators=estimators[i])\n",
    "    rfg.fit(x_train, y_train)\n",
    "    score=rfg.score(x_test,y_test)\n",
    "    scores.append(score)\n",
    "a=pd.DataFrame(list(zip(estimators,scores)), columns=['number of estimators','accuracy'])\n",
    "a.to_excel(\"Output_data/estimators_score_abund.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(estimators,scores)), columns=['number of estimators','accuracy'])\n",
    "a.to_excel(\"Output_data/estimators_score_abund.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "#test estimator to see what is the ideal number of estimators\n",
    "estimators=np.arange(5,500,5)\n",
    "out_name=np.arange(5,500,5)\n",
    "# print(out_name)\n",
    "scores=[]\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_enrich, label_enrich,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "for i in range(len(estimators)):\n",
    "    rfg = RandomForestRegressor(n_estimators=estimators[i])\n",
    "    rfg.fit(x_train, y_train)\n",
    "    score=rfg.score(x_test,y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "a=pd.DataFrame(list(zip(estimators,scores)), columns=['number of estimators','accuracy'])\n",
    "a.to_excel(\"Output_data/estimators_score_enrich.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot distrubition of (feat) with -labels and dsitrubition with positive values\n",
    "# frac_aa_E,K,R, flexibility_min, Isoelectric Point, gravy, fraction_exposed_L,M,R,S,T,Y, zeta potential\n",
    "#plot PCA of features and labels after RFE\n",
    "#plot loss of features after RFE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "#look at loss as a function of feature\n",
    "feats=[]\n",
    "scores=[]\n",
    "df=df_abund\n",
    "id='abund3'\n",
    "label=label_abund\n",
    "# frames=['df','df_norm','df_NSP_drop']\n",
    "# for x,i in enumerate(df_listnew):\\\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df,label, test_size = 0.2, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=150)\n",
    "rfg.fit(x_train,y_train)\n",
    "topscore=rfg.score(x_test,y_test)\n",
    "# scores.append(topscore)\n",
    "# feats.append('nsp_drop')\n",
    "for j in x_test.columns:\n",
    "    tmp=x_test.copy()\n",
    "    # print(x_test)\n",
    "    np.random.shuffle(tmp[j].values)\n",
    "    scram_score=(topscore-rfg.score(tmp,y_test))/topscore*100\n",
    "    scores.append(scram_score)\n",
    "    feats.append(j)\n",
    "importances=rfg.feature_importances_*100\n",
    "a=pd.DataFrame(list(zip(feats,scores,importances)))\n",
    "a.to_excel(\"Output_data/loss_feats\"+id+\".xlsx\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "df=df_enrich\n",
    "id='enrich3'\n",
    "label=label_enrich\n",
    "# frames=['df','df_norm','df_NSP_drop']\n",
    "# for x,i in enumerate(df_listnew):\\\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df,label, test_size = 0.2, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=150)\n",
    "rfg.fit(x_train,y_train)\n",
    "# topscore=rfg.score(x_test,y_test)\n",
    "pred=rfg.predict(x_test)\n",
    "a=pd.DataFrame(list(zip(pred,y_test)),columns=['prediction','true'])\n",
    "a.to_excel(\"Output_data/prediction\"+id+\".xlsx\")\n",
    "# scores.append(topscore)\n",
    "# feats.append('nsp_drop')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Abundance_Controls', 53.80344715927424), ('Length', 3.183420512933438), ('secondary_structure_fraction_sheet', 2.0102238907980783), ('fraction_exposed_exposed_G', 3.7753666395000733), ('fraction_exposed_exposed_H', 2.3025589348193085), ('fraction_exposed_exposed_I', 2.0800589466395443), ('fraction_exposed_exposed_P', 2.9562933014048007), ('nsp_secondary_structure_helix', 1.9133308188771145), ('BatchID', 4.3316804352935785), ('Zeta Potential', 2.7404038499722785), ('Surface_Ligand', 2.2766756095249816), ('Dh_core', 2.8464161071617444), ('Dh_functionalized', 4.530138612141545), ('NP_incubation Concentration (mg/mL)', 5.028934254554429), ('Incubation Concentration (mg/ml)', 6.221050927104836)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(df_NSP_drop2.columns,rfg.feature_importances_*100)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "scores=[]\n",
    "importances=[]\n",
    "df_play=df_listnew[2].copy()\n",
    "to_drop=['fraction_exposed_exposed_K','flexibility_min','nsp_secondary_structure_coil','fraction_exposed_exposed_D','Dh_core','frac_aa_F','frac_aa_K','Dh_functionalized','gravy','fraction_exposed_exposed_M','nsp_secondary_structure_helix','frac_aa_E','fraction_exposed_exposed_S','Incubation Concentration (mg/ml)','fraction_exposed_exposed_L','fraction_exposed_exposed_Y','isoelectric_point','fraction_exposed_exposed_R','frac_aa_R','Zeta Potential']\n",
    "# df_play\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7129708000680841\n",
      "[0.04397567 0.03622563 0.05661056 0.07044834 0.03762556 0.05956122\n",
      " 0.04035952 0.03808894 0.06851993 0.05179837 0.04851901 0.05062163\n",
      " 0.05464555 0.04126761 0.04355335 0.12353352 0.03962461 0.04932202\n",
      " 0.04569895]\n",
      "0.7174539961269533\n",
      "[0.0459669  0.03358728 0.06313426 0.07116079 0.05764382 0.04659791\n",
      " 0.03768361 0.07137779 0.0554696  0.0533693  0.05311723 0.06129922\n",
      " 0.04356481 0.04642746 0.12628031 0.03959313 0.04765988 0.04606672]\n",
      "0.715277745807563\n",
      "[0.05106954 0.04226904 0.06596607 0.06933313 0.06505809 0.04874418\n",
      " 0.04233961 0.07155951 0.05623948 0.05777363 0.05315133 0.06566498\n",
      " 0.05070178 0.1259717  0.03876504 0.0495918  0.04580106]\n",
      "0.7109222275663655\n",
      "[0.05041272 0.04512989 0.06580425 0.07052882 0.07218905 0.05360005\n",
      " 0.07587482 0.05619295 0.06206785 0.06113592 0.070383   0.05492594\n",
      " 0.12765857 0.03762476 0.05091379 0.0455576 ]\n",
      "0.7078284536314533\n",
      "[0.05461364 0.04682368 0.07049538 0.07341076 0.06403836 0.05497546\n",
      " 0.07713649 0.05486788 0.05952191 0.06358799 0.06739948 0.05071482\n",
      " 0.1438323  0.07190185 0.04668001]\n",
      "0.7038909859920557\n",
      "[0.05086785 0.06888459 0.081371   0.07357788 0.06884502 0.08240534\n",
      " 0.06160218 0.05918136 0.0659603  0.0725649  0.05609348 0.14111325\n",
      " 0.07131507 0.04621776]\n",
      "0.7030960553201626\n",
      "[0.05350617 0.09551209 0.09066354 0.07782851 0.09634206 0.07052944\n",
      " 0.05523274 0.06913252 0.07011627 0.06385595 0.14120249 0.06939326\n",
      " 0.04668497]\n",
      "0.6958094607887657\n",
      "[0.05401199 0.09808522 0.09459634 0.07500312 0.09717117 0.069202\n",
      " 0.05763986 0.06793469 0.07362916 0.06455049 0.20240229 0.04577366]\n",
      "0.6950809578307522\n",
      "[0.07017729 0.10337894 0.08845629 0.10928879 0.08064683 0.06777003\n",
      " 0.07687283 0.0851568  0.06867491 0.20242043 0.04715686]\n",
      "0.6965524975913653\n",
      "[0.08749065 0.1141259  0.10481935 0.11983288 0.0680614  0.08756788\n",
      " 0.0914784  0.07632527 0.20414696 0.04615131]\n",
      "0.6933182405728687\n",
      "[0.09959911 0.12187436 0.10688446 0.13980778 0.07907439 0.09486511\n",
      " 0.10485669 0.20568724 0.04735085]\n",
      "0.6953862352002065\n",
      "[0.12978796 0.14043647 0.15842183 0.09419318 0.11046814 0.11360769\n",
      " 0.20558654 0.04749819]\n",
      "0.6879254874854495\n",
      "[0.14330172 0.16798252 0.18875398 0.12274379 0.12804157 0.2017608\n",
      " 0.04741562]\n",
      "0.678404189975427\n",
      "[0.14916498 0.17984696 0.19813589 0.12271948 0.13351444 0.21661824]\n",
      "0.6731528137300304\n",
      "[0.19702274 0.24187312 0.16344931 0.17400651 0.22364833]\n",
      "0.6657406679496114\n",
      "[0.24394472 0.31838629 0.22684953 0.21081945]\n",
      "0.6639873376528889\n",
      "[0.37363269 0.4287578  0.1976095 ]\n",
      "0.630992869278167\n",
      "[0.80942932 0.19057068]\n",
      "0.07267494631926397\n",
      "[1.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [79]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m x_train, x_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(df_play,labels, test_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m     11\u001B[0m rfg\u001B[38;5;241m=\u001B[39mRandomForestRegressor(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m150\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m \u001B[43mrfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m topscore\u001B[38;5;241m=\u001B[39mrfg\u001B[38;5;241m.\u001B[39mscore(x_test,y_test)\n\u001B[0;32m     14\u001B[0m feat_imps\u001B[38;5;241m=\u001B[39mrfg\u001B[38;5;241m.\u001B[39mfeature_importances_\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:331\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m issparse(y):\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse multilabel-indicator for y is not supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 331\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDTYPE\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    335\u001B[0m     sample_weight \u001B[38;5;241m=\u001B[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:596\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    594\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 596\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    597\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    599\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1074\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1069\u001B[0m         estimator_name \u001B[38;5;241m=\u001B[39m _check_estimator_name(estimator)\n\u001B[0;32m   1070\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1071\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1072\u001B[0m     )\n\u001B[1;32m-> 1074\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1075\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1076\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1077\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_large_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1078\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1080\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1081\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1082\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1083\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_nd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1084\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_min_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1085\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_min_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_min_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1086\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1087\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1088\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1090\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[0;32m   1092\u001B[0m check_consistent_length(X, y)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:768\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    764\u001B[0m     pandas_requires_conversion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28many\u001B[39m(\n\u001B[0;32m    765\u001B[0m         _pandas_dtype_needs_early_conversion(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m dtypes_orig\n\u001B[0;32m    766\u001B[0m     )\n\u001B[0;32m    767\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(dtype_iter, np\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;28;01mfor\u001B[39;00m dtype_iter \u001B[38;5;129;01min\u001B[39;00m dtypes_orig):\n\u001B[1;32m--> 768\u001B[0m         dtype_orig \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult_type\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdtypes_orig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    770\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype_numeric:\n\u001B[0;32m    771\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m dtype_orig\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    772\u001B[0m         \u001B[38;5;66;03m# if input is object, convert to float.\u001B[39;00m\n",
      "File \u001B[1;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36mresult_type\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "##Fit and predict after dropping the next least loss feature\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_play,labels, test_size = 0.2, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=150)\n",
    "rfg.fit(x_train,y_train)\n",
    "topscore=rfg.score(x_test,y_test)\n",
    "feat_imps=rfg.feature_importances_\n",
    "scores.append(topscore)\n",
    "importances.append(feat_imps)\n",
    "for i in to_drop:\n",
    "    df_play.drop(columns=[i],inplace=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_play,labels, test_size = 0.2, random_state=42)\n",
    "    rfg=RandomForestRegressor(n_estimators=150)\n",
    "    rfg.fit(x_train,y_train)\n",
    "    topscore=rfg.score(x_test,y_test)\n",
    "    feat_imps=rfg.feature_importances_\n",
    "    col_feats=list(zip(df_play.columns,feat_imps))\n",
    "    scores.append(topscore)\n",
    "    importances.append(col_feats)\n",
    "    print(topscore)\n",
    "    print(feat_imps)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(importances)\n",
    "a.to_excel(\"Output_data/feats_afterdrop.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "\n",
    "a=pd.DataFrame(scores)\n",
    "a.to_excel(\"Output_data/scores_afterdrop.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(feats,scores)))\n",
    "a.to_excel(\"Output_data/loss_feats.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
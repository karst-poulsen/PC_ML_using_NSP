{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Load all of the data from three independent sources into a dataframe (data_complete)\n",
    "\n",
    "# Pull together Proteomic data\n",
    "in_dir=\"Input_data/Proteomic data/abundance/\"\n",
    "#Mass Spec data input in one excel spreadsheet - Entry - Abundance labeled by NP Unique ID\n",
    "#Abundance as a percent (if not in percent could add normalization function to normalize each column prior to the melt.)\n",
    "files= os.listdir(in_dir)\n",
    "for i,f in enumerate(files):\n",
    "    if i==0:\n",
    "        raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "    else:\n",
    "        temp = pd.read_excel(in_dir+f,header=0)\n",
    "        raw_MS_data=raw_MS_data.merge(temp,how='outer',on='Entry')\n",
    "# melt to make it an accession number, NPID, Abundance dataset\n",
    "raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='NPUNID', value_name='Abundance')\n",
    "#remove prots that were added due to merge\n",
    "raw_MS_data=raw_MS_data.dropna()\n",
    "#Load controls (MS data for serums)\n",
    "controls=pd.read_excel('Input_data/Proteomic data/controls.xlsx',header=0)\n",
    "MS_data_controls = pd.merge(raw_MS_data,controls,how='inner', on='Entry')\n",
    "#Load Uniprot_data,NSPdata and NP data (these will be your X features), Uniprot and NSPdata should be static libraries until I add more proteomes.\n",
    "uniprot_filepath='UniProt/Bovine_Mouse_proteome_biopyCalcs.xlsx'\n",
    "uniprot_dat=pd.read_excel(uniprot_filepath,header=0)\n",
    "\n",
    "NSPfilePath='NetsurfP_Proteomes/Bovine_Mouse_Proteome_complete.xlsx'\n",
    "NSP_data=pd.read_excel(NSPfilePath)\n",
    "\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "NPUNdata=pd.read_excel(NP_filepath,header=0,sheet_name='NPUNID')\n",
    "NPprop=pd.read_excel(NP_filepath,header=0,sheet_name='NP_Props')\n",
    "#Merge NP data to take Identifiers and then their independent surface features or experimental characteristics\n",
    "NPdata=pd.merge(NPUNdata,NPprop,how=\"left\",on='NPID')\n",
    "\n",
    "#calculate Enrichment\n",
    "#####MAYBE add binning here to to keep negative results and improve capapbilities######\n",
    "MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "#Two potential labels are Enrichment or Abundance, but planning to just use Enrichment so drop the abundances\n",
    "MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "#Merge with uniprot database - Entry is accession number which is a unique protein Identifier, this will be seen frequently\n",
    "raw_prop_data=pd.merge(MS_data, uniprot_dat, how='left',on='Entry')\n",
    "#function found in data_prep_functions line 167, normalizes mass, length and mw by dividing all values by the max in the column\n",
    "PROT_cleaned_data = normalize_mass_length_1DF(raw_prop_data)\n",
    "Protein_data_complete = pd.merge(PROT_cleaned_data, NSP_data, left_on='Entry', right_on='Entry') #merges netsurfp features and biopython features\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein this could probably be left out......\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n",
    "#Merge with NP data\n",
    "data_complete= pd.merge(Protein_data_complete,NPdata,how='left', on='NPUNID')\n",
    "data_complete.drop(columns=['notes','Notes','BET','NPUNID'],inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Actual modeling pipe goes here####\n",
    "labels=MS_data.drop(['Entry'], axis=1)\n",
    "labels=np.ravel(labels)\n",
    "df=data_complete.drop(['Entry','Sequence'],axis=1)\n",
    "\n",
    "print(labels)\n",
    "# print(labels)\n",
    "# print(df)\n",
    "first_frame = True #starting dataframe for saving metrics\n",
    "correctness_frame = pd.DataFrame()\n",
    "metrics_frame = pd.DataFrame()\n",
    "print_metrics = 1 #0, doesn't show metrics while runnning for each model, 1 does show metrics\n",
    "trials = 100\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df,labels, test_size = 0.33, random_state=42)\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=2016)\n",
    "# for train_index, test_index in sss.split(df, labels):\n",
    "# x_train = df.iloc[train_index]\n",
    "# X_test = df.iloc[test_index]\n",
    "# y_train = labels.iloc[train_index]\n",
    "# y_test = labels.iloc[test_index]\n",
    "\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "print(rfg.feature_importances_)\n",
    "# metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfg.predict_proba(x_test)[:, 1]),\n",
    "#             'Accuracy':rfg.score(x_test, y_test), 'Recall':recall_score(y_test, rfg.predict(x_test)),\n",
    "#             'Precision':precision_score(y_test, rfg.predict(x_test), zero_division=0), 'F1':f1_score(y_test, rfg.predict(x_test))}\n",
    "# metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
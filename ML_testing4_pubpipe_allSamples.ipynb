{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NormBALFsamples.xlsx', 'Norm_Intensity _all20230403.xlsx']\n",
      "0\n",
      "BALF (474, 14)\n",
      "1\n",
      "1\n",
      "Bovine (568, 56)\n",
      "merge (1042, 69)\n"
     ]
    }
   ],
   "source": [
    "#Editable Variables\n",
    "multi_files=True #set to false if you just want to set one  prot_abund_file\n",
    "in_dir=\"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file='Input_data/Proteomic data/abundance/Intensity _all20230202.xlsx'\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file='Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "# take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "files = os.listdir(in_dir)\n",
    "print(files)\n",
    "if multi_files == True:\n",
    "    for i,f in enumerate(files):\n",
    "        print(i)\n",
    "        if i==0:\n",
    "            raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "            # print(raw_MS_data)\n",
    "            print('BALF',raw_MS_data.shape)\n",
    "        else:\n",
    "            print(i)\n",
    "            temp = pd.read_excel(in_dir+f,header=0)\n",
    "            print('Bovine',temp.shape)\n",
    "            # print(temp)\n",
    "            # print(temp)\n",
    "            raw_MS_data2=raw_MS_data.merge(temp,how='outer',on=['Entry'])\n",
    "            print('merge',raw_MS_data2.shape)\n",
    "            # print('did it')\n",
    "else:\n",
    "    raw_MS_data2=pd.read_excel(prot_abund_file,header=0)\n",
    "# raw_MS_data2\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset\n",
    "# raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "(1042, 69)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_MS_data2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "multi_files=True #set to false if you just want to set one  prot_abund_file\n",
    "in_dir=\"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file='Input_data/Proteomic data/abundance/Intensity _all20230202.xlsx'\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file='Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "id='allnps'\n",
    "# take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "files = os.listdir(in_dir)\n",
    "if multi_files == True:\n",
    "    for i,f in enumerate(files):\n",
    "        if i==0:\n",
    "            raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "        else:\n",
    "            temp = pd.read_excel(in_dir+f,header=0)\n",
    "            raw_MS_data=raw_MS_data.merge(temp,how='outer',on='Entry')\n",
    "else:\n",
    "    raw_MS_data=pd.read_excel(prot_abund_file,header=0)\n",
    "\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset\n",
    "raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "#remove prots that were added due to merge\n",
    "raw_MS_data=raw_MS_data.dropna()\n",
    "###Bring in controls (MS data for serums)##\n",
    "controls=pd.read_excel(controls_file,header=0)\n",
    "MS_data_controls = pd.merge(raw_MS_data,controls,how='left', on='Entry')\n",
    "###Bring in Uniprot_data,NSPdata and NP data##\n",
    "uniprot_dat=pd.read_excel(uniprot_filepath,header=0)\n",
    "NSP_data=pd.read_excel(NSPfilePath)\n",
    "###Bring in NP data and merge to get complete NP dataset###\n",
    "NPUNdata=pd.read_excel(NP_filepath,header=0,sheet_name='NPUNID')\n",
    "NPprop=pd.read_excel(NP_filepath,header=0,sheet_name='NP_Props')\n",
    "NPdata=pd.merge(NPUNdata,NPprop,how=\"left\",on='NPID')\n",
    "NPdata.dropna(inplace=True)\n",
    "#calculate Enrichment\n",
    "#####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "# MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "#keep abundance Controls\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance'])\n",
    "raw_prop_data=pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left',on='Entry')\n",
    "Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']),how='left', on='Entry') #merges netsurfp features and biopython features\n",
    "Protein_data_complete.fillna(0,inplace=True)\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n",
    "\n",
    "data_complete= pd.merge(Protein_data_complete,NPdata,how='inner', on='Sample_num')\n",
    "data_complete.drop(columns=['notes','Notes','NPUNID'],inplace=True)\n",
    "data_complete.fillna(0,inplace=True)\n",
    "data_complete= data_complete.replace([-np.inf],'-12')\n",
    "data_complete=data_complete.replace([np.inf],'12')\n",
    "#create ordinal variables\n",
    "# data_complete2=pd.get_dummies(data_complete, columns=['Core Material','Surface_Ligand'])\n",
    "le=LabelEncoder()\n",
    "data_complete['Core Material']=le.fit_transform(data_complete['Core Material'])\n",
    "data_complete['Surface_Ligand']=le.fit_transform(data_complete['Surface_Ligand'])\n",
    "\n",
    "#set labels (what we are trying to predict) as Enrichment column\n",
    "# labels=data_complete['Enrichment'].copy()\n",
    "label_abund=np.ravel(data_complete['Abundance'].copy())\n",
    "label_abund_df=pd.DataFrame(label_abund)\n",
    "# label_enrich=np.ravel(data_complete['Enrichment'].copy())\n",
    "#make it one dimenisional\n",
    "#drop qualitative, not neccessary, and label columns\n",
    "#create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "to_drop=data_complete.filter(like='total_exposed_')\n",
    "data_complete.drop(columns=to_drop,inplace=True)\n",
    "df=data_complete.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n",
    "# df_enrich=data_complete.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n",
    "df.to_excel(\"Input_data/Save_files/df_\"+id+\".xlsx\")\n",
    "label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "# label_enrich.to_excel(\"Input_data/Save_files/label_enrich_synth.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7562,)\n",
      "(7562, 9)\n"
     ]
    }
   ],
   "source": [
    "df_2=pd.read_excel(\"Input_data/Save_files/df_synth_RFECV\"+id+\".xlsx\")\n",
    "df_2.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "label_df=pd.read_excel(\"Input_data/Save_files/label_abund_\"+id+\".xlsx\")\n",
    "label_abund=np.ravel(label_df[0])\n",
    "print(label_abund.shape)\n",
    "print(df_2.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run PCA to seee how data differentiates#\n",
    "from sklearn.decomposition import PCA\n",
    "pca= PCA(n_components=2)\n",
    "x_pca=pca.fit_transform(df)\n",
    "\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c=label_abund, cmap='viridis')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.savefig('Output_data/PCA'+id+'.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Remove correlated features (over r2 threshold level) and output reduced dataframe (df2)# ##Maybe use in future##\n",
    "# corr_matrix = df.corr()\n",
    "# threshold = 0.8\n",
    "# correlated_features = set()\n",
    "#\n",
    "# for i in range(len(corr_matrix.columns)):\n",
    "#     for j in range(i):\n",
    "#         if threshold < abs(corr_matrix.iloc[i, j]) < 1:\n",
    "#             colname = corr_matrix.columns[i]\n",
    "#             correlated_features.add(colname)\n",
    "# correlated_features\n",
    "# df_2=df.drop(columns=correlated_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Abundance_Controls', 'Length', 'Mass', 'frac_aa_A', 'frac_aa_C',\n       'frac_aa_D', 'frac_aa_F', 'frac_aa_H', 'frac_aa_I', 'frac_aa_M',\n       'frac_aa_N', 'frac_aa_R', 'frac_aa_S', 'frac_aa_T', 'frac_aa_V',\n       'frac_aa_W', 'frac_aa_Y', 'molecular_weight', 'instability_index',\n       'flexibility_min', 'rsa_median', 'asa_sum',\n       'fraction_exposed_exposed_A', 'fraction_exposed_exposed_D',\n       'fraction_exposed_exposed_E', 'fraction_exposed_exposed_G',\n       'fraction_exposed_exposed_H', 'fraction_exposed_exposed_I',\n       'fraction_exposed_exposed_L', 'fraction_exposed_exposed_N',\n       'fraction_exposed_exposed_S', 'fraction_exposed_exposed_Y',\n       'nsp_secondary_structure_coil', 'nsp_secondary_structure_helix',\n       'nsp_disordered', 'asa_sum_normalized', 'Zeta Potential',\n       'Ligand_PEI', 'Ligand_Au', 'Surface_Ligand', 'Dtem', 'Dh_core',\n       'Dh_functionalized', 'Incubation Concentration (mg/ml)',\n       'Corona_Concentration (ug/mg)'], dtype=object)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "step=2\n",
    "feats=45\n",
    "estimator=RandomForestRegressor(n_estimators=100)\n",
    "selector = RFE(estimator, n_features_to_select=feats, step=step)\n",
    "selector = selector.fit(df, label_abund)\n",
    "selector.support_\n",
    "ranking=selector.ranking_\n",
    "feat_list = selector.get_feature_names_out()\n",
    "df_rfe=df[feat_list]\n",
    "feat_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1,  1,  1,  9,  1,  1, 12,  1,  7,  7,  1,  1,  1,  1,  5, 16,  1,\n        1,  1,  1,  1, 15,  6,  1, 10,  1, 16,  9,  3,  1,  1, 11, 20,  1,\n       18,  5,  1,  1, 19,  8, 21, 18,  4, 17, 20,  1,  1,  2,  1, 17, 13,\n        1, 15, 19,  1, 12, 14,  1,  4,  1,  6,  3, 10,  1,  1,  8, 11,  1,\n        1,  1,  1,  1, 14, 26,  1, 21, 22, 22, 23, 24, 13,  2,  1,  1,  1,\n        1,  1,  1, 23, 24, 25,  1,  1, 25, 26])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [40]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# estimator=Lasso(alpha=.05)\u001B[39;00m\n\u001B[0;32m      8\u001B[0m selector \u001B[38;5;241m=\u001B[39m RFECV(estimator\u001B[38;5;241m=\u001B[39mestimator, cv\u001B[38;5;241m=\u001B[39mcv, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneg_mean_squared_error\u001B[39m\u001B[38;5;124m'\u001B[39m, min_features_to_select\u001B[38;5;241m=\u001B[39mmin_feats, step\u001B[38;5;241m=\u001B[39mstep)\n\u001B[1;32m----> 9\u001B[0m selector \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_rfe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_abund\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m selector\u001B[38;5;241m.\u001B[39msupport_\n\u001B[0;32m     11\u001B[0m feat_list2 \u001B[38;5;241m=\u001B[39m selector\u001B[38;5;241m.\u001B[39mget_feature_names_out()\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:723\u001B[0m, in \u001B[0;36mRFECV.fit\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m    720\u001B[0m     parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)\n\u001B[0;32m    721\u001B[0m     func \u001B[38;5;241m=\u001B[39m delayed(_rfe_single_fit)\n\u001B[1;32m--> 723\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrfe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    725\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    726\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    728\u001B[0m scores \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(scores)\n\u001B[0;32m    729\u001B[0m scores_sum \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(scores, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:724\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    720\u001B[0m     parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)\n\u001B[0;32m    721\u001B[0m     func \u001B[38;5;241m=\u001B[39m delayed(_rfe_single_fit)\n\u001B[0;32m    723\u001B[0m scores \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[1;32m--> 724\u001B[0m     \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrfe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    725\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m cv\u001B[38;5;241m.\u001B[39msplit(X, y, groups)\n\u001B[0;32m    726\u001B[0m )\n\u001B[0;32m    728\u001B[0m scores \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(scores)\n\u001B[0;32m    729\u001B[0m scores_sum \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(scores, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:37\u001B[0m, in \u001B[0;36m_rfe_single_fit\u001B[1;34m(rfe, estimator, X, y, train, test, scorer)\u001B[0m\n\u001B[0;32m     35\u001B[0m X_train, y_train \u001B[38;5;241m=\u001B[39m _safe_split(estimator, X, y, train)\n\u001B[0;32m     36\u001B[0m X_test, y_test \u001B[38;5;241m=\u001B[39m _safe_split(estimator, X, y, test, train)\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[43m        \u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mscores_\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:296\u001B[0m, in \u001B[0;36mRFE._fit\u001B[1;34m(self, X, y, step_score, **fit_params)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting estimator with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m features.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m np\u001B[38;5;241m.\u001B[39msum(support_))\n\u001B[1;32m--> 296\u001B[0m estimator\u001B[38;5;241m.\u001B[39mfit(X[:, features], y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    298\u001B[0m \u001B[38;5;66;03m# Get importance and rank them\u001B[39;00m\n\u001B[0;32m    299\u001B[0m importances \u001B[38;5;241m=\u001B[39m _get_feature_importances(\n\u001B[0;32m    300\u001B[0m     estimator,\n\u001B[0;32m    301\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimportance_getter,\n\u001B[0;32m    302\u001B[0m     transform_func\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msquare\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    303\u001B[0m )\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    465\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    466\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    468\u001B[0m ]\n\u001B[0;32m    470\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    475\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 476\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    486\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1046\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1047\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1050\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 861\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    778\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 779\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 117\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    187\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 189\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    191\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1342\u001B[0m, in \u001B[0;36mDecisionTreeRegressor.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m   1313\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1314\u001B[0m     \u001B[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[0;32m   1315\u001B[0m \n\u001B[0;32m   1316\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1339\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m   1340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1342\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1344\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1345\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1346\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1347\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1348\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001B[0m, in \u001B[0;36mBaseDecisionTree.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    447\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    448\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    449\u001B[0m         splitter,\n\u001B[0;32m    450\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    455\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    456\u001B[0m     )\n\u001B[1;32m--> 458\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    461\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#run Recursive feature elimination with cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "step=1\n",
    "min_feats=1\n",
    "cv= KFold(n_splits=10)\n",
    "estimator=RandomForestRegressor(n_estimators=100)\n",
    "# estimator=Lasso(alpha=.05)\n",
    "selector = RFECV(estimator=estimator, cv=cv, scoring='neg_mean_squared_error', min_features_to_select=min_feats, step=step)\n",
    "selector = selector.fit(df_rfe, label_abund)\n",
    "selector.support_\n",
    "feat_list2 = selector.get_feature_names_out()\n",
    "selected_features= df_rfe.columns[selector.support_]\n",
    "df_2=df[feat_list2]\n",
    "df_2.to_excel(\"Input_data/Save_files/df_RFECV\"+id+\".xlsx\")\n",
    "rfecv_df=pd.DataFrame(selector.cv_results_)\n",
    "rfecv_df.to_excel(\"Output_data/RFECV_results\"+id+\".xlsx\")\n",
    "# label_abund_df.to_excel(\"Input_data/Save_files/label_abund_all.xlsx\")\n",
    "n_scores = len(selector.cv_results_[\"mean_test_score\"])\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "x=range(1, n_scores + 1)\n",
    "y=selector.cv_results_[\"mean_test_score\"]\n",
    "err=selector.cv_results_[\"std_test_score\"]\n",
    "plt.plot(x,y,'k-')\n",
    "plt.fill_between(x,y-err,y+err)\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.savefig('Output_data/RFECV45'+id+'.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#try using a meta model including a linear model and Random Forest Model#\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "#\n",
    "#\n",
    "#\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_rfe, label_abund, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # Create the base models\n",
    "# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# ridge = Ridge(alpha=0.1, random_state=42)\n",
    "#\n",
    "# rf_model = rf.fit(X_train,y_train)\n",
    "# ridge_model = ridge.fit(X_train,y_train)\n",
    "# # Make predictions with the base models\n",
    "# rf_pred = cross_val_predict(rf_model, X_train, y_train, cv=5)\n",
    "# ridge_pred = cross_val_predict(ridge_model, X_train, y_train, cv=5)\n",
    "#\n",
    "# # Create the input for the meta-model\n",
    "# stacking_input = np.column_stack((rf_pred, ridge_pred))\n",
    "#\n",
    "# # Train the meta-model (ridge regression)\n",
    "# meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "# meta_model.fit(stacking_input, y_train)\n",
    "#\n",
    "# # Make predictions with the meta-model\n",
    "# rf_pred_test = rf_model.predict(X_test)\n",
    "# ridge_pred_test = ridge_model.predict(X_test)\n",
    "# stacking_input_test = np.column_stack((rf_pred_test, ridge_pred_test))\n",
    "# y_pred_test = meta_model.predict(stacking_input_test)\n",
    "#\n",
    "# # Evaluate the performance of the stacked model\n",
    "# mse = mean_squared_error(y_test, y_pred_test)\n",
    "# print(\"MSE: {:.2f}\".format(mse))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#test estimator to see what is the ideal number of estimators\n",
    "# estimators=np.arange(5,500,5)\n",
    "# out_name=np.arange(5,500,5)\n",
    "# # print(out_name)\n",
    "# scores=[]\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df_abund, label_abund,\n",
    "#                                                         test_size=0.2,\n",
    "#                                                         random_state=42)\n",
    "#\n",
    "# for i in range(len(estimators)):\n",
    "#     rfg = RandomForestRegressor(n_estimators=estimators[i])\n",
    "#     rfg.fit(x_train, y_train)\n",
    "#     score=rfg.score(x_test,y_test)\n",
    "#     scores.append(score)\n",
    "# a=pd.DataFrame(list(zip(estimators,scores)), columns=['number of estimators','accuracy'])\n",
    "# a.to_excel(\"Output_data/estimators_score_abund.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#look at loss as a function of feature\n",
    "feats=[]\n",
    "scores=[]\n",
    "df=df_2\n",
    "id='_RFECV'\n",
    "label=label_abund\n",
    "# frames=['df','df_norm','df_NSP_drop']\n",
    "# for x,i in enumerate(df_listnew):\\\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df,label, test_size = 0.2, random_state=42)\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "importances=rfr.feature_importances_*100\n",
    "scores.append(topscore)\n",
    "feats.append('allfeats')\n",
    "importances=np.insert(importances,[0],0)\n",
    "for j in x_test.columns:\n",
    "    tmp=x_test.copy()\n",
    "    # print(x_test)\n",
    "    np.random.shuffle(tmp[j].values)\n",
    "    scram_score=rfr.score(tmp,y_test)\n",
    "    scores.append(scram_score)\n",
    "    feats.append(j)\n",
    "\n",
    "a=pd.DataFrame(list(zip(feats,scores,importances)),columns=['feat','score','importances'])\n",
    "a.to_excel(\"Output_data/scram_loss_feats\"+id+\".xlsx\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Fit and predict after dropping the next least loss feature, drop one feature at a time\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "feats=[]\n",
    "scores=[]\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "a=list(zip(rfr.feature_importances_,rfr.feature_names_in_))\n",
    "a.sort(reverse=True)\n",
    "col_import=pd.DataFrame(a,columns=['importances','names'])\n",
    "sorted_cols=col_import['names']\n",
    "feats.append('topscore')\n",
    "scores.append(topscore)\n",
    "\n",
    "#df_3=df_2.copy()\n",
    "for i in sorted_cols:\n",
    "    df_3=df_2.copy() #remove if you only want to drop each feature instead of dropping one feature at a time\n",
    "    df_3.drop(columns=[i],inplace=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_3,label_abund, test_size = 0.2, random_state=42)\n",
    "    rfr.fit(x_train,y_train)\n",
    "    score=rfr.score(x_test,y_test)\n",
    "    scores.append(score)\n",
    "    feats.append(i)\n",
    "\n",
    "df_out=pd.DataFrame(list(zip(feats,scores)),columns=['dropped feat','score without feat'])\n",
    "df_out.to_excel(\"Output_data/scores_afterdrop\"+id+\".xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Fit and predict after dropping the next least loss feature, drop one feature at a time cumulative\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "feats=[]\n",
    "scores=[]\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "a=list(zip(rfr.feature_importances_,rfr.feature_names_in_))\n",
    "a.sort(reverse=True)\n",
    "col_import=pd.DataFrame(a,columns=['importances','names'])\n",
    "sorted_cols=col_import['names']\n",
    "feats.append('topscore')\n",
    "scores.append(topscore)\n",
    "\n",
    "df_3=df_2.copy()\n",
    "for i in sorted_cols:\n",
    "    if i == sorted_cols.iloc[-1]:\n",
    "        break\n",
    "    # df_3=df_2.copy() #remove if you only want to drop each feature instead of dropping one feature at a time\n",
    "    df_3.drop(columns=[i],inplace=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_3,label_abund, test_size = 0.2, random_state=42)\n",
    "    rfr.fit(x_train,y_train)\n",
    "    score=rfr.score(x_test,y_test)\n",
    "    scores.append(score)\n",
    "    feats.append(i)\n",
    "\n",
    "df_out=pd.DataFrame(list(zip(feats,scores)),columns=['dropped feat','score without feat'])\n",
    "df_out.to_excel(\"Output_data/scores_afterdrop_cumulative_reverse\"+id+\".xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot predictions with prediction accuracies#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6580032078152742\n",
      "0.8121382250361637\n"
     ]
    }
   ],
   "source": [
    "#look at pearson correlation of predictions#\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "predictions=rfr.predict(x_test)\n",
    "r2=r2_score(y_test, predictions)\n",
    "corr,_ = pearsonr(y_test, predictions)\n",
    "\n",
    "print(r2)\n",
    "print(corr)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6064262306745349\n",
      "0.7791496130816059\n"
     ]
    }
   ],
   "source": [
    "#look at pearson correlation of predictions#\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "predictions=rfr.predict(x_test)\n",
    "r2=r2_score(y_test, predictions)\n",
    "corr,_ = pearsonr(y_test, predictions)\n",
    "\n",
    "print(r2)\n",
    "print(corr)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NormBALFsamples.xlsx', 'Norm_Intensity _all20230403.xlsx']\n"
     ]
    }
   ],
   "source": [
    "#Editable Variables\n",
    "multi_files=False #set to false if you just want to set one  prot_abund_file\n",
    "in_dir=\"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file='Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file='Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "# take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "files = os.listdir(in_dir)\n",
    "print(files)\n",
    "if multi_files == True:\n",
    "    for i,f in enumerate(files):\n",
    "        print(i)\n",
    "        if i==0:\n",
    "            raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "            # print(raw_MS_data)\n",
    "            raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "            print('BALF',raw_MS_data.shape)\n",
    "        else:\n",
    "            print(i)\n",
    "            temp = pd.read_excel(in_dir+f,header=0)\n",
    "            temp = pd.melt(temp, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "            print('Bovine',temp.shape)\n",
    "            # print(temp)\n",
    "            # print(temp)\n",
    "            raw_MS_data2=pd.concat([raw_MS_data,temp])\n",
    "            print('merge',raw_MS_data2.shape)\n",
    "            # print('did it')\n",
    "else:\n",
    "    raw_MS_data2=pd.read_excel(prot_abund_file,header=0)\n",
    "# raw_MS_data2\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset\n",
    "# raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "25738"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_MS_data2['Abundance']==0).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "multi_files=False #set to false if you just want to set one  prot_abund_file\n",
    "in_dir=\"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file='Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file='Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "id='allnps_meltb4join'\n",
    "# take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "files = os.listdir(in_dir)\n",
    "if multi_files == True:\n",
    "    for i,f in enumerate(files):\n",
    "        if i==0:\n",
    "            raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "            # print(raw_MS_data)\n",
    "            raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "        else:\n",
    "\n",
    "            temp = pd.read_excel(in_dir+f,header=0)\n",
    "            temp = pd.melt(temp, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "            raw_MS_data=pd.concat([raw_MS_data,temp])\n",
    "\n",
    "else:\n",
    "    raw_MS_data=pd.read_excel(prot_abund_file,header=0)\n",
    "    raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "#remove prots that were added due to merge\n",
    "raw_MS_data=raw_MS_data.dropna()\n",
    "###Bring in controls (MS data for serums)##\n",
    "controls=pd.read_excel(controls_file,header=0)\n",
    "MS_data_controls = pd.merge(raw_MS_data,controls,how='left', on='Entry')\n",
    "###Bring in Uniprot_data,NSPdata and NP data##\n",
    "uniprot_dat=pd.read_excel(uniprot_filepath,header=0)\n",
    "NSP_data=pd.read_excel(NSPfilePath)\n",
    "###Bring in NP data and merge to get complete NP dataset###\n",
    "NPUNdata=pd.read_excel(NP_filepath,header=0,sheet_name='NPUNID')\n",
    "NPprop=pd.read_excel(NP_filepath,header=0,sheet_name='NP_Props')\n",
    "NPdata=pd.merge(NPUNdata,NPprop,how=\"left\",on='NPID')\n",
    "NPdata.dropna(inplace=True)\n",
    "#calculate Enrichment\n",
    "#####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "# MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "#keep abundance Controls\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance'])\n",
    "raw_prop_data=pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left',on='Entry')\n",
    "Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']),how='left', on='Entry') #merges netsurfp features and biopython features\n",
    "Protein_data_complete.fillna(0,inplace=True)\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n",
    "\n",
    "data_complete= pd.merge(Protein_data_complete,NPdata,how='inner', on='Sample_num')\n",
    "data_complete.drop(columns=['notes','Notes','NPUNID'],inplace=True)\n",
    "data_complete.fillna(0,inplace=True)\n",
    "data_complete= data_complete.replace([-np.inf],'-12')\n",
    "data_complete=data_complete.replace([np.inf],'12')\n",
    "#create ordinal variables\n",
    "# data_complete2=pd.get_dummies(data_complete, columns=['Core Material','Surface_Ligand'])\n",
    "le=LabelEncoder()\n",
    "data_complete['Core Material']=le.fit_transform(data_complete['Core Material'])\n",
    "data_complete['Surface_Ligand']=le.fit_transform(data_complete['Surface_Ligand'])\n",
    "\n",
    "#set labels (what we are trying to predict) as Enrichment column\n",
    "# labels=data_complete['Enrichment'].copy()\n",
    "label_abund=np.ravel(data_complete['Abundance'].copy())\n",
    "label_abund_df=pd.DataFrame(label_abund)\n",
    "# label_enrich=np.ravel(data_complete['Enrichment'].copy())\n",
    "#make it one dimenisional\n",
    "#drop qualitative, not neccessary, and label columns\n",
    "#create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "to_drop=data_complete.filter(like='total_exposed_')\n",
    "data_complete.drop(columns=to_drop,inplace=True)\n",
    "df=data_complete.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n",
    "# df_enrich=data_complete.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n",
    "df.to_excel(\"Input_data/Save_files/df_\"+id+\".xlsx\")\n",
    "label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "# label_enrich.to_excel(\"Input_data/Save_files/label_enrich_synth.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.concat((df, label_abund_df), axis=1)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_no0 = data_complete[data_complete.Abundance != 0]\n",
    "df_no0.shape\n",
    "label_abund2=np.ravel(df_no0['Abundance'].copy())\n",
    "\n",
    "# label_enrich=np.ravel(data_complete['Enrichment'].copy())\n",
    "#make it one dimenisional\n",
    "#drop qualitative, not neccessary, and label columns\n",
    "#create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "to_drop=df_no0.filter(like='total_exposed_')\n",
    "df_no0.drop(columns=to_drop,inplace=True)\n",
    "df_no0=df_no0.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2=pd.read_excel(\"Input_data/Save_files/df_synth_RFECV\"+id+\".xlsx\")\n",
    "df_2.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "label_df=pd.read_excel(\"Input_data/Save_files/label_abund_\"+id+\".xlsx\")\n",
    "label_abund=np.ravel(label_df[0])\n",
    "print(label_abund.shape)\n",
    "print(df_2.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Run PCA to seee how data differentiates#\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x=df_2\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(x)\n",
    "pca= PCA(n_components=5)\n",
    "x_pca=pca.fit_transform(X_std)\n",
    "\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c=label_abund2, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.savefig('Output_data/PCA'+id+'.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_pca"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Remove correlated features (over r2 threshold level) and output reduced dataframe (df2)# ##Maybe use in future##\n",
    "# corr_matrix = df.corr()\n",
    "# threshold = 0.8\n",
    "# correlated_features = set()\n",
    "#\n",
    "# for i in range(len(corr_matrix.columns)):\n",
    "#     for j in range(i):\n",
    "#         if threshold < abs(corr_matrix.iloc[i, j]) < 1:\n",
    "#             colname = corr_matrix.columns[i]\n",
    "#             correlated_features.add(colname)\n",
    "# correlated_features\n",
    "# df_2=df.drop(columns=correlated_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "step=2\n",
    "feats=50\n",
    "estimator=RandomForestRegressor(n_estimators=100)\n",
    "selector = RFE(estimator, n_features_to_select=feats, step=step)\n",
    "selector = selector.fit(df_no0, label_abund2)\n",
    "selector.support_\n",
    "ranking=selector.ranking_\n",
    "feat_list = selector.get_feature_names_out()\n",
    "df_rfe=df_no0[feat_list]\n",
    "feat_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#run Recursive feature elimination with cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "id2='dropped_controlAbundance'\n",
    "step=2\n",
    "min_feats=5\n",
    "cv= KFold(n_splits=10)\n",
    "estimator=RandomForestRegressor(n_estimators=100)\n",
    "# estimator=Lasso(alpha=.05)\n",
    "selector = RFECV(estimator=estimator, cv=cv, scoring='neg_mean_squared_error', min_features_to_select=min_feats, step=step)\n",
    "selector = selector.fit(df_rfe, label_abund2)\n",
    "selector.support_\n",
    "feat_list2 = selector.get_feature_names_out()\n",
    "selected_features= df_rfe.columns[selector.support_]\n",
    "df_2=df[feat_list2]\n",
    "df_2.to_excel(\"Input_data/Save_files/df_RFECV\"+id+id2+\".xlsx\")\n",
    "rfecv_df=pd.DataFrame(selector.cv_results_)\n",
    "rfecv_df.to_excel(\"Output_data/RFECV_results\"+id+id2+\".xlsx\")\n",
    "# label_abund_df.to_excel(\"Input_data/Save_files/label_abund_all.xlsx\")\n",
    "n_scores = len(selector.cv_results_[\"mean_test_score\"])\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "x=range(1, n_scores + 1)\n",
    "y=selector.cv_results_[\"mean_test_score\"]\n",
    "err=selector.cv_results_[\"std_test_score\"]\n",
    "plt.plot(x,y,'k-')\n",
    "plt.fill_between(x,y-err,y+err)\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.savefig('Output_data/RFECV45'+id+id2+'.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2=df_no0[feat_list2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_abund2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#test estimator to see what is the ideal number of estimators\n",
    "# estimators=np.arange(5,500,5)\n",
    "# out_name=np.arange(5,500,5)\n",
    "# # print(out_name)\n",
    "# scores=[]\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df_abund, label_abund,\n",
    "#                                                         test_size=0.2,\n",
    "#                                                         random_state=42)\n",
    "#\n",
    "# for i in range(len(estimators)):\n",
    "#     rfg = RandomForestRegressor(n_estimators=estimators[i])\n",
    "#     rfg.fit(x_train, y_train)\n",
    "#     score=rfg.score(x_test,y_test)\n",
    "#     scores.append(score)\n",
    "# a=pd.DataFrame(list(zip(estimators,scores)), columns=['number of estimators','accuracy'])\n",
    "# a.to_excel(\"Output_data/estimators_score_abund.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##SCRAM Loss##\n",
    "#look at loss as a function of feature\n",
    "# df_2.drop(columns='Abundance_Controls',inplace=True)\n",
    "feats=[]\n",
    "r2s=[]\n",
    "pearson=[]\n",
    "id2='norm'\n",
    "test_percent=0.2\n",
    "label=label_abund2\n",
    "# frames=['df','df_norm','df_NSP_drop']\n",
    "# for x,i in enumerate(df_listnew):\\\n",
    "# df_2.drop(columns=['Abundance Controls'],inplace=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label, test_size = test_percent, random_state=42)\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "predictions=rfr.predict(x_test)\n",
    "r2=r2_score(y_test, predictions)\n",
    "corr,_ = pearsonr(y_test, predictions)\n",
    "r2s.append(r2)\n",
    "pearson.append(corr)\n",
    "feat_import= rfr.feature_importances_\n",
    "feats.append('allfeats')\n",
    "feat_import=np.insert(feat_import, [0], 0)\n",
    "for j in x_test.columns:\n",
    "    tmp=x_test.copy()\n",
    "    # print(x_test)\n",
    "    np.random.shuffle(tmp[j].values)\n",
    "    scram_score=rfr.score(tmp,y_test)\n",
    "    predictions=rfr.predict(tmp)\n",
    "    r2=r2_score(y_test, predictions)\n",
    "    corr,_ = pearsonr(y_test, predictions)\n",
    "    r2s.append(r2)\n",
    "    pearson.append(corr)\n",
    "    feats.append(j)\n",
    "\n",
    "a=pd.DataFrame(list(zip(feats, pearson,r2s, feat_import)), columns=['feat', 'pearson','R2', 'importances'])\n",
    "a.to_excel(\"Output_data/scram_loss_feats\"+id2+id+\".xlsx\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.xticks(rotation=90)\n",
    "ax.plot(feats, pearson, color='tab:blue',marker= 'o')\n",
    "ax.plot(feats, r2s, color='tab:red',marker='s')\n",
    "ax.plot(feats, feat_import, color='tab:purple', marker='x')\n",
    "\n",
    "plt.title('Score as a function of feature scrambling')\n",
    "plt.savefig('Output_data/RFECV50_FeatScramLoss'+id+'.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##FEAT DROP NOT Cumulative##\n",
    "##Fit and predict after dropping the next least loss feature, drop one feature at a time\n",
    "\n",
    "feats=[]\n",
    "r2s=[]\n",
    "pearson=[]\n",
    "id2='norm'\n",
    "test_percent=0.4\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = test_percent, random_state=42)\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "a=list(zip(rfr.feature_importances_,rfr.feature_names_in_))\n",
    "a.sort(reverse=True)\n",
    "col_import=pd.DataFrame(a,columns=['importances','names'])\n",
    "sorted_cols=col_import['names']\n",
    "feats.append('topscore')\n",
    "scores.append(topscore)\n",
    "\n",
    "#df_3=df_2.copy()\n",
    "for i in sorted_cols:\n",
    "    df_3=df_2.copy() #remove if you only want to drop each feature instead of dropping one feature at a time\n",
    "    df_3.drop(columns=[i],inplace=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_3,label_abund, test_size = 0.2, random_state=42)\n",
    "    rfr.fit(x_train,y_train)\n",
    "    predictions=rfr.predict(x_test)\n",
    "    r2=r2_score(y_test, predictions)\n",
    "    corr,_ = pearsonr(y_test, predictions)\n",
    "    r2s.append(r2)\n",
    "    pearson.append(corr)\n",
    "    feats.append(i)\n",
    "\n",
    "df_out=pd.DataFrame(list(zip(feats,scores)),columns=['dropped feat','score without feat'])\n",
    "df_out.to_excel(\"Output_data/scores_afterdrop\"+id+id2+\".xlsx\")\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(rotation=90)\n",
    "# Create a bar chart for the first y-axis\n",
    "ax1.plot(feats, pearson, color='tab:blue')\n",
    "ax1.set_xlabel('Features')\n",
    "ax1.set_ylabel('Pearson Correlation', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax1.set_ylim(0,1)\n",
    "\n",
    "# Create a second y-axis object\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Create a line chart for the second y-axis\n",
    "ax2.plot(feats, r2s, color='tab:red', marker='o')\n",
    "ax2.set_ylabel('R^2', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "ax2.set_ylim(0,1)\n",
    "plt.savefig('Output_data/RFECV50_FeatDropLoss'+id+'.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##CUMUlATIVE FEATURE DROP##\n",
    "##Fit and predict after dropping the next least loss feature, drop one feature at a time cumulative\n",
    "\n",
    "feats=[]\n",
    "r2s=[]\n",
    "pearson=[]\n",
    "id2='norm'\n",
    "test_percent=0.3\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund2, test_size = test_percent, random_state=42)\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "predictions=rfr.predict(x_test)\n",
    "r2=r2_score(y_test, predictions)\n",
    "corr,_ = pearsonr(y_test, predictions)\n",
    "a=list(zip(rfr.feature_importances_,rfr.feature_names_in_))\n",
    "a.sort(reverse=True)\n",
    "col_import=pd.DataFrame(a,columns=['importances','names'])\n",
    "sorted_cols=col_import['names']\n",
    "feats.append('All Feats')\n",
    "r2s.append(r2)\n",
    "pearson.append(corr)\n",
    "\n",
    "df_3=df_2.copy()\n",
    "for i in sorted_cols:\n",
    "    if i == sorted_cols.iloc[-1]:\n",
    "        break\n",
    "    # df_3=df_2.copy() #remove if you only want to drop each feature instead of dropping one feature at a time\n",
    "    df_3.drop(columns=[i],inplace=True)\n",
    "    print(df_3.shape)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_3,label_abund2, test_size = test_percent, random_state=42)\n",
    "    rfr.fit(x_train,y_train)\n",
    "    predictions=rfr.predict(x_test)\n",
    "    r2=r2_score(y_test, predictions)\n",
    "    corr,_ = pearsonr(y_test, predictions)\n",
    "    r2s.append(r2)\n",
    "    pearson.append(corr)\n",
    "    feats.append(i)\n",
    "\n",
    "df_out=pd.DataFrame(list(zip(feats,pearson,r2s)),columns=['dropped feat','Pearson','r2'])\n",
    "df_out.to_excel(\"Output_data/scores_afterdrop_cumulative_reverse\"+id+id2+\".xlsx\")\n",
    "# Create a figure and axis object\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(rotation=90)\n",
    "# Create a bar chart for the first y-axis\n",
    "ax1.plot(feats, pearson, color='tab:blue')\n",
    "ax1.set_xlabel('Features')\n",
    "ax1.set_ylabel('Pearson Correlation', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax1.set_ylim(0,1)\n",
    "\n",
    "# Create a second y-axis object\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Create a line chart for the second y-axis\n",
    "ax2.plot(feats, r2s, color='tab:red', marker='o')\n",
    "ax2.set_ylabel('R^2', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "ax2.set_ylim(0,1)\n",
    "\n",
    "# Set the title and display the chart\n",
    "plt.title('Cumulative Feature Drop')\n",
    "\n",
    "plt.savefig('Output_data/RFECV50_FeatDropLossCumul'+id+id2+'.png',bbox_inches='tight')\n",
    "plt.show()\n",
    "#\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.bar(feats, scores)\n",
    "# plt.xlabel('Feature')\n",
    "# plt.ylabel('Score')\n",
    "# plt.title('Score after Cumulative Feature Drop')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.savefig('Output_data/RFECV50_FeatDropLossCumul'+id+'.png',bbox_inches='tight')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "      Length      Mass  frac_aa_A  frac_aa_C  frac_aa_D  frac_aa_E  frac_aa_F  \\\n0      607.0   69293.0   0.079077   0.057661   0.065898   0.097199   0.049423   \n1      359.0   38419.0   0.097493   0.038997   0.064067   0.050139   0.033426   \n2     1236.0  140374.0   0.038026   0.066343   0.046926   0.080906   0.027508   \n3      465.0   52347.0   0.064516   0.019355   0.045161   0.075269   0.053763   \n4      416.0   46104.0   0.084135   0.014423   0.050481   0.064904   0.055288   \n...      ...       ...        ...        ...        ...        ...        ...   \n8515   548.0   57765.0   0.093066   0.060219   0.041971   0.063869   0.032847   \n8516   415.0   47231.0   0.091566   0.019277   0.040964   0.067470   0.045783   \n8517   193.0   22748.0   0.031088   0.015544   0.056995   0.036269   0.062176   \n8518   113.0   12977.0   0.106195   0.026549   0.053097   0.070796   0.026549   \n8519   562.0   61589.0   0.083630   0.008897   0.060498   0.055160   0.051601   \n\n      frac_aa_G  frac_aa_K  frac_aa_N  ...  nsp_secondary_structure_helix  \\\n0      0.028007   0.098847   0.023064  ...                          0.710   \n1      0.061281   0.041783   0.019499  ...                          0.120   \n2      0.076861   0.065534   0.037217  ...                          0.005   \n3      0.049462   0.068817   0.053763  ...                          0.241   \n4      0.052885   0.076923   0.055288  ...                          0.257   \n...         ...        ...        ...  ...                            ...   \n8515   0.135036   0.029197   0.014599  ...                          0.088   \n8516   0.053012   0.036145   0.038554  ...                          0.337   \n8517   0.036269   0.082902   0.062176  ...                          0.073   \n8518   0.053097   0.044248   0.017699  ...                          0.310   \n8519   0.090747   0.065836   0.042705  ...                          0.367   \n\n      nsp_disordered  asa_sum_normalized  Zeta Potential  Ligand_Au  \\\n0              0.043            0.560216           -42.0          0   \n1              0.348            0.745338           -42.0          0   \n2              0.019            0.676699           -42.0          0   \n3              0.168            0.590988           -42.0          0   \n4              0.127            0.578761           -42.0          0   \n...              ...                 ...             ...        ...   \n8515           0.356            0.768389            12.0          1   \n8516           0.108            0.582805            12.0          1   \n8517           0.031            0.549793            12.0          1   \n8518           0.142            0.664929            12.0          1   \n8519           0.002            0.471792            12.0          1   \n\n      Surface_Ligand  Dtem  Dh_core  Dh_functionalized  \\\n0                  0  82.0    149.0              149.0   \n1                  0  82.0    149.0              149.0   \n2                  0  82.0    149.0              149.0   \n3                  0  82.0    149.0              149.0   \n4                  0  82.0    149.0              149.0   \n...              ...   ...      ...                ...   \n8515               2  98.0    149.0              229.0   \n8516               2  98.0    149.0              229.0   \n8517               2  98.0    149.0              229.0   \n8518               2  98.0    149.0              229.0   \n8519               2  98.0    149.0              229.0   \n\n      Incubation Concentration (mg/ml)  \n0                                  4.0  \n1                                  4.0  \n2                                  4.0  \n3                                  4.0  \n4                                  4.0  \n...                                ...  \n8515                              40.0  \n8516                              40.0  \n8517                              40.0  \n8518                              40.0  \n8519                              40.0  \n\n[8520 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Length</th>\n      <th>Mass</th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_K</th>\n      <th>frac_aa_N</th>\n      <th>...</th>\n      <th>nsp_secondary_structure_helix</th>\n      <th>nsp_disordered</th>\n      <th>asa_sum_normalized</th>\n      <th>Zeta Potential</th>\n      <th>Ligand_Au</th>\n      <th>Surface_Ligand</th>\n      <th>Dtem</th>\n      <th>Dh_core</th>\n      <th>Dh_functionalized</th>\n      <th>Incubation Concentration (mg/ml)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>607.0</td>\n      <td>69293.0</td>\n      <td>0.079077</td>\n      <td>0.057661</td>\n      <td>0.065898</td>\n      <td>0.097199</td>\n      <td>0.049423</td>\n      <td>0.028007</td>\n      <td>0.098847</td>\n      <td>0.023064</td>\n      <td>...</td>\n      <td>0.710</td>\n      <td>0.043</td>\n      <td>0.560216</td>\n      <td>-42.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>82.0</td>\n      <td>149.0</td>\n      <td>149.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>359.0</td>\n      <td>38419.0</td>\n      <td>0.097493</td>\n      <td>0.038997</td>\n      <td>0.064067</td>\n      <td>0.050139</td>\n      <td>0.033426</td>\n      <td>0.061281</td>\n      <td>0.041783</td>\n      <td>0.019499</td>\n      <td>...</td>\n      <td>0.120</td>\n      <td>0.348</td>\n      <td>0.745338</td>\n      <td>-42.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>82.0</td>\n      <td>149.0</td>\n      <td>149.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1236.0</td>\n      <td>140374.0</td>\n      <td>0.038026</td>\n      <td>0.066343</td>\n      <td>0.046926</td>\n      <td>0.080906</td>\n      <td>0.027508</td>\n      <td>0.076861</td>\n      <td>0.065534</td>\n      <td>0.037217</td>\n      <td>...</td>\n      <td>0.005</td>\n      <td>0.019</td>\n      <td>0.676699</td>\n      <td>-42.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>82.0</td>\n      <td>149.0</td>\n      <td>149.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>465.0</td>\n      <td>52347.0</td>\n      <td>0.064516</td>\n      <td>0.019355</td>\n      <td>0.045161</td>\n      <td>0.075269</td>\n      <td>0.053763</td>\n      <td>0.049462</td>\n      <td>0.068817</td>\n      <td>0.053763</td>\n      <td>...</td>\n      <td>0.241</td>\n      <td>0.168</td>\n      <td>0.590988</td>\n      <td>-42.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>82.0</td>\n      <td>149.0</td>\n      <td>149.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>416.0</td>\n      <td>46104.0</td>\n      <td>0.084135</td>\n      <td>0.014423</td>\n      <td>0.050481</td>\n      <td>0.064904</td>\n      <td>0.055288</td>\n      <td>0.052885</td>\n      <td>0.076923</td>\n      <td>0.055288</td>\n      <td>...</td>\n      <td>0.257</td>\n      <td>0.127</td>\n      <td>0.578761</td>\n      <td>-42.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>82.0</td>\n      <td>149.0</td>\n      <td>149.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8515</th>\n      <td>548.0</td>\n      <td>57765.0</td>\n      <td>0.093066</td>\n      <td>0.060219</td>\n      <td>0.041971</td>\n      <td>0.063869</td>\n      <td>0.032847</td>\n      <td>0.135036</td>\n      <td>0.029197</td>\n      <td>0.014599</td>\n      <td>...</td>\n      <td>0.088</td>\n      <td>0.356</td>\n      <td>0.768389</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>98.0</td>\n      <td>149.0</td>\n      <td>229.0</td>\n      <td>40.0</td>\n    </tr>\n    <tr>\n      <th>8516</th>\n      <td>415.0</td>\n      <td>47231.0</td>\n      <td>0.091566</td>\n      <td>0.019277</td>\n      <td>0.040964</td>\n      <td>0.067470</td>\n      <td>0.045783</td>\n      <td>0.053012</td>\n      <td>0.036145</td>\n      <td>0.038554</td>\n      <td>...</td>\n      <td>0.337</td>\n      <td>0.108</td>\n      <td>0.582805</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>98.0</td>\n      <td>149.0</td>\n      <td>229.0</td>\n      <td>40.0</td>\n    </tr>\n    <tr>\n      <th>8517</th>\n      <td>193.0</td>\n      <td>22748.0</td>\n      <td>0.031088</td>\n      <td>0.015544</td>\n      <td>0.056995</td>\n      <td>0.036269</td>\n      <td>0.062176</td>\n      <td>0.036269</td>\n      <td>0.082902</td>\n      <td>0.062176</td>\n      <td>...</td>\n      <td>0.073</td>\n      <td>0.031</td>\n      <td>0.549793</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>98.0</td>\n      <td>149.0</td>\n      <td>229.0</td>\n      <td>40.0</td>\n    </tr>\n    <tr>\n      <th>8518</th>\n      <td>113.0</td>\n      <td>12977.0</td>\n      <td>0.106195</td>\n      <td>0.026549</td>\n      <td>0.053097</td>\n      <td>0.070796</td>\n      <td>0.026549</td>\n      <td>0.053097</td>\n      <td>0.044248</td>\n      <td>0.017699</td>\n      <td>...</td>\n      <td>0.310</td>\n      <td>0.142</td>\n      <td>0.664929</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>98.0</td>\n      <td>149.0</td>\n      <td>229.0</td>\n      <td>40.0</td>\n    </tr>\n    <tr>\n      <th>8519</th>\n      <td>562.0</td>\n      <td>61589.0</td>\n      <td>0.083630</td>\n      <td>0.008897</td>\n      <td>0.060498</td>\n      <td>0.055160</td>\n      <td>0.051601</td>\n      <td>0.090747</td>\n      <td>0.065836</td>\n      <td>0.042705</td>\n      <td>...</td>\n      <td>0.367</td>\n      <td>0.002</td>\n      <td>0.471792</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>98.0</td>\n      <td>149.0</td>\n      <td>229.0</td>\n      <td>40.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8520 rows × 38 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.drop(columns=['Corona_Concentration (ug/mg)'],inplace=True)\n",
    "df_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
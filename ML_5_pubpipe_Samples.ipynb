{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "\n",
    "scorings=['r2','neg_mean_squared_error','mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    id = 'Con_drop_' +str(z)+ '_30' + '%zeros'\n",
    "    # take files in_dir and combine then into a pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "    # melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "    files = os.listdir(in_dir)\n",
    "    if multi_files == True:\n",
    "        for i, f in enumerate(files):\n",
    "            if i == 0:\n",
    "                raw_MS_data = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = raw_MS_data.shape[1]\n",
    "                cutoff = int(zerosperrow * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                # print(raw_MS_data)\n",
    "                raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = temp.shape[1]\n",
    "                cutoff = int(zerosperrow * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                temp = temp.drop(temp[(temp == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                temp = pd.melt(temp, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "                raw_MS_data = pd.concat([raw_MS_data, temp])\n",
    "                print('final shape after melt', raw_MS_data.shape)\n",
    "                print('number of zeros in the dataset:',(raw_MS_data == 0).sum().sum())\n",
    "\n",
    "\n",
    "    else:\n",
    "        raw_MS_data = pd.read_excel(prot_abund_file, header=0)\n",
    "        cols = raw_MS_data.shape[1]\n",
    "        cutoff = int(zerosperrow * cols)\n",
    "        print('shape beofre dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "        print('shape after dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "    #remove prots that were added due to merge\n",
    "    raw_MS_data = raw_MS_data.dropna()\n",
    "    ###Bring in controls (MS data for serums)##\n",
    "    controls = pd.read_excel(controls_file, header=0)\n",
    "    MS_data_controls = pd.merge(raw_MS_data, controls, how='left', on='Entry')\n",
    "    ###Bring in Uniprot_data,NSPdata and NP data##\n",
    "    uniprot_dat = pd.read_excel(uniprot_filepath, header=0)\n",
    "    NSP_data = pd.read_excel(NSPfilePath)\n",
    "    ###Bring in NP data and merge to get complete NP dataset###\n",
    "    NPUNdata = pd.read_excel(NP_filepath, header=0, sheet_name='NPUNID')\n",
    "    NPprop = pd.read_excel(NP_filepath, header=0, sheet_name='NP_Props')\n",
    "    NPdata = pd.merge(NPUNdata, NPprop, how=\"left\", on='NPID')\n",
    "    NPdata.dropna(inplace=True)\n",
    "    #calculate Enrichment\n",
    "    #####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "    # MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "    # MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "    raw_prop_data = pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left', on='Entry')\n",
    "    Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']), how='left',\n",
    "                                     on='Entry')\n",
    "    #merges netsurfp features and biopython features\n",
    "    Protein_data_complete.fillna(0, inplace=True)\n",
    "    data_complete = pd.merge(Protein_data_complete, NPdata, how='inner', on='Sample_num')\n",
    "    data_complete.fillna(0, inplace=True)\n",
    "    #create ordinal variables for the core materials and surface ligands\n",
    "    le = LabelEncoder()\n",
    "    data_complete['Core Material'] = le.fit_transform(data_complete['Core Material'])\n",
    "    data_complete['Surface_Ligand'] = le.fit_transform(data_complete['Surface_Ligand'])\n",
    "    #shuffle data using sample\n",
    "    data_complete = data_complete.sample(frac=1)\n",
    "    #set labels (what we are trying to predict) as Abundance\n",
    "    label_abund_df = data_complete['Abundance'].copy()\n",
    "    label_abund = np.ravel(label_abund_df)\n",
    "    NPIDs=data_complete['NPUNID'].copy()\n",
    "    data_complete.drop(columns=['notes', 'Notes', 'NPUNID'], inplace=True)\n",
    "\n",
    "    #make it one dimenisional\n",
    "    #drop qualitative, not neccessary, and label columns\n",
    "    #create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "    to_drop = data_complete.filter(like='total_exposed_')\n",
    "    data_complete.drop(columns=to_drop, inplace=True)\n",
    "    df = data_complete.drop(\n",
    "        ['Entry', 'Abundance', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5',\n",
    "         'Raw_FileID'], axis=1)\n",
    "    if abund_controls == False:\n",
    "        df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "\n",
    "    df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "    df_out.to_excel(\"Input_data/Save_files/df_whole_\"+id+\".xlsx\")\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "\n",
    "    #Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    df=pd.read_excel('Input_data/Save_files/df_whole_noCon_drop30%zeros.xlsx')\n",
    "    labels= df['label_abund_df'].copy()\n",
    "    id_col= df['NPIDs'].copy()\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "    # Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([df, NPIDs,label_abund_df],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## Drop a nanoparticle and then predict it###\n",
    "##NPs of interest##\n",
    "#easy to hard NPUNID 1,20,19,16,7,31,34,34\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0  Length   Mass  frac_aa_A  frac_aa_C  frac_aa_D  frac_aa_E  \\\n0             18     202  22144   0.103960   0.039604   0.044554   0.079208   \n1            435     733  84731   0.047749   0.009550   0.075034   0.132333   \n2            217     103  12217   0.126214   0.029126   0.029126   0.087379   \n3            412     481  53698   0.056133   0.008316   0.049896   0.049896   \n4           1589     202  23182   0.089109   0.024752   0.049505   0.099010   \n...          ...     ...    ...        ...        ...        ...        ...   \n1765         398     625  69872   0.049600   0.057600   0.043200   0.049600   \n1766        1146     807  90976   0.052045   0.057001   0.054523   0.086741   \n1767         843       0      0   0.000000   0.000000   0.000000   0.000000   \n1768         267     474  53342   0.059072   0.059072   0.059072   0.084388   \n1769        1514     683  74408   0.090776   0.016105   0.042460   0.058565   \n\n      frac_aa_F  frac_aa_G  frac_aa_H  ...  Dh_functionalized  Shaken  \\\n0      0.034653   0.074257   0.014851  ...                149       1   \n1      0.034106   0.043656   0.016371  ...                282       1   \n2      0.038835   0.019417   0.009709  ...                229       1   \n3      0.060291   0.056133   0.022869  ...                282       1   \n4      0.049505   0.034653   0.024752  ...                291       1   \n...         ...        ...        ...  ...                ...     ...   \n1765   0.046400   0.070400   0.040000  ...                282       1   \n1766   0.024783   0.064436   0.012392  ...                151       1   \n1767   0.000000   0.000000   0.000000  ...                218       1   \n1768   0.046414   0.029536   0.018987  ...                226       1   \n1769   0.023426   0.070278   0.026354  ...                609       1   \n\n      Centrifuged  ProteinID  NP_incubation Concentration (mg/mL)  \\\n0               0          1                                    5   \n1               0          1                                    5   \n2               0          1                                    5   \n3               0          1                                    5   \n4               0          1                                    5   \n...           ...        ...                                  ...   \n1765            0          1                                    5   \n1766            0          1                                    5   \n1767            0          1                                    5   \n1768            0          1                                    5   \n1769            0          1                                    5   \n\n      Incubation Concentration (mg/ml)  Corona_Concentration (ug/mg)  \\\n0                                    4                     19.211444   \n1                                    4                     52.085492   \n2                                    4                     24.697213   \n3                                    4                     52.085492   \n4                                   40                    115.659641   \n...                                ...                           ...   \n1765                                 4                     52.085492   \n1766                                40                     35.981901   \n1767                                40                     73.474471   \n1768                                 4                    122.773587   \n1769                                40                     25.108958   \n\n      Incubation Time (minutes)  Temperature  NPUNID  \n0                            30           25      19  \n1                            30           25      22  \n2                            30           25      20  \n3                            30           25      22  \n4                            30           25      32  \n...                         ...          ...     ...  \n1765                         30           25      22  \n1766                         30           25      28  \n1767                         30           25      26  \n1768                         30           25      21  \n1769                         30           25      31  \n\n[1770 rows x 96 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Length</th>\n      <th>Mass</th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_H</th>\n      <th>...</th>\n      <th>Dh_functionalized</th>\n      <th>Shaken</th>\n      <th>Centrifuged</th>\n      <th>ProteinID</th>\n      <th>NP_incubation Concentration (mg/mL)</th>\n      <th>Incubation Concentration (mg/ml)</th>\n      <th>Corona_Concentration (ug/mg)</th>\n      <th>Incubation Time (minutes)</th>\n      <th>Temperature</th>\n      <th>NPUNID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18</td>\n      <td>202</td>\n      <td>22144</td>\n      <td>0.103960</td>\n      <td>0.039604</td>\n      <td>0.044554</td>\n      <td>0.079208</td>\n      <td>0.034653</td>\n      <td>0.074257</td>\n      <td>0.014851</td>\n      <td>...</td>\n      <td>149</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>19.211444</td>\n      <td>30</td>\n      <td>25</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>435</td>\n      <td>733</td>\n      <td>84731</td>\n      <td>0.047749</td>\n      <td>0.009550</td>\n      <td>0.075034</td>\n      <td>0.132333</td>\n      <td>0.034106</td>\n      <td>0.043656</td>\n      <td>0.016371</td>\n      <td>...</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>217</td>\n      <td>103</td>\n      <td>12217</td>\n      <td>0.126214</td>\n      <td>0.029126</td>\n      <td>0.029126</td>\n      <td>0.087379</td>\n      <td>0.038835</td>\n      <td>0.019417</td>\n      <td>0.009709</td>\n      <td>...</td>\n      <td>229</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>24.697213</td>\n      <td>30</td>\n      <td>25</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>412</td>\n      <td>481</td>\n      <td>53698</td>\n      <td>0.056133</td>\n      <td>0.008316</td>\n      <td>0.049896</td>\n      <td>0.049896</td>\n      <td>0.060291</td>\n      <td>0.056133</td>\n      <td>0.022869</td>\n      <td>...</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1589</td>\n      <td>202</td>\n      <td>23182</td>\n      <td>0.089109</td>\n      <td>0.024752</td>\n      <td>0.049505</td>\n      <td>0.099010</td>\n      <td>0.049505</td>\n      <td>0.034653</td>\n      <td>0.024752</td>\n      <td>...</td>\n      <td>291</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>115.659641</td>\n      <td>30</td>\n      <td>25</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>398</td>\n      <td>625</td>\n      <td>69872</td>\n      <td>0.049600</td>\n      <td>0.057600</td>\n      <td>0.043200</td>\n      <td>0.049600</td>\n      <td>0.046400</td>\n      <td>0.070400</td>\n      <td>0.040000</td>\n      <td>...</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>1766</th>\n      <td>1146</td>\n      <td>807</td>\n      <td>90976</td>\n      <td>0.052045</td>\n      <td>0.057001</td>\n      <td>0.054523</td>\n      <td>0.086741</td>\n      <td>0.024783</td>\n      <td>0.064436</td>\n      <td>0.012392</td>\n      <td>...</td>\n      <td>151</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>35.981901</td>\n      <td>30</td>\n      <td>25</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1767</th>\n      <td>843</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>1768</th>\n      <td>267</td>\n      <td>474</td>\n      <td>53342</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.084388</td>\n      <td>0.046414</td>\n      <td>0.029536</td>\n      <td>0.018987</td>\n      <td>...</td>\n      <td>226</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>122.773587</td>\n      <td>30</td>\n      <td>25</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1769</th>\n      <td>1514</td>\n      <td>683</td>\n      <td>74408</td>\n      <td>0.090776</td>\n      <td>0.016105</td>\n      <td>0.042460</td>\n      <td>0.058565</td>\n      <td>0.023426</td>\n      <td>0.070278</td>\n      <td>0.026354</td>\n      <td>...</td>\n      <td>609</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>25.108958</td>\n      <td>30</td>\n      <td>25</td>\n      <td>31</td>\n    </tr>\n  </tbody>\n</table>\n<p>1770 rows Ã— 96 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label_abund_df'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3620\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'label_abund_df'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m df_filepath\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput_data/Save_files/df_RFECV_noCon_drop30\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mzeros.xlsx\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      3\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_excel(df_filepath, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m labels\u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel_abund_df\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m      5\u001B[0m id_col\u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNPIDs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m      6\u001B[0m df\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel_abund_df\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNPIDs\u001B[39m\u001B[38;5;124m'\u001B[39m],inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3623\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3625\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3626\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3627\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'label_abund_df'"
     ]
    }
   ],
   "source": [
    "NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "df_filepath='Input_data/Save_files/df_RFECV_noCon_drop30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels= df['label_abund_df'].copy()\n",
    "id_col= df['NPIDs'].copy()\n",
    "df.drop(columns=['label_abund_df','NPIDs'],inplace=True)\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "scores=remove_one_predict_mse(NPUNID_list,model,labels,id_col,df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x=[1,2,3,4]\n",
    "y=[1,2,3,4]\n",
    "y2=[2,4,6,8]\n",
    "y3=[4,8,12,16]\n",
    "\n",
    "plt.plot(x, y, color='tab:blue', marker='o',label='pearson')\n",
    "plt.plot(x, y2, color='tab:red', marker='s',label='R2')\n",
    "plt.plot(x, y3, color='tab:green', marker='^',label='mse')\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Score as a function of Feat Drop\\n'+id)\n",
    "plt.savefig('Output_data/feat_drop_cumulative' + id + '.png', bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'adjusted_mutual_info_score',\n 'adjusted_rand_score',\n 'average_precision',\n 'balanced_accuracy',\n 'completeness_score',\n 'explained_variance',\n 'f1',\n 'f1_macro',\n 'f1_micro',\n 'f1_samples',\n 'f1_weighted',\n 'fowlkes_mallows_score',\n 'homogeneity_score',\n 'jaccard',\n 'jaccard_macro',\n 'jaccard_micro',\n 'jaccard_samples',\n 'jaccard_weighted',\n 'matthews_corrcoef',\n 'max_error',\n 'mutual_info_score',\n 'neg_brier_score',\n 'neg_log_loss',\n 'neg_mean_absolute_error',\n 'neg_mean_absolute_percentage_error',\n 'neg_mean_gamma_deviance',\n 'neg_mean_poisson_deviance',\n 'neg_mean_squared_error',\n 'neg_mean_squared_log_error',\n 'neg_median_absolute_error',\n 'neg_root_mean_squared_error',\n 'normalized_mutual_info_score',\n 'precision',\n 'precision_macro',\n 'precision_micro',\n 'precision_samples',\n 'precision_weighted',\n 'r2',\n 'rand_score',\n 'recall',\n 'recall_macro',\n 'recall_micro',\n 'recall_samples',\n 'recall_weighted',\n 'roc_auc',\n 'roc_auc_ovo',\n 'roc_auc_ovo_weighted',\n 'roc_auc_ovr',\n 'roc_auc_ovr_weighted',\n 'top_k_accuracy',\n 'v_measure_score']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Get total number of rows\n",
    "total_count = len(df)\n",
    "df = df.rename(columns=lambda x: x.replace('/', ''))\n",
    "# Loop over each column of the dataframe\n",
    "for col in df.columns:\n",
    "    # Create a histogram of the current column, with bins=10\n",
    "    data=df[col]\n",
    "    plt.hist(data, weights=np.ones_like(data) / len(data))\n",
    "\n",
    "    # Set the title of the histogram to the column name and percent of total population\n",
    "    plt.title(str(col))\n",
    "    plt.savefig('Output_data/{}.png'.format(str(col)))\n",
    "    plt.close()\n",
    "    # Show the histogram\n",
    "    # plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'adjusted_mutual_info_score',\n 'adjusted_rand_score',\n 'average_precision',\n 'balanced_accuracy',\n 'completeness_score',\n 'explained_variance',\n 'f1',\n 'f1_macro',\n 'f1_micro',\n 'f1_samples',\n 'f1_weighted',\n 'fowlkes_mallows_score',\n 'homogeneity_score',\n 'jaccard',\n 'jaccard_macro',\n 'jaccard_micro',\n 'jaccard_samples',\n 'jaccard_weighted',\n 'matthews_corrcoef',\n 'max_error',\n 'mutual_info_score',\n 'neg_brier_score',\n 'neg_log_loss',\n 'neg_mean_absolute_error',\n 'neg_mean_absolute_percentage_error',\n 'neg_mean_gamma_deviance',\n 'neg_mean_poisson_deviance',\n 'neg_mean_squared_error',\n 'neg_mean_squared_log_error',\n 'neg_median_absolute_error',\n 'neg_root_mean_squared_error',\n 'normalized_mutual_info_score',\n 'precision',\n 'precision_macro',\n 'precision_micro',\n 'precision_samples',\n 'precision_weighted',\n 'r2',\n 'rand_score',\n 'recall',\n 'recall_macro',\n 'recall_micro',\n 'recall_samples',\n 'recall_weighted',\n 'roc_auc',\n 'roc_auc_ovo',\n 'roc_auc_ovo_weighted',\n 'roc_auc_ovr',\n 'roc_auc_ovr_weighted',\n 'top_k_accuracy',\n 'v_measure_score']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   pearson_mean  pearson_std  R2_mean  R2_std  MSE_mean  MSE_std  \\\n0            10           23        4       3         1        2   \n\n   Number of Features  hi  \n0                  10  hi  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pearson_mean</th>\n      <th>pearson_std</th>\n      <th>R2_mean</th>\n      <th>R2_std</th>\n      <th>MSE_mean</th>\n      <th>MSE_std</th>\n      <th>Number of Features</th>\n      <th>hi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>23</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>10</td>\n      <td>hi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_mean=10\n",
    "pearson_std=23\n",
    "R2_mean=4\n",
    "R2_std=3\n",
    "MSE_mean=1\n",
    "MSE_std=2\n",
    "id='hi'\n",
    "data=[[pearson_mean,pearson_std,R2_mean,R2_std,MSE_mean,MSE_std,10,id]]\n",
    "scores=pd.DataFrame(data,columns=['pearson_mean','pearson_std','R2_mean','R2_std','MSE_mean','MSE_std','Number of Features',str(id)])\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        rf_model = RandomForestRegressor()\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
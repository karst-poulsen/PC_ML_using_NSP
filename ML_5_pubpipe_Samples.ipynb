{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NormBALFsamples.xlsx', 'Norm_Intensity _all20230403.xlsx']\n"
     ]
    }
   ],
   "source": [
    "#Editable Variables\n",
    "multi_files=False #set to false if you just want to set one  prot_abund_file\n",
    "in_dir=\"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file='Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file='Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "model=RandomForestRegressor(n_estimators=150)\n",
    "# take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "files = os.listdir(in_dir)\n",
    "print(files)\n",
    "if multi_files == True:\n",
    "    for i,f in enumerate(files):\n",
    "        print(i)\n",
    "        if i==0:\n",
    "            raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "            # print(raw_MS_data)\n",
    "            raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "            print('BALF',raw_MS_data.shape)\n",
    "        else:\n",
    "            print(i)\n",
    "            temp = pd.read_excel(in_dir+f,header=0)\n",
    "            temp = pd.melt(temp, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "            print('Bovine',temp.shape)\n",
    "            # print(temp)\n",
    "            # print(temp)\n",
    "            raw_MS_data2=pd.concat([raw_MS_data,temp])\n",
    "            print('merge',raw_MS_data2.shape)\n",
    "            # print('did it')\n",
    "else:\n",
    "    raw_MS_data2=pd.read_excel(prot_abund_file,header=0)\n",
    "# raw_MS_data2\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset\n",
    "# raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape beofre dropping rows (568, 56)\n",
      "shape after dropping rows (63, 56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmp95\\AppData\\Local\\Temp\\ipykernel_1396\\4265355859.py:102: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  plt.colorbar()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer ran successfully\n",
      "Scramble Scoring ran successfully\n",
      "feat drop ran successfully\n",
      "shape beofre dropping rows (568, 56)\n",
      "shape after dropping rows (44, 56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmp95\\AppData\\Local\\Temp\\ipykernel_1396\\4265355859.py:102: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  plt.colorbar()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer ran successfully\n",
      "Scramble Scoring ran successfully\n",
      "feat drop ran successfully\n",
      "shape beofre dropping rows (568, 56)\n",
      "shape after dropping rows (37, 56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmp95\\AppData\\Local\\Temp\\ipykernel_1396\\4265355859.py:102: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  plt.colorbar()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer ran successfully\n",
      "Scramble Scoring ran successfully\n",
      "feat drop ran successfully\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow=[6,3,2]\n",
    "for z in zerosperrow:\n",
    "    multi_files=False #set to false if you just want to set one  prot_abund_file\n",
    "    in_dir=\"Input_data/Proteomic data/Abundance2/\"\n",
    "    prot_abund_file='Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "    NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "    controls_file='Input_data/Proteomic data/controls_combined.xlsx'\n",
    "    uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "    NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "    id='nps_noconabund_droprowswith'+str(z)+'zeros'\n",
    "    RFE_Feats=15\n",
    "    model=RandomForestRegressor(n_estimators=150)\n",
    "    # take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "    # melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "    files = os.listdir(in_dir)\n",
    "    if multi_files == True:\n",
    "        for i,f in enumerate(files):\n",
    "            if i==0:\n",
    "                raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "                # print(raw_MS_data)\n",
    "                raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp = pd.read_excel(in_dir+f,header=0)\n",
    "                temp = pd.melt(temp, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "                raw_MS_data=pd.concat([raw_MS_data,temp])\n",
    "\n",
    "    else:\n",
    "        raw_MS_data=pd.read_excel(prot_abund_file,header=0)\n",
    "        print('shape beofre dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data=raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >=z].index)\n",
    "        print('shape after dropping rows',raw_MS_data.shape)\n",
    "        raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "    #remove prots that were added due to merge\n",
    "    raw_MS_data=raw_MS_data.dropna()\n",
    "    ###Bring in controls (MS data for serums)##\n",
    "    controls=pd.read_excel(controls_file,header=0)\n",
    "    MS_data_controls = pd.merge(raw_MS_data,controls,how='left', on='Entry')\n",
    "    ###Bring in Uniprot_data,NSPdata and NP data##\n",
    "    uniprot_dat=pd.read_excel(uniprot_filepath,header=0)\n",
    "    NSP_data=pd.read_excel(NSPfilePath)\n",
    "    ###Bring in NP data and merge to get complete NP dataset###\n",
    "    NPUNdata=pd.read_excel(NP_filepath,header=0,sheet_name='NPUNID')\n",
    "    NPprop=pd.read_excel(NP_filepath,header=0,sheet_name='NP_Props')\n",
    "    NPdata=pd.merge(NPUNdata,NPprop,how=\"left\",on='NPID')\n",
    "    NPdata.dropna(inplace=True)\n",
    "    #calculate Enrichment\n",
    "    #####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "    # MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "    # MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "    #keep abundance Controls\n",
    "    # MS_data=MS_data_controls.drop(columns=['Abundance'])\n",
    "    raw_prop_data=pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left',on='Entry')\n",
    "    Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']),how='left', on='Entry') #merges netsurfp features and biopython features\n",
    "    Protein_data_complete.fillna(0,inplace=True)\n",
    "    #creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "    for df in [Protein_data_complete]:\n",
    "        for col in ['asa_sum']:\n",
    "            df[col+'_normalized'] = df[col] / df['Mass']\n",
    "\n",
    "    data_complete= pd.merge(Protein_data_complete,NPdata,how='inner', on='Sample_num')\n",
    "    data_complete.drop(columns=['notes','Notes','NPUNID'],inplace=True)\n",
    "    data_complete.fillna(0,inplace=True)\n",
    "    data_complete= data_complete.replace([-np.inf],'-12')\n",
    "    data_complete=data_complete.replace([np.inf],'12')\n",
    "    #create ordinal variables\n",
    "    # data_complete2=pd.get_dummies(data_complete, columns=['Core Material','Surface_Ligand'])\n",
    "    le=LabelEncoder()\n",
    "    data_complete['Core Material']=le.fit_transform(data_complete['Core Material'])\n",
    "    data_complete['Surface_Ligand']=le.fit_transform(data_complete['Surface_Ligand'])\n",
    "\n",
    "    #set labels (what we are trying to predict) as Enrichment column\n",
    "    # labels=data_complete['Enrichment'].copy()\n",
    "    label_abund=np.ravel(data_complete['Abundance'].copy())\n",
    "    label_abund_df=pd.DataFrame(label_abund)\n",
    "    # label_enrich=np.ravel(data_complete['Enrichment'].copy())\n",
    "    #make it one dimenisional\n",
    "    #drop qualitative, not neccessary, and label columns\n",
    "    #create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "    to_drop=data_complete.filter(like='total_exposed_')\n",
    "    data_complete.drop(columns=to_drop,inplace=True)\n",
    "    df=data_complete.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n",
    "    df.drop(columns=['Abundance_Controls'],inplace=True)\n",
    "\n",
    "    # df.to_excel(\"Input_data/Save_files/df_\"+id+\".xlsx\")\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "\n",
    "\n",
    "    #Run PCA to seee how data differentiates#\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(df)\n",
    "    pca= PCA(n_components=5)\n",
    "    x_pca=pca.fit_transform(X_std)\n",
    "    plt.scatter(x_pca[:, 0], x_pca[:, 1], c=label_abund, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.savefig('Output_data/PCA'+id+'.png')\n",
    "    plt.close('all')\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    step=2\n",
    "    estimator=RandomForestRegressor(n_estimators=100)\n",
    "    selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "    selector = selector.fit(df, label_abund)\n",
    "    selector.support_\n",
    "    ranking=selector.ranking_\n",
    "    feat_list = selector.get_feature_names_out()\n",
    "    df=df[feat_list]\n",
    "    feat_list\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    # from sklearn.model_selection import KFold\n",
    "    # id2='dropped_controlAbundance'\n",
    "    # step=2\n",
    "    # min_feats=5\n",
    "    # cv= KFold(n_splits=10)\n",
    "    # estimator=RandomForestRegressor(n_estimators=100)\n",
    "    # # estimator=Lasso(alpha=.05)\n",
    "    # selector = RFECV(estimator=estimator, cv=cv, scoring='neg_mean_squared_error', min_features_to_select=min_feats, step=step)\n",
    "    # selector = selector.fit(df_rfe, label_abund)\n",
    "    # selector.support_\n",
    "    # feat_list2 = selector.get_feature_names_out()\n",
    "    # selected_features= df_rfe.columns[selector.support_]\n",
    "    # df_2=df[feat_list2]\n",
    "    # df_2.to_excel(\"Input_data/Save_files/df_RFECV\"+id+id2+\".xlsx\")\n",
    "    # rfecv_df=pd.DataFrame(selector.cv_results_)\n",
    "    # rfecv_df.to_excel(\"Output_data/RFECV_results\"+id+id2+\".xlsx\")\n",
    "    # # label_abund_df.to_excel(\"Input_data/Save_files/label_abund_all.xlsx\")\n",
    "    # n_scores = len(selector.cv_results_[\"mean_test_score\"])\n",
    "    # plt.figure()\n",
    "    # plt.xlabel(\"Number of features selected\")\n",
    "    # plt.ylabel(\"Mean test accuracy\")\n",
    "    # x=range(1, n_scores + 1)\n",
    "    # y=selector.cv_results_[\"mean_test_score\"]\n",
    "    # err=selector.cv_results_[\"std_test_score\"]\n",
    "    # plt.plot(x,y,'k-')\n",
    "    # plt.fill_between(x,y-err,y+err)\n",
    "    # plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "    # plt.savefig('Output_data/RFECV'+id+'.png')\n",
    "\n",
    "    #Quality control\n",
    "    scorer(df,label_abund,model,id,10)\n",
    "    scram_score(df,label_abund,model,id,0.2)\n",
    "    feat_drop(df,label_abund,model,id,0.2)\n",
    "print('done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
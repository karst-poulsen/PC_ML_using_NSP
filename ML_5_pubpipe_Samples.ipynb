{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before dropping rows (474, 14)\n",
      "shape after dropping rows (27, 14)\n",
      "shape before dropping rows (351, 3)\n",
      "shape after dropping rows (351, 3)\n",
      "final shape after melt (6841, 3)\n",
      "number of zeros in the dataset: 752\n",
      "shape before dropping rows (474, 14)\n",
      "shape after dropping rows (27, 14)\n",
      "shape before dropping rows (351, 3)\n",
      "shape after dropping rows (351, 3)\n",
      "final shape after melt (6841, 3)\n",
      "number of zeros in the dataset: 752\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 23>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     67\u001B[0m MS_data_controls \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mmerge(raw_MS_data, controls, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m, on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEntry\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m###Bring in Uniprot_data,NSPdata and NP data##\u001B[39;00m\n\u001B[1;32m---> 69\u001B[0m uniprot_dat \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[43muniprot_filepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     70\u001B[0m NSP_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_excel(NSPfilePath)\n\u001B[0;32m     71\u001B[0m \u001B[38;5;66;03m###Bring in NP data and merge to get complete NP dataset###\u001B[39;00m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:465\u001B[0m, in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    460\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    461\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    462\u001B[0m     )\n\u001B[0;32m    464\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 465\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m        \u001B[49m\u001B[43msheet_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msheet_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    468\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m        \u001B[49m\u001B[43musecols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43musecols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m        \u001B[49m\u001B[43msqueeze\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconverters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconverters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrue_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrue_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    475\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfalse_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfalse_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskiprows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskiprows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnrows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m        \u001B[49m\u001B[43mna_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_default_na\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_default_na\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mna_filter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_filter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdate_parser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_parser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthousands\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthousands\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecimal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecimal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    486\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskipfooter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipfooter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_float\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_float\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmangle_dupe_cols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmangle_dupe_cols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    492\u001B[0m     \u001B[38;5;66;03m# make sure to close opened file handles\u001B[39;00m\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m should_close:\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1458\u001B[0m, in \u001B[0;36mExcelFile.parse\u001B[1;34m(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001B[0m\n\u001B[0;32m   1424\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse\u001B[39m(\n\u001B[0;32m   1425\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1426\u001B[0m     sheet_name: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds,\n\u001B[0;32m   1446\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, DataFrame] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mint\u001B[39m, DataFrame]:\n\u001B[0;32m   1447\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1448\u001B[0m \u001B[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001B[39;00m\n\u001B[0;32m   1449\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;124;03m        DataFrame from the passed in Excel file.\u001B[39;00m\n\u001B[0;32m   1457\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader\u001B[38;5;241m.\u001B[39mparse(\n\u001B[0;32m   1459\u001B[0m         sheet_name\u001B[38;5;241m=\u001B[39msheet_name,\n\u001B[0;32m   1460\u001B[0m         header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[0;32m   1461\u001B[0m         names\u001B[38;5;241m=\u001B[39mnames,\n\u001B[0;32m   1462\u001B[0m         index_col\u001B[38;5;241m=\u001B[39mindex_col,\n\u001B[0;32m   1463\u001B[0m         usecols\u001B[38;5;241m=\u001B[39musecols,\n\u001B[0;32m   1464\u001B[0m         squeeze\u001B[38;5;241m=\u001B[39msqueeze,\n\u001B[0;32m   1465\u001B[0m         converters\u001B[38;5;241m=\u001B[39mconverters,\n\u001B[0;32m   1466\u001B[0m         true_values\u001B[38;5;241m=\u001B[39mtrue_values,\n\u001B[0;32m   1467\u001B[0m         false_values\u001B[38;5;241m=\u001B[39mfalse_values,\n\u001B[0;32m   1468\u001B[0m         skiprows\u001B[38;5;241m=\u001B[39mskiprows,\n\u001B[0;32m   1469\u001B[0m         nrows\u001B[38;5;241m=\u001B[39mnrows,\n\u001B[0;32m   1470\u001B[0m         na_values\u001B[38;5;241m=\u001B[39mna_values,\n\u001B[0;32m   1471\u001B[0m         parse_dates\u001B[38;5;241m=\u001B[39mparse_dates,\n\u001B[0;32m   1472\u001B[0m         date_parser\u001B[38;5;241m=\u001B[39mdate_parser,\n\u001B[0;32m   1473\u001B[0m         thousands\u001B[38;5;241m=\u001B[39mthousands,\n\u001B[0;32m   1474\u001B[0m         comment\u001B[38;5;241m=\u001B[39mcomment,\n\u001B[0;32m   1475\u001B[0m         skipfooter\u001B[38;5;241m=\u001B[39mskipfooter,\n\u001B[0;32m   1476\u001B[0m         convert_float\u001B[38;5;241m=\u001B[39mconvert_float,\n\u001B[0;32m   1477\u001B[0m         mangle_dupe_cols\u001B[38;5;241m=\u001B[39mmangle_dupe_cols,\n\u001B[0;32m   1478\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds,\n\u001B[0;32m   1479\u001B[0m     )\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:638\u001B[0m, in \u001B[0;36mBaseExcelReader.parse\u001B[1;34m(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001B[0m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# assume an integer if not a string\u001B[39;00m\n\u001B[0;32m    636\u001B[0m     sheet \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_sheet_by_index(asheetname)\n\u001B[1;32m--> 638\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_sheet_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43msheet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_float\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(sheet, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclose\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    640\u001B[0m     \u001B[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001B[39;00m\n\u001B[0;32m    641\u001B[0m     sheet\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:575\u001B[0m, in \u001B[0;36mOpenpyxlReader.get_sheet_data\u001B[1;34m(self, sheet, convert_float)\u001B[0m\n\u001B[0;32m    573\u001B[0m data: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[Scalar]] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    574\u001B[0m last_row_with_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 575\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row_number, row \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sheet\u001B[38;5;241m.\u001B[39mrows):\n\u001B[0;32m    576\u001B[0m     converted_row \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_cell(cell, convert_float) \u001B[38;5;28;01mfor\u001B[39;00m cell \u001B[38;5;129;01min\u001B[39;00m row]\n\u001B[0;32m    577\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m converted_row \u001B[38;5;129;01mand\u001B[39;00m converted_row[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    578\u001B[0m         \u001B[38;5;66;03m# trim trailing empty elements\u001B[39;00m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:79\u001B[0m, in \u001B[0;36mReadOnlyWorksheet._cells_by_row\u001B[1;34m(self, min_col, min_row, max_col, max_row, values_only)\u001B[0m\n\u001B[0;32m     75\u001B[0m src \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_source()\n\u001B[0;32m     76\u001B[0m parser \u001B[38;5;241m=\u001B[39m WorkSheetParser(src, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shared_strings,\n\u001B[0;32m     77\u001B[0m                          data_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparent\u001B[38;5;241m.\u001B[39mdata_only, epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparent\u001B[38;5;241m.\u001B[39mepoch,\n\u001B[0;32m     78\u001B[0m                          date_formats\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparent\u001B[38;5;241m.\u001B[39m_date_formats)\n\u001B[1;32m---> 79\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, row \u001B[38;5;129;01min\u001B[39;00m parser\u001B[38;5;241m.\u001B[39mparse():\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_row \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m idx \u001B[38;5;241m>\u001B[39m max_row:\n\u001B[0;32m     81\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:144\u001B[0m, in \u001B[0;36mWorkSheetParser.parse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    125\u001B[0m properties \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    126\u001B[0m     PRINT_TAG: (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprint_options\u001B[39m\u001B[38;5;124m'\u001B[39m, PrintOptions),\n\u001B[0;32m    127\u001B[0m     MARGINS_TAG: (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpage_margins\u001B[39m\u001B[38;5;124m'\u001B[39m, PageMargins),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    139\u001B[0m \n\u001B[0;32m    140\u001B[0m }\n\u001B[0;32m    142\u001B[0m it \u001B[38;5;241m=\u001B[39m iterparse(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource) \u001B[38;5;66;03m# add a finaliser to close the source when this becomes possible\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, element \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[0;32m    145\u001B[0m     tag_name \u001B[38;5;241m=\u001B[39m element\u001B[38;5;241m.\u001B[39mtag\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tag_name \u001B[38;5;129;01min\u001B[39;00m dispatcher:\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\xml\\etree\\ElementTree.py:1262\u001B[0m, in \u001B[0;36miterparse.<locals>.iterator\u001B[1;34m(source)\u001B[0m\n\u001B[0;32m   1260\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m pullparser\u001B[38;5;241m.\u001B[39mread_events()\n\u001B[0;32m   1261\u001B[0m \u001B[38;5;66;03m# load event buffer\u001B[39;00m\n\u001B[1;32m-> 1262\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43msource\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[0;32m   1264\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\zipfile.py:924\u001B[0m, in \u001B[0;36mZipExtFile.read\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m    922\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_offset \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    923\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m n \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eof:\n\u001B[1;32m--> 924\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    925\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(data):\n\u001B[0;32m    926\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_readbuffer \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\zipfile.py:1000\u001B[0m, in \u001B[0;36mZipExtFile._read1\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m    998\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compress_type \u001B[38;5;241m==\u001B[39m ZIP_DEFLATED:\n\u001B[0;32m    999\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(n, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMIN_READ_SIZE)\n\u001B[1;32m-> 1000\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decompressor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecompress\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1001\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eof \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39meof \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m                  \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compress_left \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m                  \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39munconsumed_tail)\n\u001B[0;32m   1004\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eof:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Large OG Pipeline # Move lower #\n",
    "\n",
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "\n",
    "scorings=['r2','neg_mean_squared_error','mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    id = 'Con_drop_' +str(z)+ '_30' + '%zeros'\n",
    "    # take files in_dir and combine then into a pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "    # melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "    files = os.listdir(in_dir)\n",
    "    if multi_files == True:\n",
    "        for i, f in enumerate(files):\n",
    "            if i == 0:\n",
    "                raw_MS_data = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = raw_MS_data.shape[1]\n",
    "                cutoff = int(zerosperrow * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                # print(raw_MS_data)\n",
    "                raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = temp.shape[1]\n",
    "                cutoff = int(zerosperrow * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                temp = temp.drop(temp[(temp == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                temp = pd.melt(temp, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "                raw_MS_data = pd.concat([raw_MS_data, temp])\n",
    "                print('final shape after melt', raw_MS_data.shape)\n",
    "                print('number of zeros in the dataset:',(raw_MS_data == 0).sum().sum())\n",
    "\n",
    "\n",
    "    else:\n",
    "        raw_MS_data = pd.read_excel(prot_abund_file, header=0)\n",
    "        cols = raw_MS_data.shape[1]\n",
    "        cutoff = int(zerosperrow * cols)\n",
    "        print('shape beofre dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "        print('shape after dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "    #remove prots that were added due to merge\n",
    "    raw_MS_data = raw_MS_data.dropna()\n",
    "    ###Bring in controls (MS data for serums)##\n",
    "    controls = pd.read_excel(controls_file, header=0)\n",
    "    MS_data_controls = pd.merge(raw_MS_data, controls, how='left', on='Entry')\n",
    "    ###Bring in Uniprot_data,NSPdata and NP data##\n",
    "    uniprot_dat = pd.read_excel(uniprot_filepath, header=0)\n",
    "    NSP_data = pd.read_excel(NSPfilePath)\n",
    "    ###Bring in NP data and merge to get complete NP dataset###\n",
    "    NPUNdata = pd.read_excel(NP_filepath, header=0, sheet_name='NPUNID')\n",
    "    NPprop = pd.read_excel(NP_filepath, header=0, sheet_name='NP_Props')\n",
    "    NPdata = pd.merge(NPUNdata, NPprop, how=\"left\", on='NPID')\n",
    "    NPdata.dropna(inplace=True)\n",
    "    #calculate Enrichment\n",
    "    #####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "    # MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "    # MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "    raw_prop_data = pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left', on='Entry')\n",
    "    Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']), how='left',\n",
    "                                     on='Entry')\n",
    "    #merges netsurfp features and biopython features\n",
    "    Protein_data_complete.fillna(0, inplace=True)\n",
    "    data_complete = pd.merge(Protein_data_complete, NPdata, how='inner', on='Sample_num')\n",
    "    data_complete.fillna(0, inplace=True)\n",
    "    #create ordinal variables for the core materials and surface ligands\n",
    "    le = LabelEncoder()\n",
    "    data_complete['Core Material'] = le.fit_transform(data_complete['Core Material'])\n",
    "    data_complete['Surface_Ligand'] = le.fit_transform(data_complete['Surface_Ligand'])\n",
    "    #shuffle data using sample\n",
    "    data_complete = data_complete.sample(frac=1)\n",
    "    #set labels (what we are trying to predict) as Abundance\n",
    "    label_abund_df = data_complete['Abundance'].copy()\n",
    "    label_abund = np.ravel(label_abund_df)\n",
    "    NPIDs=data_complete['NPUNID'].copy()\n",
    "    data_complete.drop(columns=['notes', 'Notes', 'NPUNID'], inplace=True)\n",
    "\n",
    "    #make it one dimenisional\n",
    "    #drop qualitative, not neccessary, and label columns\n",
    "    #create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "    to_drop = data_complete.filter(like='total_exposed_')\n",
    "    data_complete.drop(columns=to_drop, inplace=True)\n",
    "    df = data_complete.drop(\n",
    "        ['Entry', 'Abundance', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5',\n",
    "         'Raw_FileID'], axis=1)\n",
    "    if abund_controls == False:\n",
    "        df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "\n",
    "    df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "    df_out.to_excel(\"Input_data/Save_files/df_whole_\"+id+\".xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Large OG Pipeline # Move lower #\n",
    "\n",
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "\n",
    "scorings=['r2','neg_mean_squared_error','mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    id = 'Con_drop_' +str(z)+ '_30' + '%zeros'\n",
    "    # take files in_dir and combine then into a pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "    # melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "    files = os.listdir(in_dir)\n",
    "    if multi_files == True:\n",
    "        for i, f in enumerate(files):\n",
    "            if i == 0:\n",
    "                raw_MS_data = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = raw_MS_data.shape[1]\n",
    "                cutoff = int(zerosperrow * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                # print(raw_MS_data)\n",
    "                raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = temp.shape[1]\n",
    "                cutoff = int(zerosperrow * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                temp = temp.drop(temp[(temp == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                temp = pd.melt(temp, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "                raw_MS_data = pd.concat([raw_MS_data, temp])\n",
    "                print('final shape after melt', raw_MS_data.shape)\n",
    "                print('number of zeros in the dataset:',(raw_MS_data == 0).sum().sum())\n",
    "\n",
    "\n",
    "    else:\n",
    "        raw_MS_data = pd.read_excel(prot_abund_file, header=0)\n",
    "        cols = raw_MS_data.shape[1]\n",
    "        cutoff = int(zerosperrow * cols)\n",
    "        print('shape beofre dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "        print('shape after dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "    #remove prots that were added due to merge\n",
    "    raw_MS_data = raw_MS_data.dropna()\n",
    "    ###Bring in controls (MS data for serums)##\n",
    "    controls = pd.read_excel(controls_file, header=0)\n",
    "    MS_data_controls = pd.merge(raw_MS_data, controls, how='left', on='Entry')\n",
    "    ###Bring in Uniprot_data,NSPdata and NP data##\n",
    "    uniprot_dat = pd.read_excel(uniprot_filepath, header=0)\n",
    "    NSP_data = pd.read_excel(NSPfilePath)\n",
    "    ###Bring in NP data and merge to get complete NP dataset###\n",
    "    NPUNdata = pd.read_excel(NP_filepath, header=0, sheet_name='NPUNID')\n",
    "    NPprop = pd.read_excel(NP_filepath, header=0, sheet_name='NP_Props')\n",
    "    NPdata = pd.merge(NPUNdata, NPprop, how=\"left\", on='NPID')\n",
    "    NPdata.dropna(inplace=True)\n",
    "    #calculate Enrichment\n",
    "    #####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "    # MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "    # MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "    raw_prop_data = pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left', on='Entry')\n",
    "    Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']), how='left',\n",
    "                                     on='Entry')\n",
    "    #merges netsurfp features and biopython features\n",
    "    Protein_data_complete.fillna(0, inplace=True)\n",
    "    data_complete = pd.merge(Protein_data_complete, NPdata, how='inner', on='Sample_num')\n",
    "    data_complete.fillna(0, inplace=True)\n",
    "    #create ordinal variables for the core materials and surface ligands\n",
    "    le = LabelEncoder()\n",
    "    data_complete['Core Material'] = le.fit_transform(data_complete['Core Material'])\n",
    "    data_complete['Surface_Ligand'] = le.fit_transform(data_complete['Surface_Ligand'])\n",
    "    #shuffle data using sample\n",
    "    data_complete = data_complete.sample(frac=1)\n",
    "    #set labels (what we are trying to predict) as Abundance\n",
    "    label_abund_df = data_complete['Abundance'].copy()\n",
    "    label_abund = np.ravel(label_abund_df)\n",
    "    NPIDs=data_complete['NPUNID'].copy()\n",
    "    data_complete.drop(columns=['notes', 'Notes', 'NPUNID'], inplace=True)\n",
    "\n",
    "    #make it one dimenisional\n",
    "    #drop qualitative, not neccessary, and label columns\n",
    "    #create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "    to_drop = data_complete.filter(like='total_exposed_')\n",
    "    data_complete.drop(columns=to_drop, inplace=True)\n",
    "    df = data_complete.drop(\n",
    "        ['Entry', 'Abundance', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5',\n",
    "         'Raw_FileID'], axis=1)\n",
    "    if abund_controls == False:\n",
    "        df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "\n",
    "    df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "    df_out.to_excel(\"Input_data/Save_files/df_whole_\"+id+\".xlsx\")\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "\n",
    "    #Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Running Binary Classification systems\n",
    "\n",
    "abund_thresh=np.arange(0.01,1.0,0.01).tolist()\n",
    "df_filepath='Input_data/Save_files/df_whole_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "model=RandomForestClassifier()\n",
    "# id='RFR_AFilter_'+str(i)+'NoCon'\n",
    "# Create an empty pandas DataFrame to store the evaluation metrics\n",
    "eval_df = pd.DataFrame(columns=['Abund Thresh', 'F1', 'AUROC', 'Accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "# Iterate through multiple iterations of model training and testing\n",
    "for i in abund_thresh:\n",
    "    df_a=df.copy()\n",
    "    df_a['binary_target']= df_a['Abundance'].apply(lambda t: 1 if t>=i else 0)\n",
    "    labels_df= df_a['binary_target'].copy()\n",
    "    # id_col= df_a['NPUNID'].copy()\n",
    "    label_binary=np.ravel(labels_df)\n",
    "    df_a=df_a.drop(columns=['Abundance','NPUNID','binary_target']).copy()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df, label_binary, test_size=0.2, random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    # Append a new row to the eval_df DataFrame with the evaluation metrics for this iteration\n",
    "    eval_df = eval_df.append({'Abund Thresh': i,\n",
    "                              'F1': f1,\n",
    "                              'AUROC': auroc,\n",
    "                              'Accuracy': accuracy,\n",
    "                              'Precision': precision,\n",
    "                              'Recall': recall}, ignore_index=True)\n",
    "\n",
    "eval_df.to_excel('Output_data/AbundThresholdingRFC.xlsx', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###Filtering Abundance to see if i stick to higher abundance I get better performance##\n",
    "rfecv_ = False  #True_runs RFECV\n",
    "rfe_ = True     #True runs Recursive feature elimination\n",
    "abund_controls = False # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "df_filepath = 'Input_data/Save_files/df_whole_drop_30%zeros.xlsx'\n",
    "df_a = pd.read_excel(df_filepath, header=0)\n",
    "# scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 12\n",
    "Abund_filter = np.arange(0.0001,0.1001,0.001).tolist()\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "summary_tmp=[]\n",
    "for i in Abund_filter:\n",
    "    df= df_a[df_a['Abundance']>=i]\n",
    "    id='RFR_AFilter_'+str(i)+'NoCon'\n",
    "    labels_df= df['Abundance'].copy()\n",
    "    id_col= df['NPUNID'].copy()\n",
    "    label_abund=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    if abund_controls == False:\n",
    "        df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        # df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        # df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "    #Run Recursive feature elimination to remove down to RFE_Feats\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        selector = RFE(model, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        # df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        # df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "    tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    tmp2['Abundance Filter']=i\n",
    "\n",
    "    summary_tmp.append(tmp2)\n",
    "\n",
    "    summary=pd.concat(summary_tmp,axis=0)\n",
    "summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_a = pd.read_excel(df_filepath, header=0)\n",
    "# df=df_a[df_a['Abundance']>=0.01]\n",
    "# df_a['Abundance']=np.log10(df_a['Abundance']+1)\n",
    "df['Abundance']=np.log2(df['Abundance']+1)\n",
    "plt.hist(df['Abundance'],bins=25)\n",
    "plt.xlim(-2,2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_a = pd.read_excel(df_filepath, header=0)\n",
    "# df=df_a[df_a['Abundance']>=0.01]\n",
    "# df_a['Abundance']=np.log10(df_a['Abundance']+1)\n",
    "df['Abundance']=np.log2(df['Abundance']+1)\n",
    "plt.hist(df['Abundance'],bins=25)\n",
    "plt.xlim(-2,2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    df=pd.read_excel('Input_data/Save_files/df_whole_noCon_drop30%zeros.xlsx')\n",
    "    labels_df= df['label_abund_df'].copy()\n",
    "    id_col= df['NPIDs'].copy()\n",
    "    labels=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "    # Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #Quality control\n",
    "    scram_score(df, label_abund, model, id, 0.2)\n",
    "    feat_drop(df, label_abund, model, id, 0.2)\n",
    "    feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    tmp2['TotalZeros']=zeros\n",
    "    tmp2['Percent_zeros']=percent_zeros\n",
    "    summary_tmp.append(tmp2)\n",
    "    lasso_feature_selection(df, label_abund, id)\n",
    "    summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    df=pd.read_excel('Input_data/Save_files/df_whole_noCon_drop30%zeros.xlsx')\n",
    "    labels_df= df['label_abund_df'].copy()\n",
    "    id_col= df['NPIDs'].copy()\n",
    "    labels=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "    # Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([df, NPIDs,label_abund_df],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Drop a nanoparticle and then predict it###\n",
    "##NPs of interest##\n",
    "#easy to hard NPUNID 1,20,19,16,7,31,34,34\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "df.columns\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "scores=remove_one_predict_mse(NPUNID_list,model,labels,id_col,df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.concat([labels, id_col, df], axis=1)\n",
    "id_list=NPUNID_list\n",
    "# Initialize a list to hold MSE scores for each removed ID\n",
    "mse_scores = []\n",
    "\n",
    "# Loop through each ID in the list\n",
    "for id_to_remove in id_list:\n",
    "    # Remove the row with the current ID from the dataframe\n",
    "    removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "    df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "    # Split the remaining data into features and target\n",
    "    X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "    y_train = df[labels.name]\n",
    "\n",
    "    # Fit a random forest regression model to the training data\n",
    "    # model.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained model to predict on the removed ID\n",
    "    X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "    y_true = removed_row[labels.name]\n",
    "    print(X_test.shape)\n",
    "    print(y_true.shape)\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE score and add it to the list\n",
    "    # mse_score = mean_squared_error(y_true, y_pred)\n",
    "    # mse_scores.append(mse_score)\n",
    "    #\n",
    "    # Add the removed row back into the dataframe\n",
    "    df = pd.concat([df, removed_row], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    df=pd.read_excel('Input_data/Save_files/df_whole_noCon_drop30%zeros.xlsx')\n",
    "    labels_df= df['label_abund_df'].copy()\n",
    "    id_col= df['NPIDs'].copy()\n",
    "    labels=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "    # Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Drop a nanoparticle and then predict it###\n",
    "##NPs of interest##\n",
    "#easy to hard NPUNID 1,20,19,16,7,31,34,34\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "      Abundance_Controls  Length    Mass  frac_aa_A  frac_aa_C  frac_aa_D  \\\n0               0.000000     258   27890   0.073643   0.081395   0.046512   \n1               0.000100     441   48712   0.068027   0.011338   0.047619   \n2               0.002023     396   44471   0.050505   0.030303   0.058081   \n3               0.004530    2211  248983   0.050204   0.008593   0.065129   \n4               0.000000     258   27890   0.073643   0.081395   0.046512   \n...                  ...     ...     ...        ...        ...        ...   \n1765            0.139882     345   38252   0.057971   0.066667   0.028986   \n1766            0.343098     465   52347   0.064516   0.019355   0.045161   \n1767            0.000000     807   90976   0.052045   0.057001   0.054523   \n1768            1.120803     474   53342   0.059072   0.059072   0.059072   \n1769            0.009005    1463  138938   0.097744   0.012303   0.043746   \n\n      frac_aa_E  frac_aa_F  frac_aa_G  frac_aa_H  ...  Dh_core  \\\n0      0.077519   0.011628   0.100775   0.042636  ...      229   \n1      0.106576   0.018141   0.102041   0.013605  ...      229   \n2      0.047980   0.065657   0.070707   0.108586  ...      229   \n3      0.061511   0.034826   0.055631   0.027589  ...      229   \n4      0.077519   0.011628   0.100775   0.042636  ...      229   \n...         ...        ...        ...        ...  ...      ...   \n1765   0.057971   0.060870   0.069565   0.028986  ...      229   \n1766   0.075269   0.053763   0.049462   0.012903  ...      229   \n1767   0.086741   0.024783   0.064436   0.012392  ...      149   \n1768   0.084388   0.046414   0.029536   0.018987  ...      229   \n1769   0.051948   0.016405   0.265892   0.006152  ...      229   \n\n      Dh_functionalized  Shaken  Centrifuged  ProteinID  \\\n0                   316       1            0          1   \n1                   316       1            0          1   \n2                   218       1            0          1   \n3                   218       1            0          1   \n4                   282       1            0          1   \n...                 ...     ...          ...        ...   \n1765                291       1            0          1   \n1766                316       1            0          1   \n1767                226       1            0          1   \n1768                282       1            0          1   \n1769                609       1            0          1   \n\n      NP_incubation Concentration (mg/mL)  Incubation Concentration (mg/ml)  \\\n0                                       5                                 4   \n1                                       5                                 4   \n2                                       5                                40   \n3                                       5                                40   \n4                                       5                                 4   \n...                                   ...                               ...   \n1765                                    5                                40   \n1766                                    5                                40   \n1767                                    5                                 4   \n1768                                    5                                40   \n1769                                    5                                40   \n\n      Corona_Concentration (ug/mg)  Incubation Time (minutes)  Temperature  \n0                        27.929767                         30           25  \n1                        27.929767                         30           25  \n2                        73.474471                         30           25  \n3                        73.474471                         30           25  \n4                        52.085492                         30           25  \n...                            ...                        ...          ...  \n1765                    115.659641                         30           25  \n1766                     56.774264                         30           25  \n1767                    122.773587                         30           25  \n1768                    123.140432                         30           25  \n1769                     25.108958                         30           25  \n\n[1770 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Abundance_Controls</th>\n      <th>Length</th>\n      <th>Mass</th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_H</th>\n      <th>...</th>\n      <th>Dh_core</th>\n      <th>Dh_functionalized</th>\n      <th>Shaken</th>\n      <th>Centrifuged</th>\n      <th>ProteinID</th>\n      <th>NP_incubation Concentration (mg/mL)</th>\n      <th>Incubation Concentration (mg/ml)</th>\n      <th>Corona_Concentration (ug/mg)</th>\n      <th>Incubation Time (minutes)</th>\n      <th>Temperature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000100</td>\n      <td>441</td>\n      <td>48712</td>\n      <td>0.068027</td>\n      <td>0.011338</td>\n      <td>0.047619</td>\n      <td>0.106576</td>\n      <td>0.018141</td>\n      <td>0.102041</td>\n      <td>0.013605</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002023</td>\n      <td>396</td>\n      <td>44471</td>\n      <td>0.050505</td>\n      <td>0.030303</td>\n      <td>0.058081</td>\n      <td>0.047980</td>\n      <td>0.065657</td>\n      <td>0.070707</td>\n      <td>0.108586</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004530</td>\n      <td>2211</td>\n      <td>248983</td>\n      <td>0.050204</td>\n      <td>0.008593</td>\n      <td>0.065129</td>\n      <td>0.061511</td>\n      <td>0.034826</td>\n      <td>0.055631</td>\n      <td>0.027589</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>0.139882</td>\n      <td>345</td>\n      <td>38252</td>\n      <td>0.057971</td>\n      <td>0.066667</td>\n      <td>0.028986</td>\n      <td>0.057971</td>\n      <td>0.060870</td>\n      <td>0.069565</td>\n      <td>0.028986</td>\n      <td>...</td>\n      <td>229</td>\n      <td>291</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>115.659641</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1766</th>\n      <td>0.343098</td>\n      <td>465</td>\n      <td>52347</td>\n      <td>0.064516</td>\n      <td>0.019355</td>\n      <td>0.045161</td>\n      <td>0.075269</td>\n      <td>0.053763</td>\n      <td>0.049462</td>\n      <td>0.012903</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>56.774264</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1767</th>\n      <td>0.000000</td>\n      <td>807</td>\n      <td>90976</td>\n      <td>0.052045</td>\n      <td>0.057001</td>\n      <td>0.054523</td>\n      <td>0.086741</td>\n      <td>0.024783</td>\n      <td>0.064436</td>\n      <td>0.012392</td>\n      <td>...</td>\n      <td>149</td>\n      <td>226</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>122.773587</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1768</th>\n      <td>1.120803</td>\n      <td>474</td>\n      <td>53342</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.084388</td>\n      <td>0.046414</td>\n      <td>0.029536</td>\n      <td>0.018987</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>123.140432</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1769</th>\n      <td>0.009005</td>\n      <td>1463</td>\n      <td>138938</td>\n      <td>0.097744</td>\n      <td>0.012303</td>\n      <td>0.043746</td>\n      <td>0.051948</td>\n      <td>0.016405</td>\n      <td>0.265892</td>\n      <td>0.006152</td>\n      <td>...</td>\n      <td>229</td>\n      <td>609</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>25.108958</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n<p>1770 rows × 95 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Drop a nanoparticle and then predict it###\n",
    "##NPs of interest##\n",
    "#easy to hard NPUNID 1,20,19,16,7,31,34,34\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Abundance_Controls, Length, Mass, frac_aa_A, frac_aa_C, frac_aa_D, frac_aa_E, frac_aa_F, frac_aa_G, frac_aa_H, frac_aa_I, frac_aa_K, frac_aa_L, frac_aa_M, frac_aa_N, frac_aa_P, frac_aa_Q, frac_aa_R, frac_aa_S, frac_aa_T, frac_aa_V, frac_aa_W, frac_aa_Y, molecular_weight, aromaticity, instability_index, flexibility_mean, flexibility_std, flexibility_var, flexibility_max, flexibility_min, flexibility_median, isoelectric_point, secondary_structure_fraction_helix, secondary_structure_fraction_turn, secondary_structure_fraction_sheet, secondary_structure_fraction_disordered, gravy, fraction_exposed, fraction_buried, fraction_exposed_nonpolar_total, fraction_exposed_nonpolar_exposed, fraction_exposed_polar_total, fraction_exposed_polar_exposed, rsa_mean, rsa_median, rsa_std, asa_sum, fraction_exposed_exposed_A, fraction_exposed_exposed_C, fraction_exposed_exposed_D, fraction_exposed_exposed_E, fraction_exposed_exposed_F, fraction_exposed_exposed_G, fraction_exposed_exposed_H, fraction_exposed_exposed_I, fraction_exposed_exposed_K, fraction_exposed_exposed_L, fraction_exposed_exposed_M, fraction_exposed_exposed_N, fraction_exposed_exposed_P, fraction_exposed_exposed_Q, fraction_exposed_exposed_R, fraction_exposed_exposed_S, fraction_exposed_exposed_T, fraction_exposed_exposed_V, fraction_exposed_exposed_W, fraction_exposed_exposed_Y, nsp_secondary_structure_coil, nsp_secondary_structure_sheet, nsp_secondary_structure_helix, nsp_disordered, BatchID, Zeta Potential, Core Material, Ligand_Carboxylate, Ligand_BSA, Ligand_Amine, Ligand_Citrate, Ligand_PEG, Ligand_PEI, Ligand_PVP, Ligand_Au, Surface_Ligand, Dtem, Dh_core, Dh_functionalized, Shaken, Centrifuged, ProteinID, NP_incubation Concentration (mg/mL), Incubation Concentration (mg/ml), Corona_Concentration (ug/mg), Incubation Time (minutes), Temperature, NPUNID, Abundance]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 98 columns]\n"
     ]
    }
   ],
   "source": [
    "NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "df_filepath='Input_data/Save_files/df_whole_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "# df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Remove the row with the current ID from the dataframe\n",
    "removed_row = df.loc[df['NPUNID'] == NPUNID_list[0]].copy()\n",
    "print(removed_row)\n",
    "# df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "#\n",
    "# # Split the remaining data into features and target\n",
    "# X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "# y_train = df[labels.name]\n",
    "#\n",
    "# # Fit a random forest regression model to the training data\n",
    "# model.fit(X_train, y_train)\n",
    "#\n",
    "# # Use the trained model to predict on the removed ID\n",
    "# X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "# y_true = removed_row[labels.name]\n",
    "# y_pred = model.predict(X_test)\n",
    "#\n",
    "# # Calculate the MSE score and add it to the list\n",
    "# mse_score = mean_squared_error(y_true, y_pred)\n",
    "# mse_scores.append(mse_score)\n",
    "#\n",
    "# # Add the removed row back into the dataframe\n",
    "# df = pd.concat([df, removed_row], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 96)) while a minimum of 1 is required by RandomForestRegressor.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m df\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[0;32m      8\u001B[0m model\u001B[38;5;241m=\u001B[39mRandomForestRegressor(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m scores\u001B[38;5;241m=\u001B[39m\u001B[43mremove_one_predict_mse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNPUNID_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43mid_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36mremove_one_predict_mse\u001B[1;34m(id_list, model, labels, id_col, data)\u001B[0m\n\u001B[0;32m     32\u001B[0m X_test \u001B[38;5;241m=\u001B[39m removed_row\u001B[38;5;241m.\u001B[39mdrop([labels\u001B[38;5;241m.\u001B[39mname, id_col\u001B[38;5;241m.\u001B[39mname], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     33\u001B[0m y_true \u001B[38;5;241m=\u001B[39m removed_row[labels\u001B[38;5;241m.\u001B[39mname]\n\u001B[1;32m---> 34\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Calculate the MSE score and add it to the list\u001B[39;00m\n\u001B[0;32m     37\u001B[0m mse_score \u001B[38;5;241m=\u001B[39m mean_squared_error(y_true, y_pred)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:991\u001B[0m, in \u001B[0;36mForestRegressor.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    989\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    990\u001B[0m \u001B[38;5;66;03m# Check data\u001B[39;00m\n\u001B[1;32m--> 991\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_X_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    993\u001B[0m \u001B[38;5;66;03m# Assign chunk of trees to jobs\u001B[39;00m\n\u001B[0;32m    994\u001B[0m n_jobs, _, _ \u001B[38;5;241m=\u001B[39m _partition_estimators(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_estimators, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:605\u001B[0m, in \u001B[0;36mBaseForest._validate_X_predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    602\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    603\u001B[0m \u001B[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001B[39;00m\n\u001B[0;32m    604\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 605\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m issparse(X) \u001B[38;5;129;01mand\u001B[39;00m (X\u001B[38;5;241m.\u001B[39mindices\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc \u001B[38;5;129;01mor\u001B[39;00m X\u001B[38;5;241m.\u001B[39mindptr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc):\n\u001B[0;32m    607\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo support for np.int64 index based sparse matrices\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:577\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 577\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    578\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:909\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    907\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n\u001B[0;32m    908\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_samples \u001B[38;5;241m<\u001B[39m ensure_min_samples:\n\u001B[1;32m--> 909\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    910\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    911\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    912\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_samples, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_samples, context)\n\u001B[0;32m    913\u001B[0m         )\n\u001B[0;32m    915\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    916\u001B[0m     n_features \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: Found array with 0 sample(s) (shape=(0, 96)) while a minimum of 1 is required by RandomForestRegressor."
     ]
    }
   ],
   "source": [
    "NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "df.columns\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "scores=remove_one_predict_mse(NPUNID_list,model,labels,id_col,df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "      Abundance_Controls  Length    Mass  frac_aa_A  frac_aa_C  frac_aa_D  \\\n0               0.000000     258   27890   0.073643   0.081395   0.046512   \n1               0.000100     441   48712   0.068027   0.011338   0.047619   \n2               0.002023     396   44471   0.050505   0.030303   0.058081   \n3               0.004530    2211  248983   0.050204   0.008593   0.065129   \n4               0.000000     258   27890   0.073643   0.081395   0.046512   \n...                  ...     ...     ...        ...        ...        ...   \n1765            0.139882     345   38252   0.057971   0.066667   0.028986   \n1766            0.343098     465   52347   0.064516   0.019355   0.045161   \n1767            0.000000     807   90976   0.052045   0.057001   0.054523   \n1768            1.120803     474   53342   0.059072   0.059072   0.059072   \n1769            0.009005    1463  138938   0.097744   0.012303   0.043746   \n\n      frac_aa_E  frac_aa_F  frac_aa_G  frac_aa_H  ...  Dh_core  \\\n0      0.077519   0.011628   0.100775   0.042636  ...      229   \n1      0.106576   0.018141   0.102041   0.013605  ...      229   \n2      0.047980   0.065657   0.070707   0.108586  ...      229   \n3      0.061511   0.034826   0.055631   0.027589  ...      229   \n4      0.077519   0.011628   0.100775   0.042636  ...      229   \n...         ...        ...        ...        ...  ...      ...   \n1765   0.057971   0.060870   0.069565   0.028986  ...      229   \n1766   0.075269   0.053763   0.049462   0.012903  ...      229   \n1767   0.086741   0.024783   0.064436   0.012392  ...      149   \n1768   0.084388   0.046414   0.029536   0.018987  ...      229   \n1769   0.051948   0.016405   0.265892   0.006152  ...      229   \n\n      Dh_functionalized  Shaken  Centrifuged  ProteinID  \\\n0                   316       1            0          1   \n1                   316       1            0          1   \n2                   218       1            0          1   \n3                   218       1            0          1   \n4                   282       1            0          1   \n...                 ...     ...          ...        ...   \n1765                291       1            0          1   \n1766                316       1            0          1   \n1767                226       1            0          1   \n1768                282       1            0          1   \n1769                609       1            0          1   \n\n      NP_incubation Concentration (mg/mL)  Incubation Concentration (mg/ml)  \\\n0                                       5                                 4   \n1                                       5                                 4   \n2                                       5                                40   \n3                                       5                                40   \n4                                       5                                 4   \n...                                   ...                               ...   \n1765                                    5                                40   \n1766                                    5                                40   \n1767                                    5                                 4   \n1768                                    5                                40   \n1769                                    5                                40   \n\n      Corona_Concentration (ug/mg)  Incubation Time (minutes)  Temperature  \n0                        27.929767                         30           25  \n1                        27.929767                         30           25  \n2                        73.474471                         30           25  \n3                        73.474471                         30           25  \n4                        52.085492                         30           25  \n...                            ...                        ...          ...  \n1765                    115.659641                         30           25  \n1766                     56.774264                         30           25  \n1767                    122.773587                         30           25  \n1768                    123.140432                         30           25  \n1769                     25.108958                         30           25  \n\n[1770 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Abundance_Controls</th>\n      <th>Length</th>\n      <th>Mass</th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_H</th>\n      <th>...</th>\n      <th>Dh_core</th>\n      <th>Dh_functionalized</th>\n      <th>Shaken</th>\n      <th>Centrifuged</th>\n      <th>ProteinID</th>\n      <th>NP_incubation Concentration (mg/mL)</th>\n      <th>Incubation Concentration (mg/ml)</th>\n      <th>Corona_Concentration (ug/mg)</th>\n      <th>Incubation Time (minutes)</th>\n      <th>Temperature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000100</td>\n      <td>441</td>\n      <td>48712</td>\n      <td>0.068027</td>\n      <td>0.011338</td>\n      <td>0.047619</td>\n      <td>0.106576</td>\n      <td>0.018141</td>\n      <td>0.102041</td>\n      <td>0.013605</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002023</td>\n      <td>396</td>\n      <td>44471</td>\n      <td>0.050505</td>\n      <td>0.030303</td>\n      <td>0.058081</td>\n      <td>0.047980</td>\n      <td>0.065657</td>\n      <td>0.070707</td>\n      <td>0.108586</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004530</td>\n      <td>2211</td>\n      <td>248983</td>\n      <td>0.050204</td>\n      <td>0.008593</td>\n      <td>0.065129</td>\n      <td>0.061511</td>\n      <td>0.034826</td>\n      <td>0.055631</td>\n      <td>0.027589</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>0.139882</td>\n      <td>345</td>\n      <td>38252</td>\n      <td>0.057971</td>\n      <td>0.066667</td>\n      <td>0.028986</td>\n      <td>0.057971</td>\n      <td>0.060870</td>\n      <td>0.069565</td>\n      <td>0.028986</td>\n      <td>...</td>\n      <td>229</td>\n      <td>291</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>115.659641</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1766</th>\n      <td>0.343098</td>\n      <td>465</td>\n      <td>52347</td>\n      <td>0.064516</td>\n      <td>0.019355</td>\n      <td>0.045161</td>\n      <td>0.075269</td>\n      <td>0.053763</td>\n      <td>0.049462</td>\n      <td>0.012903</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>56.774264</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1767</th>\n      <td>0.000000</td>\n      <td>807</td>\n      <td>90976</td>\n      <td>0.052045</td>\n      <td>0.057001</td>\n      <td>0.054523</td>\n      <td>0.086741</td>\n      <td>0.024783</td>\n      <td>0.064436</td>\n      <td>0.012392</td>\n      <td>...</td>\n      <td>149</td>\n      <td>226</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>122.773587</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1768</th>\n      <td>1.120803</td>\n      <td>474</td>\n      <td>53342</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.084388</td>\n      <td>0.046414</td>\n      <td>0.029536</td>\n      <td>0.018987</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>123.140432</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1769</th>\n      <td>0.009005</td>\n      <td>1463</td>\n      <td>138938</td>\n      <td>0.097744</td>\n      <td>0.012303</td>\n      <td>0.043746</td>\n      <td>0.051948</td>\n      <td>0.016405</td>\n      <td>0.265892</td>\n      <td>0.006152</td>\n      <td>...</td>\n      <td>229</td>\n      <td>609</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>25.108958</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n<p>1770 rows × 95 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot index with multidimensional key",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Loop through each ID in the list\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m id_to_remove \u001B[38;5;129;01min\u001B[39;00m id_list:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Remove the row with the current ID from the dataframe\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m     removed_row \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mid_col\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mid_to_remove\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     10\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mloc[df[id_col\u001B[38;5;241m.\u001B[39mname] \u001B[38;5;241m!=\u001B[39m id_to_remove]\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# Split the remaining data into features and target\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:967\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    964\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    966\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[1;32m--> 967\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1189\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(labels, MultiIndex)):\n\u001B[0;32m   1188\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndim\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m-> 1189\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot index with multidimensional key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_iterable(key, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[0;32m   1193\u001B[0m \u001B[38;5;66;03m# nested tuple slicing\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot index with multidimensional key"
     ]
    }
   ],
   "source": [
    "df = pd.concat([labels, id_col, df], axis=1)\n",
    "id_list=NPUNID_list\n",
    "# Initialize a list to hold MSE scores for each removed ID\n",
    "mse_scores = []\n",
    "\n",
    "# Loop through each ID in the list\n",
    "for id_to_remove in id_list:\n",
    "    # Remove the row with the current ID from the dataframe\n",
    "    removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "    df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "    # Split the remaining data into features and target\n",
    "    X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "    y_train = df[labels.name]\n",
    "\n",
    "    # Fit a random forest regression model to the training data\n",
    "    # model.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained model to predict on the removed ID\n",
    "    X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "    y_true = removed_row[labels.name]\n",
    "    print(X_test.shape)\n",
    "    print(y_true.shape)\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE score and add it to the list\n",
    "    # mse_score = mean_squared_error(y_true, y_pred)\n",
    "    # mse_scores.append(mse_score)\n",
    "    #\n",
    "    # Add the removed row back into the dataframe\n",
    "    df = pd.concat([df, removed_row], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x=[1,2,3,4]\n",
    "y=[1,2,3,4]\n",
    "y2=[2,4,6,8]\n",
    "y3=[4,8,12,16]\n",
    "\n",
    "plt.plot(x, y, color='tab:blue', marker='o',label='pearson')\n",
    "plt.plot(x, y2, color='tab:red', marker='s',label='R2')\n",
    "plt.plot(x, y3, color='tab:green', marker='^',label='mse')\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Score as a function of Feat Drop\\n'+id)\n",
    "plt.savefig('Output_data/feat_drop_cumulative' + id + '.png', bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'adjusted_mutual_info_score',\n 'adjusted_rand_score',\n 'average_precision',\n 'balanced_accuracy',\n 'completeness_score',\n 'explained_variance',\n 'f1',\n 'f1_macro',\n 'f1_micro',\n 'f1_samples',\n 'f1_weighted',\n 'fowlkes_mallows_score',\n 'homogeneity_score',\n 'jaccard',\n 'jaccard_macro',\n 'jaccard_micro',\n 'jaccard_samples',\n 'jaccard_weighted',\n 'matthews_corrcoef',\n 'max_error',\n 'mutual_info_score',\n 'neg_brier_score',\n 'neg_log_loss',\n 'neg_mean_absolute_error',\n 'neg_mean_absolute_percentage_error',\n 'neg_mean_gamma_deviance',\n 'neg_mean_poisson_deviance',\n 'neg_mean_squared_error',\n 'neg_mean_squared_log_error',\n 'neg_median_absolute_error',\n 'neg_root_mean_squared_error',\n 'normalized_mutual_info_score',\n 'precision',\n 'precision_macro',\n 'precision_micro',\n 'precision_samples',\n 'precision_weighted',\n 'r2',\n 'rand_score',\n 'recall',\n 'recall_macro',\n 'recall_micro',\n 'recall_samples',\n 'recall_weighted',\n 'roc_auc',\n 'roc_auc_ovo',\n 'roc_auc_ovo_weighted',\n 'roc_auc_ovr',\n 'roc_auc_ovr_weighted',\n 'top_k_accuracy',\n 'v_measure_score']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Get total number of rows\n",
    "total_count = len(df)\n",
    "df = df.rename(columns=lambda x: x.replace('/', ''))\n",
    "# Loop over each column of the dataframe\n",
    "for col in df.columns:\n",
    "    # Create a histogram of the current column, with bins=10\n",
    "    data=df[col]\n",
    "    plt.hist(data, weights=np.ones_like(data) / len(data))\n",
    "\n",
    "    # Set the title of the histogram to the column name and percent of total population\n",
    "    plt.title(str(col))\n",
    "    plt.savefig('Output_data/{}.png'.format(str(col)))\n",
    "    plt.close()\n",
    "    # Show the histogram\n",
    "    # plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'adjusted_mutual_info_score',\n 'adjusted_rand_score',\n 'average_precision',\n 'balanced_accuracy',\n 'completeness_score',\n 'explained_variance',\n 'f1',\n 'f1_macro',\n 'f1_micro',\n 'f1_samples',\n 'f1_weighted',\n 'fowlkes_mallows_score',\n 'homogeneity_score',\n 'jaccard',\n 'jaccard_macro',\n 'jaccard_micro',\n 'jaccard_samples',\n 'jaccard_weighted',\n 'matthews_corrcoef',\n 'max_error',\n 'mutual_info_score',\n 'neg_brier_score',\n 'neg_log_loss',\n 'neg_mean_absolute_error',\n 'neg_mean_absolute_percentage_error',\n 'neg_mean_gamma_deviance',\n 'neg_mean_poisson_deviance',\n 'neg_mean_squared_error',\n 'neg_mean_squared_log_error',\n 'neg_median_absolute_error',\n 'neg_root_mean_squared_error',\n 'normalized_mutual_info_score',\n 'precision',\n 'precision_macro',\n 'precision_micro',\n 'precision_samples',\n 'precision_weighted',\n 'r2',\n 'rand_score',\n 'recall',\n 'recall_macro',\n 'recall_micro',\n 'recall_samples',\n 'recall_weighted',\n 'roc_auc',\n 'roc_auc_ovo',\n 'roc_auc_ovo_weighted',\n 'roc_auc_ovr',\n 'roc_auc_ovr_weighted',\n 'top_k_accuracy',\n 'v_measure_score']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   pearson_mean  pearson_std  R2_mean  R2_std  MSE_mean  MSE_std  \\\n0            10           23        4       3         1        2   \n\n   Number of Features  hi  \n0                  10  hi  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pearson_mean</th>\n      <th>pearson_std</th>\n      <th>R2_mean</th>\n      <th>R2_std</th>\n      <th>MSE_mean</th>\n      <th>MSE_std</th>\n      <th>Number of Features</th>\n      <th>hi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>23</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>10</td>\n      <td>hi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_mean=10\n",
    "pearson_std=23\n",
    "R2_mean=4\n",
    "R2_std=3\n",
    "MSE_mean=1\n",
    "MSE_std=2\n",
    "id='hi'\n",
    "data=[[pearson_mean,pearson_std,R2_mean,R2_std,MSE_mean,MSE_std,10,id]]\n",
    "scores=pd.DataFrame(data,columns=['pearson_mean','pearson_std','R2_mean','R2_std','MSE_mean','MSE_std','Number of Features',str(id)])\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        rf_model = RandomForestRegressor()\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
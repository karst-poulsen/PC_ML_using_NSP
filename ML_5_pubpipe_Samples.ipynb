{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "\n",
    "scorings=['r2','neg_mean_squared_error','mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    id = 'Con_drop_' +str(z)+ '_30' + '%zeros'\n",
    "    # take files in_dir and combine then into a pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "    # melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "    files = os.listdir(in_dir)\n",
    "    if multi_files == True:\n",
    "        for i, f in enumerate(files):\n",
    "            if i == 0:\n",
    "                raw_MS_data = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = raw_MS_data.shape[1]\n",
    "                cutoff = int(zerosperrow * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                # print(raw_MS_data)\n",
    "                raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = temp.shape[1]\n",
    "                cutoff = int(zerosperrow * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                temp = temp.drop(temp[(temp == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                temp = pd.melt(temp, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "                raw_MS_data = pd.concat([raw_MS_data, temp])\n",
    "                print('final shape after melt', raw_MS_data.shape)\n",
    "                print('number of zeros in the dataset:',(raw_MS_data == 0).sum().sum())\n",
    "\n",
    "\n",
    "    else:\n",
    "        raw_MS_data = pd.read_excel(prot_abund_file, header=0)\n",
    "        cols = raw_MS_data.shape[1]\n",
    "        cutoff = int(zerosperrow * cols)\n",
    "        print('shape beofre dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "        print('shape after dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "    #remove prots that were added due to merge\n",
    "    raw_MS_data = raw_MS_data.dropna()\n",
    "    ###Bring in controls (MS data for serums)##\n",
    "    controls = pd.read_excel(controls_file, header=0)\n",
    "    MS_data_controls = pd.merge(raw_MS_data, controls, how='left', on='Entry')\n",
    "    ###Bring in Uniprot_data,NSPdata and NP data##\n",
    "    uniprot_dat = pd.read_excel(uniprot_filepath, header=0)\n",
    "    NSP_data = pd.read_excel(NSPfilePath)\n",
    "    ###Bring in NP data and merge to get complete NP dataset###\n",
    "    NPUNdata = pd.read_excel(NP_filepath, header=0, sheet_name='NPUNID')\n",
    "    NPprop = pd.read_excel(NP_filepath, header=0, sheet_name='NP_Props')\n",
    "    NPdata = pd.merge(NPUNdata, NPprop, how=\"left\", on='NPID')\n",
    "    NPdata.dropna(inplace=True)\n",
    "    #calculate Enrichment\n",
    "    #####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "    # MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "    # MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "    raw_prop_data = pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left', on='Entry')\n",
    "    Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']), how='left',\n",
    "                                     on='Entry')\n",
    "    #merges netsurfp features and biopython features\n",
    "    Protein_data_complete.fillna(0, inplace=True)\n",
    "    data_complete = pd.merge(Protein_data_complete, NPdata, how='inner', on='Sample_num')\n",
    "    data_complete.fillna(0, inplace=True)\n",
    "    #create ordinal variables for the core materials and surface ligands\n",
    "    le = LabelEncoder()\n",
    "    data_complete['Core Material'] = le.fit_transform(data_complete['Core Material'])\n",
    "    data_complete['Surface_Ligand'] = le.fit_transform(data_complete['Surface_Ligand'])\n",
    "    #shuffle data using sample\n",
    "    data_complete = data_complete.sample(frac=1)\n",
    "    #set labels (what we are trying to predict) as Abundance\n",
    "    label_abund_df = data_complete['Abundance'].copy()\n",
    "    label_abund = np.ravel(label_abund_df)\n",
    "    NPIDs=data_complete['NPUNID'].copy()\n",
    "    data_complete.drop(columns=['notes', 'Notes', 'NPUNID'], inplace=True)\n",
    "\n",
    "    #make it one dimenisional\n",
    "    #drop qualitative, not neccessary, and label columns\n",
    "    #create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "    to_drop = data_complete.filter(like='total_exposed_')\n",
    "    data_complete.drop(columns=to_drop, inplace=True)\n",
    "    df = data_complete.drop(\n",
    "        ['Entry', 'Abundance', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5',\n",
    "         'Raw_FileID'], axis=1)\n",
    "    if abund_controls == False:\n",
    "        df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "\n",
    "    df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "    df_out.to_excel(\"Input_data/Save_files/df_whole_\"+id+\".xlsx\")\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "\n",
    "    #Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "\n",
    "scorings=['r2','neg_mean_squared_error','mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    id = 'Con_drop_' +str(z)+ '_30' + '%zeros'\n",
    "    # take files in_dir and combine then into a pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "    # melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "\n",
    "    if abund_controls == False:\n",
    "        df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    #Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###Filtering Abundance to see if i stick to higher abundance I get better performance##\n",
    "rfecv_ = False  #True_runs RFECV\n",
    "rfe_ = True     #True runs Recursive feature elimination\n",
    "abund_controls = False # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "df_filepath='Input_data/Save_files/df_whole_drop_30%zeros.xlsx'\n",
    "df_a = pd.read_excel(df_filepath, header=0)\n",
    "# scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 12\n",
    "Abund_filter = np.arange(0.0001,0.1001,0.001).tolist()\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "summary_tmp=[]\n",
    "for i in Abund_filter:\n",
    "    df= df_a[df_a['Abundance']>=i]\n",
    "    id='RFR_AFilter_'+str(i)+'NoCon'\n",
    "    labels_df= df['Abundance'].copy()\n",
    "    id_col= df['NPUNID'].copy()\n",
    "    label_abund=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    if abund_controls == False:\n",
    "        df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        # df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        # df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "    #Run Recursive feature elimination to remove down to RFE_Feats\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        selector = RFE(model, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        # df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        # df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "    tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    tmp2['Abundance Filter']=i\n",
    "\n",
    "    summary_tmp.append(tmp2)\n",
    "\n",
    "    summary=pd.concat(summary_tmp,axis=0)\n",
    "summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmp95\\AppData\\Local\\Temp\\ipykernel_10936\\20906758.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Abundance']=np.log2(df['Abundance']+1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([495., 188.,  82.,  88.,  56.,  35.,  38.,  33.,  15.,  19.,  26.,\n         18.,  24.,  10.,  12.,  18.,  17.,  14.,  10.,   8.,   7.,  15.,\n          7.,   8.,   4.,   5.,   5.,   0.,   6.,   3.,   4.,   1.,   4.,\n          1.,   3.,   2.,   3.,   2.,   3.,   3.,   0.,   0.,   4.,   1.,\n          2.,   0.,   1.,   1.,   3.,   1.]),\n array([0.01467548, 0.11929701, 0.22391853, 0.32854005, 0.43316158,\n        0.5377831 , 0.64240463, 0.74702615, 0.85164767, 0.9562692 ,\n        1.06089072, 1.16551225, 1.27013377, 1.3747553 , 1.47937682,\n        1.58399834, 1.68861987, 1.79324139, 1.89786292, 2.00248444,\n        2.10710596, 2.21172749, 2.31634901, 2.42097054, 2.52559206,\n        2.63021358, 2.73483511, 2.83945663, 2.94407816, 3.04869968,\n        3.15332121, 3.25794273, 3.36256425, 3.46718578, 3.5718073 ,\n        3.67642883, 3.78105035, 3.88567187, 3.9902934 , 4.09491492,\n        4.19953645, 4.30415797, 4.40877949, 4.51340102, 4.61802254,\n        4.72264407, 4.82726559, 4.93188712, 5.03650864, 5.14113016,\n        5.24575169]),\n <BarContainer object of 50 artists>)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_a = pd.read_excel(df_filepath, header=0)\n",
    "df=df_a[df_a['Abundance']>=0.01]\n",
    "# df_a['Abundance']=np.log10(df_a['Abundance']+1)\n",
    "df['Abundance']=np.log2(df['Abundance']+1)\n",
    "plt.hist(df['Abundance'],bins=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 12\n",
    "Abund_filter = np.arange(0.0001,0.01,0.0001).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    df=pd.read_excel('Input_data/Save_files/df_whole_noCon_drop30%zeros.xlsx')\n",
    "    labels_df= df['label_abund_df'].copy()\n",
    "    id_col= df['NPIDs'].copy()\n",
    "    labels=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "    # Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([df, NPIDs,label_abund_df],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Drop a nanoparticle and then predict it###\n",
    "##NPs of interest##\n",
    "#easy to hard NPUNID 1,20,19,16,7,31,34,34\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "df.columns\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "scores=remove_one_predict_mse(NPUNID_list,model,labels,id_col,df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.concat([labels, id_col, df], axis=1)\n",
    "id_list=NPUNID_list\n",
    "# Initialize a list to hold MSE scores for each removed ID\n",
    "mse_scores = []\n",
    "\n",
    "# Loop through each ID in the list\n",
    "for id_to_remove in id_list:\n",
    "    # Remove the row with the current ID from the dataframe\n",
    "    removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "    df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "    # Split the remaining data into features and target\n",
    "    X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "    y_train = df[labels.name]\n",
    "\n",
    "    # Fit a random forest regression model to the training data\n",
    "    # model.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained model to predict on the removed ID\n",
    "    X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "    y_true = removed_row[labels.name]\n",
    "    print(X_test.shape)\n",
    "    print(y_true.shape)\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE score and add it to the list\n",
    "    # mse_score = mean_squared_error(y_true, y_pred)\n",
    "    # mse_scores.append(mse_score)\n",
    "    #\n",
    "    # Add the removed row back into the dataframe\n",
    "    df = pd.concat([df, removed_row], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    df=pd.read_excel('Input_data/Save_files/df_whole_noCon_drop30%zeros.xlsx')\n",
    "    labels_df= df['label_abund_df'].copy()\n",
    "    id_col= df['NPIDs'].copy()\n",
    "    labels=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "    # Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([df, NPIDs,label_abund_df],axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "## Drop a nanoparticle and then predict it###\n",
    "##NPs of interest##\n",
    "#easy to hard NPUNID 1,20,19,16,7,31,34,34\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0  Length   Mass  frac_aa_A  frac_aa_C  frac_aa_D  frac_aa_E  \\\n0             18     202  22144   0.103960   0.039604   0.044554   0.079208   \n1            435     733  84731   0.047749   0.009550   0.075034   0.132333   \n2            217     103  12217   0.126214   0.029126   0.029126   0.087379   \n3            412     481  53698   0.056133   0.008316   0.049896   0.049896   \n4           1589     202  23182   0.089109   0.024752   0.049505   0.099010   \n...          ...     ...    ...        ...        ...        ...        ...   \n1765         398     625  69872   0.049600   0.057600   0.043200   0.049600   \n1766        1146     807  90976   0.052045   0.057001   0.054523   0.086741   \n1767         843       0      0   0.000000   0.000000   0.000000   0.000000   \n1768         267     474  53342   0.059072   0.059072   0.059072   0.084388   \n1769        1514     683  74408   0.090776   0.016105   0.042460   0.058565   \n\n      frac_aa_F  frac_aa_G  frac_aa_H  ...  Dh_functionalized  Shaken  \\\n0      0.034653   0.074257   0.014851  ...                149       1   \n1      0.034106   0.043656   0.016371  ...                282       1   \n2      0.038835   0.019417   0.009709  ...                229       1   \n3      0.060291   0.056133   0.022869  ...                282       1   \n4      0.049505   0.034653   0.024752  ...                291       1   \n...         ...        ...        ...  ...                ...     ...   \n1765   0.046400   0.070400   0.040000  ...                282       1   \n1766   0.024783   0.064436   0.012392  ...                151       1   \n1767   0.000000   0.000000   0.000000  ...                218       1   \n1768   0.046414   0.029536   0.018987  ...                226       1   \n1769   0.023426   0.070278   0.026354  ...                609       1   \n\n      Centrifuged  ProteinID  NP_incubation Concentration (mg/mL)  \\\n0               0          1                                    5   \n1               0          1                                    5   \n2               0          1                                    5   \n3               0          1                                    5   \n4               0          1                                    5   \n...           ...        ...                                  ...   \n1765            0          1                                    5   \n1766            0          1                                    5   \n1767            0          1                                    5   \n1768            0          1                                    5   \n1769            0          1                                    5   \n\n      Incubation Concentration (mg/ml)  Corona_Concentration (ug/mg)  \\\n0                                    4                     19.211444   \n1                                    4                     52.085492   \n2                                    4                     24.697213   \n3                                    4                     52.085492   \n4                                   40                    115.659641   \n...                                ...                           ...   \n1765                                 4                     52.085492   \n1766                                40                     35.981901   \n1767                                40                     73.474471   \n1768                                 4                    122.773587   \n1769                                40                     25.108958   \n\n      Incubation Time (minutes)  Temperature  NPUNID  \n0                            30           25      19  \n1                            30           25      22  \n2                            30           25      20  \n3                            30           25      22  \n4                            30           25      32  \n...                         ...          ...     ...  \n1765                         30           25      22  \n1766                         30           25      28  \n1767                         30           25      26  \n1768                         30           25      21  \n1769                         30           25      31  \n\n[1770 rows x 96 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Length</th>\n      <th>Mass</th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_H</th>\n      <th>...</th>\n      <th>Dh_functionalized</th>\n      <th>Shaken</th>\n      <th>Centrifuged</th>\n      <th>ProteinID</th>\n      <th>NP_incubation Concentration (mg/mL)</th>\n      <th>Incubation Concentration (mg/ml)</th>\n      <th>Corona_Concentration (ug/mg)</th>\n      <th>Incubation Time (minutes)</th>\n      <th>Temperature</th>\n      <th>NPUNID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18</td>\n      <td>202</td>\n      <td>22144</td>\n      <td>0.103960</td>\n      <td>0.039604</td>\n      <td>0.044554</td>\n      <td>0.079208</td>\n      <td>0.034653</td>\n      <td>0.074257</td>\n      <td>0.014851</td>\n      <td>...</td>\n      <td>149</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>19.211444</td>\n      <td>30</td>\n      <td>25</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>435</td>\n      <td>733</td>\n      <td>84731</td>\n      <td>0.047749</td>\n      <td>0.009550</td>\n      <td>0.075034</td>\n      <td>0.132333</td>\n      <td>0.034106</td>\n      <td>0.043656</td>\n      <td>0.016371</td>\n      <td>...</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>217</td>\n      <td>103</td>\n      <td>12217</td>\n      <td>0.126214</td>\n      <td>0.029126</td>\n      <td>0.029126</td>\n      <td>0.087379</td>\n      <td>0.038835</td>\n      <td>0.019417</td>\n      <td>0.009709</td>\n      <td>...</td>\n      <td>229</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>24.697213</td>\n      <td>30</td>\n      <td>25</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>412</td>\n      <td>481</td>\n      <td>53698</td>\n      <td>0.056133</td>\n      <td>0.008316</td>\n      <td>0.049896</td>\n      <td>0.049896</td>\n      <td>0.060291</td>\n      <td>0.056133</td>\n      <td>0.022869</td>\n      <td>...</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1589</td>\n      <td>202</td>\n      <td>23182</td>\n      <td>0.089109</td>\n      <td>0.024752</td>\n      <td>0.049505</td>\n      <td>0.099010</td>\n      <td>0.049505</td>\n      <td>0.034653</td>\n      <td>0.024752</td>\n      <td>...</td>\n      <td>291</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>115.659641</td>\n      <td>30</td>\n      <td>25</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>398</td>\n      <td>625</td>\n      <td>69872</td>\n      <td>0.049600</td>\n      <td>0.057600</td>\n      <td>0.043200</td>\n      <td>0.049600</td>\n      <td>0.046400</td>\n      <td>0.070400</td>\n      <td>0.040000</td>\n      <td>...</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>1766</th>\n      <td>1146</td>\n      <td>807</td>\n      <td>90976</td>\n      <td>0.052045</td>\n      <td>0.057001</td>\n      <td>0.054523</td>\n      <td>0.086741</td>\n      <td>0.024783</td>\n      <td>0.064436</td>\n      <td>0.012392</td>\n      <td>...</td>\n      <td>151</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>35.981901</td>\n      <td>30</td>\n      <td>25</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1767</th>\n      <td>843</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>1768</th>\n      <td>267</td>\n      <td>474</td>\n      <td>53342</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.084388</td>\n      <td>0.046414</td>\n      <td>0.029536</td>\n      <td>0.018987</td>\n      <td>...</td>\n      <td>226</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>122.773587</td>\n      <td>30</td>\n      <td>25</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1769</th>\n      <td>1514</td>\n      <td>683</td>\n      <td>74408</td>\n      <td>0.090776</td>\n      <td>0.016105</td>\n      <td>0.042460</td>\n      <td>0.058565</td>\n      <td>0.023426</td>\n      <td>0.070278</td>\n      <td>0.026354</td>\n      <td>...</td>\n      <td>609</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>25.108958</td>\n      <td>30</td>\n      <td>25</td>\n      <td>31</td>\n    </tr>\n  </tbody>\n</table>\n<p>1770 rows × 96 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 96)) while a minimum of 1 is required by RandomForestRegressor.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m df\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[0;32m      8\u001B[0m model\u001B[38;5;241m=\u001B[39mRandomForestRegressor(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m scores\u001B[38;5;241m=\u001B[39m\u001B[43mremove_one_predict_mse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNPUNID_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43mid_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36mremove_one_predict_mse\u001B[1;34m(id_list, model, labels, id_col, data)\u001B[0m\n\u001B[0;32m     32\u001B[0m X_test \u001B[38;5;241m=\u001B[39m removed_row\u001B[38;5;241m.\u001B[39mdrop([labels\u001B[38;5;241m.\u001B[39mname, id_col\u001B[38;5;241m.\u001B[39mname], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     33\u001B[0m y_true \u001B[38;5;241m=\u001B[39m removed_row[labels\u001B[38;5;241m.\u001B[39mname]\n\u001B[1;32m---> 34\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Calculate the MSE score and add it to the list\u001B[39;00m\n\u001B[0;32m     37\u001B[0m mse_score \u001B[38;5;241m=\u001B[39m mean_squared_error(y_true, y_pred)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:991\u001B[0m, in \u001B[0;36mForestRegressor.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    989\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    990\u001B[0m \u001B[38;5;66;03m# Check data\u001B[39;00m\n\u001B[1;32m--> 991\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_X_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    993\u001B[0m \u001B[38;5;66;03m# Assign chunk of trees to jobs\u001B[39;00m\n\u001B[0;32m    994\u001B[0m n_jobs, _, _ \u001B[38;5;241m=\u001B[39m _partition_estimators(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_estimators, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:605\u001B[0m, in \u001B[0;36mBaseForest._validate_X_predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    602\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    603\u001B[0m \u001B[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001B[39;00m\n\u001B[0;32m    604\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 605\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m issparse(X) \u001B[38;5;129;01mand\u001B[39;00m (X\u001B[38;5;241m.\u001B[39mindices\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc \u001B[38;5;129;01mor\u001B[39;00m X\u001B[38;5;241m.\u001B[39mindptr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc):\n\u001B[0;32m    607\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo support for np.int64 index based sparse matrices\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:577\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 577\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    578\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:909\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    907\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n\u001B[0;32m    908\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_samples \u001B[38;5;241m<\u001B[39m ensure_min_samples:\n\u001B[1;32m--> 909\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    910\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    911\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    912\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_samples, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_samples, context)\n\u001B[0;32m    913\u001B[0m         )\n\u001B[0;32m    915\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    916\u001B[0m     n_features \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: Found array with 0 sample(s) (shape=(0, 96)) while a minimum of 1 is required by RandomForestRegressor."
     ]
    }
   ],
   "source": [
    "NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "df.columns\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "scores=remove_one_predict_mse(NPUNID_list,model,labels,id_col,df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "      Abundance_Controls  Length    Mass  frac_aa_A  frac_aa_C  frac_aa_D  \\\n0               0.000000     258   27890   0.073643   0.081395   0.046512   \n1               0.000100     441   48712   0.068027   0.011338   0.047619   \n2               0.002023     396   44471   0.050505   0.030303   0.058081   \n3               0.004530    2211  248983   0.050204   0.008593   0.065129   \n4               0.000000     258   27890   0.073643   0.081395   0.046512   \n...                  ...     ...     ...        ...        ...        ...   \n1765            0.139882     345   38252   0.057971   0.066667   0.028986   \n1766            0.343098     465   52347   0.064516   0.019355   0.045161   \n1767            0.000000     807   90976   0.052045   0.057001   0.054523   \n1768            1.120803     474   53342   0.059072   0.059072   0.059072   \n1769            0.009005    1463  138938   0.097744   0.012303   0.043746   \n\n      frac_aa_E  frac_aa_F  frac_aa_G  frac_aa_H  ...  Dh_core  \\\n0      0.077519   0.011628   0.100775   0.042636  ...      229   \n1      0.106576   0.018141   0.102041   0.013605  ...      229   \n2      0.047980   0.065657   0.070707   0.108586  ...      229   \n3      0.061511   0.034826   0.055631   0.027589  ...      229   \n4      0.077519   0.011628   0.100775   0.042636  ...      229   \n...         ...        ...        ...        ...  ...      ...   \n1765   0.057971   0.060870   0.069565   0.028986  ...      229   \n1766   0.075269   0.053763   0.049462   0.012903  ...      229   \n1767   0.086741   0.024783   0.064436   0.012392  ...      149   \n1768   0.084388   0.046414   0.029536   0.018987  ...      229   \n1769   0.051948   0.016405   0.265892   0.006152  ...      229   \n\n      Dh_functionalized  Shaken  Centrifuged  ProteinID  \\\n0                   316       1            0          1   \n1                   316       1            0          1   \n2                   218       1            0          1   \n3                   218       1            0          1   \n4                   282       1            0          1   \n...                 ...     ...          ...        ...   \n1765                291       1            0          1   \n1766                316       1            0          1   \n1767                226       1            0          1   \n1768                282       1            0          1   \n1769                609       1            0          1   \n\n      NP_incubation Concentration (mg/mL)  Incubation Concentration (mg/ml)  \\\n0                                       5                                 4   \n1                                       5                                 4   \n2                                       5                                40   \n3                                       5                                40   \n4                                       5                                 4   \n...                                   ...                               ...   \n1765                                    5                                40   \n1766                                    5                                40   \n1767                                    5                                 4   \n1768                                    5                                40   \n1769                                    5                                40   \n\n      Corona_Concentration (ug/mg)  Incubation Time (minutes)  Temperature  \n0                        27.929767                         30           25  \n1                        27.929767                         30           25  \n2                        73.474471                         30           25  \n3                        73.474471                         30           25  \n4                        52.085492                         30           25  \n...                            ...                        ...          ...  \n1765                    115.659641                         30           25  \n1766                     56.774264                         30           25  \n1767                    122.773587                         30           25  \n1768                    123.140432                         30           25  \n1769                     25.108958                         30           25  \n\n[1770 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Abundance_Controls</th>\n      <th>Length</th>\n      <th>Mass</th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_H</th>\n      <th>...</th>\n      <th>Dh_core</th>\n      <th>Dh_functionalized</th>\n      <th>Shaken</th>\n      <th>Centrifuged</th>\n      <th>ProteinID</th>\n      <th>NP_incubation Concentration (mg/mL)</th>\n      <th>Incubation Concentration (mg/ml)</th>\n      <th>Corona_Concentration (ug/mg)</th>\n      <th>Incubation Time (minutes)</th>\n      <th>Temperature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000100</td>\n      <td>441</td>\n      <td>48712</td>\n      <td>0.068027</td>\n      <td>0.011338</td>\n      <td>0.047619</td>\n      <td>0.106576</td>\n      <td>0.018141</td>\n      <td>0.102041</td>\n      <td>0.013605</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002023</td>\n      <td>396</td>\n      <td>44471</td>\n      <td>0.050505</td>\n      <td>0.030303</td>\n      <td>0.058081</td>\n      <td>0.047980</td>\n      <td>0.065657</td>\n      <td>0.070707</td>\n      <td>0.108586</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004530</td>\n      <td>2211</td>\n      <td>248983</td>\n      <td>0.050204</td>\n      <td>0.008593</td>\n      <td>0.065129</td>\n      <td>0.061511</td>\n      <td>0.034826</td>\n      <td>0.055631</td>\n      <td>0.027589</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>0.139882</td>\n      <td>345</td>\n      <td>38252</td>\n      <td>0.057971</td>\n      <td>0.066667</td>\n      <td>0.028986</td>\n      <td>0.057971</td>\n      <td>0.060870</td>\n      <td>0.069565</td>\n      <td>0.028986</td>\n      <td>...</td>\n      <td>229</td>\n      <td>291</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>115.659641</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1766</th>\n      <td>0.343098</td>\n      <td>465</td>\n      <td>52347</td>\n      <td>0.064516</td>\n      <td>0.019355</td>\n      <td>0.045161</td>\n      <td>0.075269</td>\n      <td>0.053763</td>\n      <td>0.049462</td>\n      <td>0.012903</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>56.774264</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1767</th>\n      <td>0.000000</td>\n      <td>807</td>\n      <td>90976</td>\n      <td>0.052045</td>\n      <td>0.057001</td>\n      <td>0.054523</td>\n      <td>0.086741</td>\n      <td>0.024783</td>\n      <td>0.064436</td>\n      <td>0.012392</td>\n      <td>...</td>\n      <td>149</td>\n      <td>226</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>122.773587</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1768</th>\n      <td>1.120803</td>\n      <td>474</td>\n      <td>53342</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.084388</td>\n      <td>0.046414</td>\n      <td>0.029536</td>\n      <td>0.018987</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>123.140432</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1769</th>\n      <td>0.009005</td>\n      <td>1463</td>\n      <td>138938</td>\n      <td>0.097744</td>\n      <td>0.012303</td>\n      <td>0.043746</td>\n      <td>0.051948</td>\n      <td>0.016405</td>\n      <td>0.265892</td>\n      <td>0.006152</td>\n      <td>...</td>\n      <td>229</td>\n      <td>609</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>25.108958</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n<p>1770 rows × 95 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot index with multidimensional key",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Loop through each ID in the list\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m id_to_remove \u001B[38;5;129;01min\u001B[39;00m id_list:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Remove the row with the current ID from the dataframe\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m     removed_row \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mid_col\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mid_to_remove\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     10\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mloc[df[id_col\u001B[38;5;241m.\u001B[39mname] \u001B[38;5;241m!=\u001B[39m id_to_remove]\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# Split the remaining data into features and target\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:967\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    964\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    966\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[1;32m--> 967\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1189\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(labels, MultiIndex)):\n\u001B[0;32m   1188\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndim\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m-> 1189\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot index with multidimensional key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_iterable(key, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[0;32m   1193\u001B[0m \u001B[38;5;66;03m# nested tuple slicing\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot index with multidimensional key"
     ]
    }
   ],
   "source": [
    "df = pd.concat([labels, id_col, df], axis=1)\n",
    "id_list=NPUNID_list\n",
    "# Initialize a list to hold MSE scores for each removed ID\n",
    "mse_scores = []\n",
    "\n",
    "# Loop through each ID in the list\n",
    "for id_to_remove in id_list:\n",
    "    # Remove the row with the current ID from the dataframe\n",
    "    removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "    df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "    # Split the remaining data into features and target\n",
    "    X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "    y_train = df[labels.name]\n",
    "\n",
    "    # Fit a random forest regression model to the training data\n",
    "    # model.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained model to predict on the removed ID\n",
    "    X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "    y_true = removed_row[labels.name]\n",
    "    print(X_test.shape)\n",
    "    print(y_true.shape)\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE score and add it to the list\n",
    "    # mse_score = mean_squared_error(y_true, y_pred)\n",
    "    # mse_scores.append(mse_score)\n",
    "    #\n",
    "    # Add the removed row back into the dataframe\n",
    "    df = pd.concat([df, removed_row], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x=[1,2,3,4]\n",
    "y=[1,2,3,4]\n",
    "y2=[2,4,6,8]\n",
    "y3=[4,8,12,16]\n",
    "\n",
    "plt.plot(x, y, color='tab:blue', marker='o',label='pearson')\n",
    "plt.plot(x, y2, color='tab:red', marker='s',label='R2')\n",
    "plt.plot(x, y3, color='tab:green', marker='^',label='mse')\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Score as a function of Feat Drop\\n'+id)\n",
    "plt.savefig('Output_data/feat_drop_cumulative' + id + '.png', bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'adjusted_mutual_info_score',\n 'adjusted_rand_score',\n 'average_precision',\n 'balanced_accuracy',\n 'completeness_score',\n 'explained_variance',\n 'f1',\n 'f1_macro',\n 'f1_micro',\n 'f1_samples',\n 'f1_weighted',\n 'fowlkes_mallows_score',\n 'homogeneity_score',\n 'jaccard',\n 'jaccard_macro',\n 'jaccard_micro',\n 'jaccard_samples',\n 'jaccard_weighted',\n 'matthews_corrcoef',\n 'max_error',\n 'mutual_info_score',\n 'neg_brier_score',\n 'neg_log_loss',\n 'neg_mean_absolute_error',\n 'neg_mean_absolute_percentage_error',\n 'neg_mean_gamma_deviance',\n 'neg_mean_poisson_deviance',\n 'neg_mean_squared_error',\n 'neg_mean_squared_log_error',\n 'neg_median_absolute_error',\n 'neg_root_mean_squared_error',\n 'normalized_mutual_info_score',\n 'precision',\n 'precision_macro',\n 'precision_micro',\n 'precision_samples',\n 'precision_weighted',\n 'r2',\n 'rand_score',\n 'recall',\n 'recall_macro',\n 'recall_micro',\n 'recall_samples',\n 'recall_weighted',\n 'roc_auc',\n 'roc_auc_ovo',\n 'roc_auc_ovo_weighted',\n 'roc_auc_ovr',\n 'roc_auc_ovr_weighted',\n 'top_k_accuracy',\n 'v_measure_score']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Get total number of rows\n",
    "total_count = len(df)\n",
    "df = df.rename(columns=lambda x: x.replace('/', ''))\n",
    "# Loop over each column of the dataframe\n",
    "for col in df.columns:\n",
    "    # Create a histogram of the current column, with bins=10\n",
    "    data=df[col]\n",
    "    plt.hist(data, weights=np.ones_like(data) / len(data))\n",
    "\n",
    "    # Set the title of the histogram to the column name and percent of total population\n",
    "    plt.title(str(col))\n",
    "    plt.savefig('Output_data/{}.png'.format(str(col)))\n",
    "    plt.close()\n",
    "    # Show the histogram\n",
    "    # plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'adjusted_mutual_info_score',\n 'adjusted_rand_score',\n 'average_precision',\n 'balanced_accuracy',\n 'completeness_score',\n 'explained_variance',\n 'f1',\n 'f1_macro',\n 'f1_micro',\n 'f1_samples',\n 'f1_weighted',\n 'fowlkes_mallows_score',\n 'homogeneity_score',\n 'jaccard',\n 'jaccard_macro',\n 'jaccard_micro',\n 'jaccard_samples',\n 'jaccard_weighted',\n 'matthews_corrcoef',\n 'max_error',\n 'mutual_info_score',\n 'neg_brier_score',\n 'neg_log_loss',\n 'neg_mean_absolute_error',\n 'neg_mean_absolute_percentage_error',\n 'neg_mean_gamma_deviance',\n 'neg_mean_poisson_deviance',\n 'neg_mean_squared_error',\n 'neg_mean_squared_log_error',\n 'neg_median_absolute_error',\n 'neg_root_mean_squared_error',\n 'normalized_mutual_info_score',\n 'precision',\n 'precision_macro',\n 'precision_micro',\n 'precision_samples',\n 'precision_weighted',\n 'r2',\n 'rand_score',\n 'recall',\n 'recall_macro',\n 'recall_micro',\n 'recall_samples',\n 'recall_weighted',\n 'roc_auc',\n 'roc_auc_ovo',\n 'roc_auc_ovo_weighted',\n 'roc_auc_ovr',\n 'roc_auc_ovr_weighted',\n 'top_k_accuracy',\n 'v_measure_score']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   pearson_mean  pearson_std  R2_mean  R2_std  MSE_mean  MSE_std  \\\n0            10           23        4       3         1        2   \n\n   Number of Features  hi  \n0                  10  hi  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pearson_mean</th>\n      <th>pearson_std</th>\n      <th>R2_mean</th>\n      <th>R2_std</th>\n      <th>MSE_mean</th>\n      <th>MSE_std</th>\n      <th>Number of Features</th>\n      <th>hi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>23</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>10</td>\n      <td>hi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_mean=10\n",
    "pearson_std=23\n",
    "R2_mean=4\n",
    "R2_std=3\n",
    "MSE_mean=1\n",
    "MSE_std=2\n",
    "id='hi'\n",
    "data=[[pearson_mean,pearson_std,R2_mean,R2_std,MSE_mean,MSE_std,10,id]]\n",
    "scores=pd.DataFrame(data,columns=['pearson_mean','pearson_std','R2_mean','R2_std','MSE_mean','MSE_std','Number of Features',str(id)])\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        rf_model = RandomForestRegressor()\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
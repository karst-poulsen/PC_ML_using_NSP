{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T14:59:40.410700Z",
     "end_time": "2023-04-22T14:59:40.736104Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Make 3 DFs### Load saved DFs instead of running this cell\n",
    "#DF 1 All_ files combined\n",
    "#DF 2 Synth Files ( the 19 UniqueNPs that I ran with my synthesized NPs+some PS NPs)\n",
    "#DF 3 All_files 30% zeros\n",
    "multi_files = False  #set to false if you just want to set one  prot_abund_file\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/abundance/Norm_Intensity_Bovsynth022123.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database2.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "zerosperrow=1\n",
    "files = os.listdir(in_dir)\n",
    "if multi_files == True:\n",
    "    for i, f in enumerate(files):\n",
    "        if i == 0:\n",
    "            raw_MS_data = pd.read_excel(in_dir + f, header=0)\n",
    "            cols = raw_MS_data.shape[1]\n",
    "            cutoff = int(zerosperrow * cols)\n",
    "            print('shape before dropping rows', raw_MS_data.shape)\n",
    "            raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "            print('shape after dropping rows', raw_MS_data.shape)\n",
    "            # print(raw_MS_data)\n",
    "            raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "        else:\n",
    "\n",
    "            temp = pd.read_excel(in_dir + f, header=0)\n",
    "            cols = temp.shape[1]\n",
    "            cutoff = int(zerosperrow * cols)\n",
    "            print('shape before dropping rows', raw_MS_data.shape)\n",
    "            temp = temp.drop(temp[(temp == 0).sum(axis=1) >= cutoff].index)\n",
    "            print('shape after dropping rows', raw_MS_data.shape)\n",
    "            temp = pd.melt(temp, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "            raw_MS_data = pd.concat([raw_MS_data, temp])\n",
    "            print('final shape after melt', raw_MS_data.shape)\n",
    "            print('number of zeros in the dataset:',(raw_MS_data == 0).sum().sum())\n",
    "\n",
    "\n",
    "else:\n",
    "    raw_MS_data = pd.read_excel(prot_abund_file, header=0)\n",
    "    cols = raw_MS_data.shape[1]\n",
    "    cutoff = int(zerosperrow * cols)\n",
    "    print('shape beofre dropping rows', raw_MS_data.shape)\n",
    "    raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "    print('shape after dropping rows', raw_MS_data.shape)\n",
    "    raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "#remove prots that were added due to merge\n",
    "raw_MS_data = raw_MS_data.dropna()\n",
    "###Bring in controls (MS data for serums)##\n",
    "controls = pd.read_excel(controls_file, header=0)\n",
    "MS_data_controls = pd.merge(raw_MS_data, controls, how='left', on='Entry')\n",
    "###Bring in Uniprot_data,NSPdata and NP data##\n",
    "uniprot_dat = pd.read_excel(uniprot_filepath, header=0)\n",
    "NSP_data = pd.read_excel(NSPfilePath)\n",
    "###Bring in NP data and merge to get complete NP dataset###\n",
    "NPUNdata = pd.read_excel(NP_filepath, header=0, sheet_name='NPUNID')\n",
    "NPprop = pd.read_excel(NP_filepath, header=0, sheet_name='NP_Props')\n",
    "NPdata = pd.merge(NPUNdata, NPprop, how=\"left\", on='NPID')\n",
    "# NPdata.dropna(inplace=True)\n",
    "#calculate Enrichment\n",
    "#####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "# MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "raw_prop_data = pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left', on='Entry')\n",
    "Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']), how='left',\n",
    "                                 on='Entry')\n",
    "#merges netsurfp features and biopython features\n",
    "Protein_data_complete.fillna(0, inplace=True)\n",
    "# print('Sample Nums in Protein Data',Protein_data_complete['Sample_num'])\n",
    "data_complete = pd.merge(Protein_data_complete, NPdata, how='inner', on='Sample_num')\n",
    "data_complete.fillna(0, inplace=True)\n",
    "#create ordinal variables for the core materials and surface ligands\n",
    "le = LabelEncoder()\n",
    "data_complete['Core Material'] = le.fit_transform(data_complete['Core Material'])\n",
    "data_complete['Surface_Ligand'] = le.fit_transform(data_complete['Surface_Ligand'])\n",
    "#shuffle data using sample\n",
    "data_complete = data_complete.sample(frac=1)\n",
    "\n",
    "data_complete.drop(columns=['notes', 'Notes'], inplace=True)\n",
    "\n",
    "data_complete.to_excel(\"Input_data/Save_files/df_2_Synth.xlsx\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T16:02:33.549664Z",
     "end_time": "2023-04-22T16:03:12.511654Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Large OG Pipeline # Move lower #\n",
    "\n",
    "#Editable Variables\n",
    "#list of test filters\n",
    "# zerosperrow = 0.3\n",
    "# multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "# rfecv_ = True  #True_runs RFECV\n",
    "# rfe_ = False     #True runs Recursive feature elimination\n",
    "# abund_controls = True # True keeps the serum as an input feature\n",
    "# splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "# in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "# prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "# NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "# controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "# uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "# NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "#\n",
    "# scorings=['r2','neg_mean_squared_error','mean_absolute_error']\n",
    "# RFE_Feats = 40\n",
    "# model = RandomForestRegressor(n_estimators=80)\n",
    "# # model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "# summary_tmp=[]\n",
    "# for z in scorings:\n",
    "#     id = 'Con_drop_' +str(z)+ '_30' + '%zeros'\n",
    "#     # take files in_dir and combine then into a pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "#     # melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "#     files = os.listdir(in_dir)\n",
    "#     if multi_files == True:\n",
    "#         for i, f in enumerate(files):\n",
    "#             if i == 0:\n",
    "#                 raw_MS_data = pd.read_excel(in_dir + f, header=0)\n",
    "#                 cols = raw_MS_data.shape[1]\n",
    "#                 cutoff = int(zerosperrow * cols)\n",
    "#                 print('shape before dropping rows', raw_MS_data.shape)\n",
    "#                 raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "#                 print('shape after dropping rows', raw_MS_data.shape)\n",
    "#                 # print(raw_MS_data)\n",
    "#                 raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "#\n",
    "#             else:\n",
    "#\n",
    "#                 temp = pd.read_excel(in_dir + f, header=0)\n",
    "#                 cols = temp.shape[1]\n",
    "#                 cutoff = int(zerosperrow * cols)\n",
    "#                 print('shape before dropping rows', raw_MS_data.shape)\n",
    "#                 temp = temp.drop(temp[(temp == 0).sum(axis=1) >= cutoff].index)\n",
    "#                 print('shape after dropping rows', raw_MS_data.shape)\n",
    "#                 temp = pd.melt(temp, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "#\n",
    "#                 raw_MS_data = pd.concat([raw_MS_data, temp])\n",
    "#                 print('final shape after melt', raw_MS_data.shape)\n",
    "#                 print('number of zeros in the dataset:',(raw_MS_data == 0).sum().sum())\n",
    "#\n",
    "#\n",
    "#     else:\n",
    "#         raw_MS_data = pd.read_excel(prot_abund_file, header=0)\n",
    "#         cols = raw_MS_data.shape[1]\n",
    "#         cutoff = int(zerosperrow * cols)\n",
    "#         print('shape beofre dropping rows', raw_MS_data.shape)\n",
    "#         raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "#         print('shape after dropping rows', raw_MS_data.shape)\n",
    "#         raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "#     #remove prots that were added due to merge\n",
    "#     raw_MS_data = raw_MS_data.dropna()\n",
    "#     ###Bring in controls (MS data for serums)##\n",
    "#     controls = pd.read_excel(controls_file, header=0)\n",
    "#     MS_data_controls = pd.merge(raw_MS_data, controls, how='left', on='Entry')\n",
    "#     ###Bring in Uniprot_data,NSPdata and NP data##\n",
    "#     uniprot_dat = pd.read_excel(uniprot_filepath, header=0)\n",
    "#     NSP_data = pd.read_excel(NSPfilePath)\n",
    "#     ###Bring in NP data and merge to get complete NP dataset###\n",
    "#     NPUNdata = pd.read_excel(NP_filepath, header=0, sheet_name='NPUNID')\n",
    "#     NPprop = pd.read_excel(NP_filepath, header=0, sheet_name='NP_Props')\n",
    "#     NPdata = pd.merge(NPUNdata, NPprop, how=\"left\", on='NPID')\n",
    "#     NPdata.dropna(inplace=True)\n",
    "#     #calculate Enrichment\n",
    "#     #####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "#     # MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "#     # MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "#     raw_prop_data = pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left', on='Entry')\n",
    "#     Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']), how='left',\n",
    "#                                      on='Entry')\n",
    "#     #merges netsurfp features and biopython features\n",
    "#     Protein_data_complete.fillna(0, inplace=True)\n",
    "#     data_complete = pd.merge(Protein_data_complete, NPdata, how='inner', on='Sample_num')\n",
    "#     data_complete.fillna(0, inplace=True)\n",
    "#     #create ordinal variables for the core materials and surface ligands\n",
    "#     le = LabelEncoder()\n",
    "#     data_complete['Core Material'] = le.fit_transform(data_complete['Core Material'])\n",
    "#     data_complete['Surface_Ligand'] = le.fit_transform(data_complete['Surface_Ligand'])\n",
    "#     #shuffle data using sample\n",
    "#     data_complete = data_complete.sample(frac=1)\n",
    "#     #set labels (what we are trying to predict) as Abundance\n",
    "#     label_abund_df = data_complete['Abundance'].copy()\n",
    "#     label_abund = np.ravel(label_abund_df)\n",
    "#     NPIDs=data_complete['NPUNID'].copy()\n",
    "#     data_complete.drop(columns=['notes', 'Notes', 'NPUNID'], inplace=True)\n",
    "#\n",
    "#     #make it one dimenisional\n",
    "#     #drop qualitative, not neccessary, and label columns\n",
    "#     #create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "#     to_drop = data_complete.filter(like='total_exposed_')\n",
    "#     data_complete.drop(columns=to_drop, inplace=True)\n",
    "#     df = data_complete.drop(\n",
    "#         ['Entry', 'Abundance', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5',\n",
    "#          'Raw_FileID'], axis=1)\n",
    "#     if abund_controls == False:\n",
    "#         df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "#\n",
    "#     df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "#     df_out.to_excel(\"Input_data/Save_files/df_whole_\"+id+\".xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "#Sample code for loading saved DF and dropping appropriate columns before running\n",
    "df_filepath='Input_data/Save_files/df_1_all.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels_df= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "label_abund=np.ravel(labels_df)\n",
    "df=df.drop(columns=['Entry', 'Abundance', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5','Raw_FileID','NPUNID'])\n",
    "# if abund_controls == False:\n",
    "#     df.drop(columns=['Abundance_Controls'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T16:11:19.779507Z",
     "end_time": "2023-04-22T16:11:40.892094Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standard Pipeline### currently setup for testing different accuracy measures of RFECV\n",
    "\n",
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "scorings=['r2','neg_mean_squared_error','mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "\n",
    "df_filepath='Input_data/Save_files/df_1_all.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels_df= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "label_abund=np.ravel(labels_df)\n",
    "df=df.drop(columns=['Entry', 'Abundance', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5','Raw_FileID','NPUNID'])\n",
    "if abund_controls == False:\n",
    "    df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    #Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n",
      "C:\\Users\\pouls\\AppData\\Local\\Temp\\ipykernel_13848\\1895467353.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append({'Abund Thresh': i,\n"
     ]
    }
   ],
   "source": [
    "### Running Binary Classification systems and seeing what abund threshold is the best for performance##\n",
    "\n",
    "abund_thresh=np.arange(0.01,1.0,0.05).tolist()\n",
    "df_filepath='Input_data/Save_files/df_1_all.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "model=RandomForestClassifier()\n",
    "# id='RFR_AFilter_'+str(i)+'NoCon'\n",
    "# Create an empty pandas DataFrame to store the evaluation metrics\n",
    "eval_df = pd.DataFrame(columns=['Abund Thresh', 'F1', 'AUROC', 'Accuracy', 'Precision', 'Recall'])\n",
    "df=df.drop(columns=['Entry', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5','Raw_FileID'])\n",
    "df=df.drop(columns=['Abundance_Controls'])\n",
    "\n",
    "# Iterate through multiple iterations of model training and testing\n",
    "for i in abund_thresh:\n",
    "    df_a=df.copy()\n",
    "    df_a['binary_target']= df_a['Abundance'].apply(lambda t: 1 if t>=i else 0)\n",
    "    labels_df = df_a['binary_target'].copy()\n",
    "    # id_col= df_a['NPUNID'].copy()\n",
    "    label_binary=np.ravel(labels_df)\n",
    "    df_a=df_a.drop(columns=['Abundance','NPUNID','binary_target']).copy()\n",
    "\n",
    "    step = 2\n",
    "    RFE_Feats=15\n",
    "    selector = RFE(model, n_features_to_select=RFE_Feats, step=step)\n",
    "    selector = selector.fit(df_a, label_binary)\n",
    "    selector.support_\n",
    "    ranking = selector.ranking_\n",
    "    feat_list = selector.get_feature_names_out()\n",
    "    df_a = df_a[feat_list].copy()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_a, label_binary, test_size=0.2, random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    # Append a new row to the eval_df DataFrame with the evaluation metrics for this iteration\n",
    "    eval_df = eval_df.append({'Abund Thresh': i,\n",
    "                              'F1': f1,\n",
    "                              'AUROC': auroc,\n",
    "                              'Accuracy': accuracy,\n",
    "                              'Precision': precision,\n",
    "                              'Recall': recall}, ignore_index=True)\n",
    "\n",
    "eval_df.to_excel('Output_data/AbundThresholdingRFC.xlsx', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###Filtering Abundance to see if i stick to higher abundance I get better performance##\n",
    "rfecv_ = False  #True_runs RFECV\n",
    "rfe_ = True     #True runs Recursive feature elimination\n",
    "abund_controls = False # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "df_filepath = 'Input_data/Save_files/df_1_all.xlsx'\n",
    "df_a = pd.read_excel(df_filepath, header=0)\n",
    "df_a=df_a.drop(columns=['Entry', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5','Raw_FileID'])\n",
    "# scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 15\n",
    "import math\n",
    "\n",
    "start = 0.0001\n",
    "stop = 1\n",
    "num_points = 15\n",
    "\n",
    "# Calculate the common difference between consecutive terms in the series\n",
    "common_diff = (math.log(stop) - math.log(start)) / (num_points - 1)\n",
    "\n",
    "# Generate the logarithmic series as a list\n",
    "Abund_filter = [start * math.exp(i * common_diff) for i in range(num_points)]\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "summary_tmp=[]\n",
    "for i in Abund_filter:\n",
    "    df= df_a[df_a['Abundance']>=i]\n",
    "    id='RFR_AFilter_'+str(i)+'NoCon'\n",
    "    labels_df= df['Abundance'].copy()\n",
    "    id_col= df['NPUNID'].copy()\n",
    "    label_abund=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    if abund_controls == False:\n",
    "        df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        # df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        # df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "    #Run Recursive feature elimination to remove down to RFE_Feats\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        selector = RFE(model, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        # df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        # df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "    tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    tmp2['Abundance Filter']=i\n",
    "\n",
    "    summary_tmp.append(tmp2)\n",
    "\n",
    "    summary=pd.concat(summary_tmp,axis=0)\n",
    "summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_a = pd.read_excel(df_filepath, header=0)\n",
    "# df=df_a[df_a['Abundance']>=0.01]\n",
    "# df_a['Abundance']=np.log10(df_a['Abundance']+1)\n",
    "df['Abundance']=np.log2(df['Abundance']+1)\n",
    "plt.hist(df['Abundance'],bins=25)\n",
    "plt.xlim(-2,2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_a = pd.read_excel(df_filepath, header=0)\n",
    "# df=df_a[df_a['Abundance']>=0.01]\n",
    "# df_a['Abundance']=np.log10(df_a['Abundance']+1)\n",
    "df['Abundance']=np.log2(df['Abundance']+1)\n",
    "plt.hist(df['Abundance'],bins=25)\n",
    "plt.xlim(-2,2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    df=pd.read_excel('Input_data/Save_files/df_whole_noCon_drop30%zeros.xlsx')\n",
    "    labels_df= df['label_abund_df'].copy()\n",
    "    id_col= df['NPIDs'].copy()\n",
    "    labels=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "    # Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #Quality control\n",
    "    scram_score(df, label_abund, model, id, 0.2)\n",
    "    feat_drop(df, label_abund, model, id, 0.2)\n",
    "    feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    tmp2['TotalZeros']=zeros\n",
    "    tmp2['Percent_zeros']=percent_zeros\n",
    "    summary_tmp.append(tmp2)\n",
    "    lasso_feature_selection(df, label_abund, id)\n",
    "    summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    df=pd.read_excel('Input_data/Save_files/df_whole_noCon_drop30%zeros.xlsx')\n",
    "    labels_df= df['label_abund_df'].copy()\n",
    "    id_col= df['NPIDs'].copy()\n",
    "    labels=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "    # Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "df.columns\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "scores=remove_one_predict_mse(NPUNID_list,model,labels,id_col,df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Abundance  Abundance_Controls  Length     Mass  frac_aa_A  frac_aa_C  \\\n",
      "0       0.000000                 0.0   448.0  51797.0   0.042411   0.004464   \n",
      "1       0.000000                 0.0   779.0  85294.0   0.078306   0.019255   \n",
      "2       0.086801                 0.0   291.0  31570.0   0.082474   0.061856   \n",
      "3       0.060396                 0.0   255.0  28405.0   0.094118   0.015686   \n",
      "4       0.000000                 0.0   200.0  22875.0   0.050000   0.015000   \n",
      "...          ...                 ...     ...      ...        ...        ...   \n",
      "25558        NaN                 NaN     NaN      NaN        NaN        NaN   \n",
      "25575        NaN                 NaN     NaN      NaN        NaN        NaN   \n",
      "25595        NaN                 NaN     NaN      NaN        NaN        NaN   \n",
      "25637        NaN                 NaN     NaN      NaN        NaN        NaN   \n",
      "25737        NaN                 NaN     NaN      NaN        NaN        NaN   \n",
      "\n",
      "       frac_aa_D  frac_aa_E  frac_aa_F  frac_aa_G  ...  Dh_core  \\\n",
      "0       0.042411   0.095982   0.064732   0.044643  ...    680.0   \n",
      "1       0.051348   0.060334   0.034660   0.103979  ...    230.0   \n",
      "2       0.044674   0.054983   0.020619   0.099656  ...    105.0   \n",
      "3       0.066667   0.090196   0.035294   0.074510  ...    149.0   \n",
      "4       0.050000   0.035000   0.060000   0.055000  ...    410.0   \n",
      "...          ...        ...        ...        ...  ...      ...   \n",
      "25558        NaN        NaN        NaN        NaN  ...      NaN   \n",
      "25575        NaN        NaN        NaN        NaN  ...      NaN   \n",
      "25595        NaN        NaN        NaN        NaN  ...      NaN   \n",
      "25637        NaN        NaN        NaN        NaN  ...      NaN   \n",
      "25737        NaN        NaN        NaN        NaN  ...      NaN   \n",
      "\n",
      "       Dh_functionalized  Shaken  Centrifuged  ProteinID  \\\n",
      "0                  680.0     1.0          1.0        2.0   \n",
      "1                  230.0     1.0          0.0        1.0   \n",
      "2                  105.0     0.0          0.0        1.0   \n",
      "3                  229.0     1.0          0.0        1.0   \n",
      "4                  410.0     1.0          1.0        2.0   \n",
      "...                  ...     ...          ...        ...   \n",
      "25558                NaN     NaN          NaN        NaN   \n",
      "25575                NaN     NaN          NaN        NaN   \n",
      "25595                NaN     NaN          NaN        NaN   \n",
      "25637                NaN     NaN          NaN        NaN   \n",
      "25737                NaN     NaN          NaN        NaN   \n",
      "\n",
      "       NP_incubation Concentration (mg/mL)  Incubation Concentration (mg/ml)  \\\n",
      "0                                    125.0                               0.2   \n",
      "1                                      3.2                               2.0   \n",
      "2                                      3.2                               4.0   \n",
      "3                                      5.0                              40.0   \n",
      "4                                     62.5                               0.2   \n",
      "...                                    ...                               ...   \n",
      "25558                                  NaN                               NaN   \n",
      "25575                                  NaN                               NaN   \n",
      "25595                                  NaN                               NaN   \n",
      "25637                                  NaN                               NaN   \n",
      "25737                                  NaN                               NaN   \n",
      "\n",
      "       Incubation Time (minutes)  Temperature  binary_target  \n",
      "0                           60.0         25.0              0  \n",
      "1                           30.0         25.0              0  \n",
      "2                         1440.0         25.0              0  \n",
      "3                           30.0         25.0              0  \n",
      "4                           60.0         25.0              0  \n",
      "...                          ...          ...            ...  \n",
      "25558                        NaN          NaN              0  \n",
      "25575                        NaN          NaN              0  \n",
      "25595                        NaN          NaN              0  \n",
      "25637                        NaN          NaN              0  \n",
      "25737                        NaN          NaN              0  \n",
      "\n",
      "[25756 rows x 117 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat([df,labels_df],axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     16\u001B[0m RFE_Feats\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m\n\u001B[0;32m     17\u001B[0m selector \u001B[38;5;241m=\u001B[39m RFE(model, n_features_to_select\u001B[38;5;241m=\u001B[39mRFE_Feats, step\u001B[38;5;241m=\u001B[39mstep)\n\u001B[1;32m---> 18\u001B[0m selector \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_a\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_abund\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m selector\u001B[38;5;241m.\u001B[39msupport_\n\u001B[0;32m     20\u001B[0m ranking \u001B[38;5;241m=\u001B[39m selector\u001B[38;5;241m.\u001B[39mranking_\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:235\u001B[0m, in \u001B[0;36mRFE.fit\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001B[39;00m\n\u001B[0;32m    217\u001B[0m \n\u001B[0;32m    218\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:296\u001B[0m, in \u001B[0;36mRFE._fit\u001B[1;34m(self, X, y, step_score, **fit_params)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting estimator with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m features.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m np\u001B[38;5;241m.\u001B[39msum(support_))\n\u001B[1;32m--> 296\u001B[0m estimator\u001B[38;5;241m.\u001B[39mfit(X[:, features], y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    298\u001B[0m \u001B[38;5;66;03m# Get importance and rank them\u001B[39;00m\n\u001B[0;32m    299\u001B[0m importances \u001B[38;5;241m=\u001B[39m _get_feature_importances(\n\u001B[0;32m    300\u001B[0m     estimator,\n\u001B[0;32m    301\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimportance_getter,\n\u001B[0;32m    302\u001B[0m     transform_func\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msquare\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    303\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    465\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    466\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    468\u001B[0m ]\n\u001B[0;32m    470\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    475\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 476\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    486\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1046\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1047\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1050\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 861\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    778\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 779\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 117\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    187\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 189\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    191\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1342\u001B[0m, in \u001B[0;36mDecisionTreeRegressor.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m   1313\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1314\u001B[0m     \u001B[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[0;32m   1315\u001B[0m \n\u001B[0;32m   1316\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1339\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m   1340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1342\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1344\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1345\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1346\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1347\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1348\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001B[0m, in \u001B[0;36mBaseDecisionTree.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    447\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    448\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    449\u001B[0m         splitter,\n\u001B[0;32m    450\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    455\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    456\u001B[0m     )\n\u001B[1;32m--> 458\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    461\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "##Testing Each NP independently## Which ones are hard to predict and which ones are easy?\n",
    "\n",
    "df_filepath = 'Input_data/Save_files/df_3_all_30%.xlsx'\n",
    "df_a = pd.read_excel(df_filepath, header=0)\n",
    "df_a=df_a.drop(columns=['Entry', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5','Raw_FileID'])\n",
    "labels= df_a['Abundance'].copy()\n",
    "id_col= df_a['NPUNID'].copy()\n",
    "label_abund=np.ravel(labels)\n",
    "df_a=df_a.drop(columns=['Abundance','NPUNID']).copy()\n",
    "# NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "# NPUNID_list=[1]\n",
    "id_list=NPUNID_list\n",
    "# Initialize a list to hold MSE scores for each removed ID\n",
    "mse_scores = []\n",
    "\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "step = 2\n",
    "RFE_Feats=15\n",
    "selector = RFE(model, n_features_to_select=RFE_Feats, step=step)\n",
    "selector = selector.fit(df_a, label_abund)\n",
    "selector.support_\n",
    "ranking = selector.ranking_\n",
    "feat_list = selector.get_feature_names_out()\n",
    "df_a = df_a[feat_list]\n",
    "df_a=pd.concat([labels, id_col, df_a],axis=1)\n",
    "# Loop through each ID in the list\n",
    "for id in id_list:\n",
    "    df=df_a.copy()\n",
    "    # Remove the row with the current ID from the dataframe\n",
    "    removed_row = df.loc[df['NPUNID'] == id].copy()\n",
    "    df = df.loc[df['NPUNID'] != id].copy()\n",
    "\n",
    "    # Split the remaining data into features and target\n",
    "    X_train = df.drop(['Abundance', 'NPUNID'], axis=1)\n",
    "    y_train = df['Abundance']\n",
    "\n",
    "    # Fit a random forest regression model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained model to predict on the removed ID\n",
    "    X_test = removed_row.drop(['Abundance', 'NPUNID'], axis=1)\n",
    "    y_true = removed_row['Abundance']\n",
    "    print(X_test.shape)\n",
    "    print(y_true.shape)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE score and add it to the list\n",
    "    mse_score = mean_squared_error(y_true, y_pred)\n",
    "    mse_scores.append(mse_score)\n",
    "out=pd.DataFrame(list(zip(NPUNID_list,mse_score)), columns=['NPUNID','Prediction Score'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = 0.3\n",
    "multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "rfecv_ = True  #True_runs RFECV\n",
    "rfe_ = False     #True runs Recursive feature elimination\n",
    "abund_controls = True # True keeps the serum as an input feature\n",
    "splits = 10     #number of splits for cross validation across QC methods and feature selection methods\n",
    "in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "scorings=['r2','neg_mean_squared_error','neg_mean_absolute_error']\n",
    "RFE_Feats = 40\n",
    "model = RandomForestRegressor(n_estimators=80)\n",
    "# model=XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n",
    "summary_tmp=[]\n",
    "for z in scorings:\n",
    "    df=pd.read_excel('Input_data/Save_files/df_whole_noCon_drop30%zeros.xlsx')\n",
    "    labels_df= df['label_abund_df'].copy()\n",
    "    id_col= df['NPIDs'].copy()\n",
    "    labels=np.ravel(labels_df)\n",
    "    df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "    # Run PCA to seee how data differentiates#\n",
    "    # PCA_plot(df,label_abund,id)\n",
    "\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    if rfecv_ == True:\n",
    "        df = RFECV_plot(df,label_abund,model,id,folds=splits,step=2,scoring=z)\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFECV(40)_\"+id+\".xlsx\")\n",
    "\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    if rfe_ == True:\n",
    "        step = 2\n",
    "        estimator = RandomForestRegressor(n_estimators=100)\n",
    "        selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "        selector = selector.fit(df, label_abund)\n",
    "        selector.support_\n",
    "        ranking = selector.ranking_\n",
    "        feat_list = selector.get_feature_names_out()\n",
    "        df = df[feat_list]\n",
    "        df_out=pd.concat([df, NPIDs,label_abund_df],axis=1)\n",
    "        df_out.to_excel(\"Input_data/Save_files/df_RFE40_\"+id+\".xlsx\")\n",
    "\n",
    "    #Quality control\n",
    "    # scram_score(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop(df, label_abund, model, id, 0.2)\n",
    "    # feat_drop_multifold(df, label_abund, model, id, 0.2, folds=splits)\n",
    "    # tmp2=scorer(df, label_abund, model, id, 10)\n",
    "    # zeros=(raw_MS_data['Abundance']==0).sum()\n",
    "    # percent_zeros=zeros/raw_MS_data.shape[0]\n",
    "    # tmp2['TotalZeros']=zeros\n",
    "    # tmp2['Percent_zeros']=percent_zeros\n",
    "    # summary_tmp.append(tmp2)\n",
    "    # lasso_feature_selection(df, label_abund, id)\n",
    "    # summary=pd.concat(summary_tmp,axis=0)\n",
    "# summary.to_excel('Output_data/'+id+'.xlsx', index=False)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Drop a nanoparticle and then predict it###\n",
    "##NPs of interest##\n",
    "#easy to hard NPUNID 1,20,19,16,7,31,34,34\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "      Abundance_Controls  Length    Mass  frac_aa_A  frac_aa_C  frac_aa_D  \\\n0               0.000000     258   27890   0.073643   0.081395   0.046512   \n1               0.000100     441   48712   0.068027   0.011338   0.047619   \n2               0.002023     396   44471   0.050505   0.030303   0.058081   \n3               0.004530    2211  248983   0.050204   0.008593   0.065129   \n4               0.000000     258   27890   0.073643   0.081395   0.046512   \n...                  ...     ...     ...        ...        ...        ...   \n1765            0.139882     345   38252   0.057971   0.066667   0.028986   \n1766            0.343098     465   52347   0.064516   0.019355   0.045161   \n1767            0.000000     807   90976   0.052045   0.057001   0.054523   \n1768            1.120803     474   53342   0.059072   0.059072   0.059072   \n1769            0.009005    1463  138938   0.097744   0.012303   0.043746   \n\n      frac_aa_E  frac_aa_F  frac_aa_G  frac_aa_H  ...  Dh_core  \\\n0      0.077519   0.011628   0.100775   0.042636  ...      229   \n1      0.106576   0.018141   0.102041   0.013605  ...      229   \n2      0.047980   0.065657   0.070707   0.108586  ...      229   \n3      0.061511   0.034826   0.055631   0.027589  ...      229   \n4      0.077519   0.011628   0.100775   0.042636  ...      229   \n...         ...        ...        ...        ...  ...      ...   \n1765   0.057971   0.060870   0.069565   0.028986  ...      229   \n1766   0.075269   0.053763   0.049462   0.012903  ...      229   \n1767   0.086741   0.024783   0.064436   0.012392  ...      149   \n1768   0.084388   0.046414   0.029536   0.018987  ...      229   \n1769   0.051948   0.016405   0.265892   0.006152  ...      229   \n\n      Dh_functionalized  Shaken  Centrifuged  ProteinID  \\\n0                   316       1            0          1   \n1                   316       1            0          1   \n2                   218       1            0          1   \n3                   218       1            0          1   \n4                   282       1            0          1   \n...                 ...     ...          ...        ...   \n1765                291       1            0          1   \n1766                316       1            0          1   \n1767                226       1            0          1   \n1768                282       1            0          1   \n1769                609       1            0          1   \n\n      NP_incubation Concentration (mg/mL)  Incubation Concentration (mg/ml)  \\\n0                                       5                                 4   \n1                                       5                                 4   \n2                                       5                                40   \n3                                       5                                40   \n4                                       5                                 4   \n...                                   ...                               ...   \n1765                                    5                                40   \n1766                                    5                                40   \n1767                                    5                                 4   \n1768                                    5                                40   \n1769                                    5                                40   \n\n      Corona_Concentration (ug/mg)  Incubation Time (minutes)  Temperature  \n0                        27.929767                         30           25  \n1                        27.929767                         30           25  \n2                        73.474471                         30           25  \n3                        73.474471                         30           25  \n4                        52.085492                         30           25  \n...                            ...                        ...          ...  \n1765                    115.659641                         30           25  \n1766                     56.774264                         30           25  \n1767                    122.773587                         30           25  \n1768                    123.140432                         30           25  \n1769                     25.108958                         30           25  \n\n[1770 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Abundance_Controls</th>\n      <th>Length</th>\n      <th>Mass</th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_H</th>\n      <th>...</th>\n      <th>Dh_core</th>\n      <th>Dh_functionalized</th>\n      <th>Shaken</th>\n      <th>Centrifuged</th>\n      <th>ProteinID</th>\n      <th>NP_incubation Concentration (mg/mL)</th>\n      <th>Incubation Concentration (mg/ml)</th>\n      <th>Corona_Concentration (ug/mg)</th>\n      <th>Incubation Time (minutes)</th>\n      <th>Temperature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000100</td>\n      <td>441</td>\n      <td>48712</td>\n      <td>0.068027</td>\n      <td>0.011338</td>\n      <td>0.047619</td>\n      <td>0.106576</td>\n      <td>0.018141</td>\n      <td>0.102041</td>\n      <td>0.013605</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002023</td>\n      <td>396</td>\n      <td>44471</td>\n      <td>0.050505</td>\n      <td>0.030303</td>\n      <td>0.058081</td>\n      <td>0.047980</td>\n      <td>0.065657</td>\n      <td>0.070707</td>\n      <td>0.108586</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004530</td>\n      <td>2211</td>\n      <td>248983</td>\n      <td>0.050204</td>\n      <td>0.008593</td>\n      <td>0.065129</td>\n      <td>0.061511</td>\n      <td>0.034826</td>\n      <td>0.055631</td>\n      <td>0.027589</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>0.139882</td>\n      <td>345</td>\n      <td>38252</td>\n      <td>0.057971</td>\n      <td>0.066667</td>\n      <td>0.028986</td>\n      <td>0.057971</td>\n      <td>0.060870</td>\n      <td>0.069565</td>\n      <td>0.028986</td>\n      <td>...</td>\n      <td>229</td>\n      <td>291</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>115.659641</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1766</th>\n      <td>0.343098</td>\n      <td>465</td>\n      <td>52347</td>\n      <td>0.064516</td>\n      <td>0.019355</td>\n      <td>0.045161</td>\n      <td>0.075269</td>\n      <td>0.053763</td>\n      <td>0.049462</td>\n      <td>0.012903</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>56.774264</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1767</th>\n      <td>0.000000</td>\n      <td>807</td>\n      <td>90976</td>\n      <td>0.052045</td>\n      <td>0.057001</td>\n      <td>0.054523</td>\n      <td>0.086741</td>\n      <td>0.024783</td>\n      <td>0.064436</td>\n      <td>0.012392</td>\n      <td>...</td>\n      <td>149</td>\n      <td>226</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>122.773587</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1768</th>\n      <td>1.120803</td>\n      <td>474</td>\n      <td>53342</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.084388</td>\n      <td>0.046414</td>\n      <td>0.029536</td>\n      <td>0.018987</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>123.140432</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1769</th>\n      <td>0.009005</td>\n      <td>1463</td>\n      <td>138938</td>\n      <td>0.097744</td>\n      <td>0.012303</td>\n      <td>0.043746</td>\n      <td>0.051948</td>\n      <td>0.016405</td>\n      <td>0.265892</td>\n      <td>0.006152</td>\n      <td>...</td>\n      <td>229</td>\n      <td>609</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>25.108958</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n<p>1770 rows  95 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Drop a nanoparticle and then predict it###\n",
    "##NPs of interest##\n",
    "#easy to hard NPUNID 1,20,19,16,7,31,34,34\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Abundance_Controls, Length, Mass, frac_aa_A, frac_aa_C, frac_aa_D, frac_aa_E, frac_aa_F, frac_aa_G, frac_aa_H, frac_aa_I, frac_aa_K, frac_aa_L, frac_aa_M, frac_aa_N, frac_aa_P, frac_aa_Q, frac_aa_R, frac_aa_S, frac_aa_T, frac_aa_V, frac_aa_W, frac_aa_Y, molecular_weight, aromaticity, instability_index, flexibility_mean, flexibility_std, flexibility_var, flexibility_max, flexibility_min, flexibility_median, isoelectric_point, secondary_structure_fraction_helix, secondary_structure_fraction_turn, secondary_structure_fraction_sheet, secondary_structure_fraction_disordered, gravy, fraction_exposed, fraction_buried, fraction_exposed_nonpolar_total, fraction_exposed_nonpolar_exposed, fraction_exposed_polar_total, fraction_exposed_polar_exposed, rsa_mean, rsa_median, rsa_std, asa_sum, fraction_exposed_exposed_A, fraction_exposed_exposed_C, fraction_exposed_exposed_D, fraction_exposed_exposed_E, fraction_exposed_exposed_F, fraction_exposed_exposed_G, fraction_exposed_exposed_H, fraction_exposed_exposed_I, fraction_exposed_exposed_K, fraction_exposed_exposed_L, fraction_exposed_exposed_M, fraction_exposed_exposed_N, fraction_exposed_exposed_P, fraction_exposed_exposed_Q, fraction_exposed_exposed_R, fraction_exposed_exposed_S, fraction_exposed_exposed_T, fraction_exposed_exposed_V, fraction_exposed_exposed_W, fraction_exposed_exposed_Y, nsp_secondary_structure_coil, nsp_secondary_structure_sheet, nsp_secondary_structure_helix, nsp_disordered, BatchID, Zeta Potential, Core Material, Ligand_Carboxylate, Ligand_BSA, Ligand_Amine, Ligand_Citrate, Ligand_PEG, Ligand_PEI, Ligand_PVP, Ligand_Au, Surface_Ligand, Dtem, Dh_core, Dh_functionalized, Shaken, Centrifuged, ProteinID, NP_incubation Concentration (mg/mL), Incubation Concentration (mg/ml), Corona_Concentration (ug/mg), Incubation Time (minutes), Temperature, NPUNID, Abundance]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 98 columns]\n"
     ]
    }
   ],
   "source": [
    "NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "df_filepath='Input_data/Save_files/df_whole_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "# df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Remove the row with the current ID from the dataframe\n",
    "removed_row = df.loc[df['NPUNID'] == NPUNID_list[0]].copy()\n",
    "print(removed_row)\n",
    "# df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "#\n",
    "# # Split the remaining data into features and target\n",
    "# X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "# y_train = df[labels.name]\n",
    "#\n",
    "# # Fit a random forest regression model to the training data\n",
    "# model.fit(X_train, y_train)\n",
    "#\n",
    "# # Use the trained model to predict on the removed ID\n",
    "# X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "# y_true = removed_row[labels.name]\n",
    "# y_pred = model.predict(X_test)\n",
    "#\n",
    "# # Calculate the MSE score and add it to the list\n",
    "# mse_score = mean_squared_error(y_true, y_pred)\n",
    "# mse_scores.append(mse_score)\n",
    "#\n",
    "# # Add the removed row back into the dataframe\n",
    "# df = pd.concat([df, removed_row], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 96)) while a minimum of 1 is required by RandomForestRegressor.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m df\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[0;32m      8\u001B[0m model\u001B[38;5;241m=\u001B[39mRandomForestRegressor(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m scores\u001B[38;5;241m=\u001B[39m\u001B[43mremove_one_predict_mse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNPUNID_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43mid_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36mremove_one_predict_mse\u001B[1;34m(id_list, model, labels, id_col, data)\u001B[0m\n\u001B[0;32m     32\u001B[0m X_test \u001B[38;5;241m=\u001B[39m removed_row\u001B[38;5;241m.\u001B[39mdrop([labels\u001B[38;5;241m.\u001B[39mname, id_col\u001B[38;5;241m.\u001B[39mname], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     33\u001B[0m y_true \u001B[38;5;241m=\u001B[39m removed_row[labels\u001B[38;5;241m.\u001B[39mname]\n\u001B[1;32m---> 34\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Calculate the MSE score and add it to the list\u001B[39;00m\n\u001B[0;32m     37\u001B[0m mse_score \u001B[38;5;241m=\u001B[39m mean_squared_error(y_true, y_pred)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:991\u001B[0m, in \u001B[0;36mForestRegressor.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    989\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    990\u001B[0m \u001B[38;5;66;03m# Check data\u001B[39;00m\n\u001B[1;32m--> 991\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_X_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    993\u001B[0m \u001B[38;5;66;03m# Assign chunk of trees to jobs\u001B[39;00m\n\u001B[0;32m    994\u001B[0m n_jobs, _, _ \u001B[38;5;241m=\u001B[39m _partition_estimators(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_estimators, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:605\u001B[0m, in \u001B[0;36mBaseForest._validate_X_predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    602\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    603\u001B[0m \u001B[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001B[39;00m\n\u001B[0;32m    604\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 605\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m issparse(X) \u001B[38;5;129;01mand\u001B[39;00m (X\u001B[38;5;241m.\u001B[39mindices\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc \u001B[38;5;129;01mor\u001B[39;00m X\u001B[38;5;241m.\u001B[39mindptr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc):\n\u001B[0;32m    607\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo support for np.int64 index based sparse matrices\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:577\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 577\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    578\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:909\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    907\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n\u001B[0;32m    908\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_samples \u001B[38;5;241m<\u001B[39m ensure_min_samples:\n\u001B[1;32m--> 909\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    910\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    911\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    912\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_samples, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_samples, context)\n\u001B[0;32m    913\u001B[0m         )\n\u001B[0;32m    915\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    916\u001B[0m     n_features \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: Found array with 0 sample(s) (shape=(0, 96)) while a minimum of 1 is required by RandomForestRegressor."
     ]
    }
   ],
   "source": [
    "NPUNID_list=[1,20,19,16,7,31,34,43,44]\n",
    "df_filepath='Input_data/Save_files/df_whole_Con_drop_30%zeros.xlsx'\n",
    "df = pd.read_excel(df_filepath, header=0)\n",
    "labels= df['Abundance'].copy()\n",
    "id_col= df['NPUNID'].copy()\n",
    "df.drop(columns=['Abundance','NPUNID'],inplace=True)\n",
    "df.columns\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "scores=remove_one_predict_mse(NPUNID_list,model,labels,id_col,df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "      Abundance_Controls  Length    Mass  frac_aa_A  frac_aa_C  frac_aa_D  \\\n0               0.000000     258   27890   0.073643   0.081395   0.046512   \n1               0.000100     441   48712   0.068027   0.011338   0.047619   \n2               0.002023     396   44471   0.050505   0.030303   0.058081   \n3               0.004530    2211  248983   0.050204   0.008593   0.065129   \n4               0.000000     258   27890   0.073643   0.081395   0.046512   \n...                  ...     ...     ...        ...        ...        ...   \n1765            0.139882     345   38252   0.057971   0.066667   0.028986   \n1766            0.343098     465   52347   0.064516   0.019355   0.045161   \n1767            0.000000     807   90976   0.052045   0.057001   0.054523   \n1768            1.120803     474   53342   0.059072   0.059072   0.059072   \n1769            0.009005    1463  138938   0.097744   0.012303   0.043746   \n\n      frac_aa_E  frac_aa_F  frac_aa_G  frac_aa_H  ...  Dh_core  \\\n0      0.077519   0.011628   0.100775   0.042636  ...      229   \n1      0.106576   0.018141   0.102041   0.013605  ...      229   \n2      0.047980   0.065657   0.070707   0.108586  ...      229   \n3      0.061511   0.034826   0.055631   0.027589  ...      229   \n4      0.077519   0.011628   0.100775   0.042636  ...      229   \n...         ...        ...        ...        ...  ...      ...   \n1765   0.057971   0.060870   0.069565   0.028986  ...      229   \n1766   0.075269   0.053763   0.049462   0.012903  ...      229   \n1767   0.086741   0.024783   0.064436   0.012392  ...      149   \n1768   0.084388   0.046414   0.029536   0.018987  ...      229   \n1769   0.051948   0.016405   0.265892   0.006152  ...      229   \n\n      Dh_functionalized  Shaken  Centrifuged  ProteinID  \\\n0                   316       1            0          1   \n1                   316       1            0          1   \n2                   218       1            0          1   \n3                   218       1            0          1   \n4                   282       1            0          1   \n...                 ...     ...          ...        ...   \n1765                291       1            0          1   \n1766                316       1            0          1   \n1767                226       1            0          1   \n1768                282       1            0          1   \n1769                609       1            0          1   \n\n      NP_incubation Concentration (mg/mL)  Incubation Concentration (mg/ml)  \\\n0                                       5                                 4   \n1                                       5                                 4   \n2                                       5                                40   \n3                                       5                                40   \n4                                       5                                 4   \n...                                   ...                               ...   \n1765                                    5                                40   \n1766                                    5                                40   \n1767                                    5                                 4   \n1768                                    5                                40   \n1769                                    5                                40   \n\n      Corona_Concentration (ug/mg)  Incubation Time (minutes)  Temperature  \n0                        27.929767                         30           25  \n1                        27.929767                         30           25  \n2                        73.474471                         30           25  \n3                        73.474471                         30           25  \n4                        52.085492                         30           25  \n...                            ...                        ...          ...  \n1765                    115.659641                         30           25  \n1766                     56.774264                         30           25  \n1767                    122.773587                         30           25  \n1768                    123.140432                         30           25  \n1769                     25.108958                         30           25  \n\n[1770 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Abundance_Controls</th>\n      <th>Length</th>\n      <th>Mass</th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_H</th>\n      <th>...</th>\n      <th>Dh_core</th>\n      <th>Dh_functionalized</th>\n      <th>Shaken</th>\n      <th>Centrifuged</th>\n      <th>ProteinID</th>\n      <th>NP_incubation Concentration (mg/mL)</th>\n      <th>Incubation Concentration (mg/ml)</th>\n      <th>Corona_Concentration (ug/mg)</th>\n      <th>Incubation Time (minutes)</th>\n      <th>Temperature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000100</td>\n      <td>441</td>\n      <td>48712</td>\n      <td>0.068027</td>\n      <td>0.011338</td>\n      <td>0.047619</td>\n      <td>0.106576</td>\n      <td>0.018141</td>\n      <td>0.102041</td>\n      <td>0.013605</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>27.929767</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002023</td>\n      <td>396</td>\n      <td>44471</td>\n      <td>0.050505</td>\n      <td>0.030303</td>\n      <td>0.058081</td>\n      <td>0.047980</td>\n      <td>0.065657</td>\n      <td>0.070707</td>\n      <td>0.108586</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004530</td>\n      <td>2211</td>\n      <td>248983</td>\n      <td>0.050204</td>\n      <td>0.008593</td>\n      <td>0.065129</td>\n      <td>0.061511</td>\n      <td>0.034826</td>\n      <td>0.055631</td>\n      <td>0.027589</td>\n      <td>...</td>\n      <td>229</td>\n      <td>218</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>73.474471</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>258</td>\n      <td>27890</td>\n      <td>0.073643</td>\n      <td>0.081395</td>\n      <td>0.046512</td>\n      <td>0.077519</td>\n      <td>0.011628</td>\n      <td>0.100775</td>\n      <td>0.042636</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>52.085492</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>0.139882</td>\n      <td>345</td>\n      <td>38252</td>\n      <td>0.057971</td>\n      <td>0.066667</td>\n      <td>0.028986</td>\n      <td>0.057971</td>\n      <td>0.060870</td>\n      <td>0.069565</td>\n      <td>0.028986</td>\n      <td>...</td>\n      <td>229</td>\n      <td>291</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>115.659641</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1766</th>\n      <td>0.343098</td>\n      <td>465</td>\n      <td>52347</td>\n      <td>0.064516</td>\n      <td>0.019355</td>\n      <td>0.045161</td>\n      <td>0.075269</td>\n      <td>0.053763</td>\n      <td>0.049462</td>\n      <td>0.012903</td>\n      <td>...</td>\n      <td>229</td>\n      <td>316</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>56.774264</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1767</th>\n      <td>0.000000</td>\n      <td>807</td>\n      <td>90976</td>\n      <td>0.052045</td>\n      <td>0.057001</td>\n      <td>0.054523</td>\n      <td>0.086741</td>\n      <td>0.024783</td>\n      <td>0.064436</td>\n      <td>0.012392</td>\n      <td>...</td>\n      <td>149</td>\n      <td>226</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>122.773587</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1768</th>\n      <td>1.120803</td>\n      <td>474</td>\n      <td>53342</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.059072</td>\n      <td>0.084388</td>\n      <td>0.046414</td>\n      <td>0.029536</td>\n      <td>0.018987</td>\n      <td>...</td>\n      <td>229</td>\n      <td>282</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>123.140432</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1769</th>\n      <td>0.009005</td>\n      <td>1463</td>\n      <td>138938</td>\n      <td>0.097744</td>\n      <td>0.012303</td>\n      <td>0.043746</td>\n      <td>0.051948</td>\n      <td>0.016405</td>\n      <td>0.265892</td>\n      <td>0.006152</td>\n      <td>...</td>\n      <td>229</td>\n      <td>609</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>40</td>\n      <td>25.108958</td>\n      <td>30</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n<p>1770 rows  95 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot index with multidimensional key",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Loop through each ID in the list\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m id_to_remove \u001B[38;5;129;01min\u001B[39;00m id_list:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Remove the row with the current ID from the dataframe\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m     removed_row \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mid_col\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mid_to_remove\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     10\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mloc[df[id_col\u001B[38;5;241m.\u001B[39mname] \u001B[38;5;241m!=\u001B[39m id_to_remove]\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# Split the remaining data into features and target\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:967\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    964\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    966\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[1;32m--> 967\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1189\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(labels, MultiIndex)):\n\u001B[0;32m   1188\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndim\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m-> 1189\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot index with multidimensional key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_iterable(key, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[0;32m   1193\u001B[0m \u001B[38;5;66;03m# nested tuple slicing\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot index with multidimensional key"
     ]
    }
   ],
   "source": [
    "df = pd.concat([labels, id_col, df], axis=1)\n",
    "id_list=NPUNID_list\n",
    "# Initialize a list to hold MSE scores for each removed ID\n",
    "mse_scores = []\n",
    "\n",
    "# Loop through each ID in the list\n",
    "for id_to_remove in id_list:\n",
    "    # Remove the row with the current ID from the dataframe\n",
    "    removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "    df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "    # Split the remaining data into features and target\n",
    "    X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "    y_train = df[labels.name]\n",
    "\n",
    "    # Fit a random forest regression model to the training data\n",
    "    # model.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained model to predict on the removed ID\n",
    "    X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "    y_true = removed_row[labels.name]\n",
    "    print(X_test.shape)\n",
    "    print(y_true.shape)\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE score and add it to the list\n",
    "    # mse_score = mean_squared_error(y_true, y_pred)\n",
    "    # mse_scores.append(mse_score)\n",
    "    #\n",
    "    # Add the removed row back into the dataframe\n",
    "    df = pd.concat([df, removed_row], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x=[1,2,3,4]\n",
    "y=[1,2,3,4]\n",
    "y2=[2,4,6,8]\n",
    "y3=[4,8,12,16]\n",
    "\n",
    "plt.plot(x, y, color='tab:blue', marker='o',label='pearson')\n",
    "plt.plot(x, y2, color='tab:red', marker='s',label='R2')\n",
    "plt.plot(x, y3, color='tab:green', marker='^',label='mse')\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Score as a function of Feat Drop\\n'+id)\n",
    "plt.savefig('Output_data/feat_drop_cumulative' + id + '.png', bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'adjusted_mutual_info_score',\n 'adjusted_rand_score',\n 'average_precision',\n 'balanced_accuracy',\n 'completeness_score',\n 'explained_variance',\n 'f1',\n 'f1_macro',\n 'f1_micro',\n 'f1_samples',\n 'f1_weighted',\n 'fowlkes_mallows_score',\n 'homogeneity_score',\n 'jaccard',\n 'jaccard_macro',\n 'jaccard_micro',\n 'jaccard_samples',\n 'jaccard_weighted',\n 'matthews_corrcoef',\n 'max_error',\n 'mutual_info_score',\n 'neg_brier_score',\n 'neg_log_loss',\n 'neg_mean_absolute_error',\n 'neg_mean_absolute_percentage_error',\n 'neg_mean_gamma_deviance',\n 'neg_mean_poisson_deviance',\n 'neg_mean_squared_error',\n 'neg_mean_squared_log_error',\n 'neg_median_absolute_error',\n 'neg_root_mean_squared_error',\n 'normalized_mutual_info_score',\n 'precision',\n 'precision_macro',\n 'precision_micro',\n 'precision_samples',\n 'precision_weighted',\n 'r2',\n 'rand_score',\n 'recall',\n 'recall_macro',\n 'recall_micro',\n 'recall_samples',\n 'recall_weighted',\n 'roc_auc',\n 'roc_auc_ovo',\n 'roc_auc_ovo_weighted',\n 'roc_auc_ovr',\n 'roc_auc_ovr_weighted',\n 'top_k_accuracy',\n 'v_measure_score']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Get total number of rows\n",
    "total_count = len(df)\n",
    "df = df.rename(columns=lambda x: x.replace('/', ''))\n",
    "# Loop over each column of the dataframe\n",
    "for col in df.columns:\n",
    "    # Create a histogram of the current column, with bins=10\n",
    "    data=df[col]\n",
    "    plt.hist(data, weights=np.ones_like(data) / len(data))\n",
    "\n",
    "    # Set the title of the histogram to the column name and percent of total population\n",
    "    plt.title(str(col))\n",
    "    plt.savefig('Output_data/{}.png'.format(str(col)))\n",
    "    plt.close()\n",
    "    # Show the histogram\n",
    "    # plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'adjusted_mutual_info_score',\n 'adjusted_rand_score',\n 'average_precision',\n 'balanced_accuracy',\n 'completeness_score',\n 'explained_variance',\n 'f1',\n 'f1_macro',\n 'f1_micro',\n 'f1_samples',\n 'f1_weighted',\n 'fowlkes_mallows_score',\n 'homogeneity_score',\n 'jaccard',\n 'jaccard_macro',\n 'jaccard_micro',\n 'jaccard_samples',\n 'jaccard_weighted',\n 'matthews_corrcoef',\n 'max_error',\n 'mutual_info_score',\n 'neg_brier_score',\n 'neg_log_loss',\n 'neg_mean_absolute_error',\n 'neg_mean_absolute_percentage_error',\n 'neg_mean_gamma_deviance',\n 'neg_mean_poisson_deviance',\n 'neg_mean_squared_error',\n 'neg_mean_squared_log_error',\n 'neg_median_absolute_error',\n 'neg_root_mean_squared_error',\n 'normalized_mutual_info_score',\n 'precision',\n 'precision_macro',\n 'precision_micro',\n 'precision_samples',\n 'precision_weighted',\n 'r2',\n 'rand_score',\n 'recall',\n 'recall_macro',\n 'recall_micro',\n 'recall_samples',\n 'recall_weighted',\n 'roc_auc',\n 'roc_auc_ovo',\n 'roc_auc_ovo_weighted',\n 'roc_auc_ovr',\n 'roc_auc_ovr_weighted',\n 'top_k_accuracy',\n 'v_measure_score']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   pearson_mean  pearson_std  R2_mean  R2_std  MSE_mean  MSE_std  \\\n0            10           23        4       3         1        2   \n\n   Number of Features  hi  \n0                  10  hi  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pearson_mean</th>\n      <th>pearson_std</th>\n      <th>R2_mean</th>\n      <th>R2_std</th>\n      <th>MSE_mean</th>\n      <th>MSE_std</th>\n      <th>Number of Features</th>\n      <th>hi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>23</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>10</td>\n      <td>hi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_mean=10\n",
    "pearson_std=23\n",
    "R2_mean=4\n",
    "R2_std=3\n",
    "MSE_mean=1\n",
    "MSE_std=2\n",
    "id='hi'\n",
    "data=[[pearson_mean,pearson_std,R2_mean,R2_std,MSE_mean,MSE_std,10,id]]\n",
    "scores=pd.DataFrame(data,columns=['pearson_mean','pearson_std','R2_mean','R2_std','MSE_mean','MSE_std','Number of Features',str(id)])\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def remove_one_predict_mse(id_list, model, labels, id_col, data):\n",
    "    \"\"\"\n",
    "    This function takes in a list of IDs, a model, labels, the ID column, and a dataset.\n",
    "    It combines the labels, ID column, and dataframe. Then it removes one ID at a time from the\n",
    "    dataframe, trains a random forest regression model, and predicts on the removed UNIDs.\n",
    "    This prediction is then scored using mean squared error.\n",
    "    \"\"\"\n",
    "    # Combine labels, ID column, and data into one dataframe\n",
    "    df = pd.concat([labels, id_col, data], axis=1)\n",
    "\n",
    "    # Initialize a list to hold MSE scores for each removed ID\n",
    "    mse_scores = []\n",
    "\n",
    "    # Loop through each ID in the list\n",
    "    for id_to_remove in id_list:\n",
    "        # Remove the row with the current ID from the dataframe\n",
    "        removed_row = df.loc[df[id_col.name] == id_to_remove].copy()\n",
    "        df = df.loc[df[id_col.name] != id_to_remove].copy()\n",
    "\n",
    "        # Split the remaining data into features and target\n",
    "        X_train = df.drop([labels.name, id_col.name], axis=1)\n",
    "        y_train = df[labels.name]\n",
    "\n",
    "        # Fit a random forest regression model to the training data\n",
    "        rf_model = RandomForestRegressor()\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Use the trained model to predict on the removed ID\n",
    "        X_test = removed_row.drop([labels.name, id_col.name], axis=1)\n",
    "        y_true = removed_row[labels.name]\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "\n",
    "        # Calculate the MSE score and add it to the list\n",
    "        mse_score = mean_squared_error(y_true, y_pred)\n",
    "        mse_scores.append(mse_score)\n",
    "\n",
    "        # Add the removed row back into the dataframe\n",
    "        df = pd.concat([df, removed_row], axis=0)\n",
    "\n",
    "    # Return the list of MSE scores\n",
    "    return mse_scores\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

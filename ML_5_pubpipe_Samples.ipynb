{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NormBALFsamples.xlsx', 'Norm_Intensity _all20230403.xlsx']\n"
     ]
    }
   ],
   "source": [
    "#Editable Variables\n",
    "multi_files=True #set to false if you just want to set one  prot_abund_file\n",
    "in_dir=\"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file='Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file='Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "model=RandomForestRegressor(n_estimators=150)\n",
    "# take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "files = os.listdir(in_dir)\n",
    "print(files)\n",
    "if multi_files == True:\n",
    "    for i,f in enumerate(files):\n",
    "        print(i)\n",
    "        if i==0:\n",
    "            raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "            # print(raw_MS_data)\n",
    "            raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "            print('BALF',raw_MS_data.shape)\n",
    "        else:\n",
    "            print(i)\n",
    "            temp = pd.read_excel(in_dir+f,header=0)\n",
    "            temp = pd.melt(temp, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "            print('Bovine',temp.shape)\n",
    "            # print(temp)\n",
    "            # print(temp)\n",
    "            raw_MS_data2=pd.concat([raw_MS_data,temp])\n",
    "            print('merge',raw_MS_data2.shape)\n",
    "            # print('did it')\n",
    "else:\n",
    "    raw_MS_data2=pd.read_excel(prot_abund_file,header=0)\n",
    "# raw_MS_data2\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset\n",
    "# raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before dropping rows (474, 14)\n",
      "shape after dropping rows (220, 14)\n",
      "shape before dropping rows (2860, 3)\n",
      "shape after dropping rows (2860, 3)\n",
      "final shape after melt (21945, 3)\n",
      "number of zeros in the dataset: 11050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmp95\\AppData\\Local\\Temp\\ipykernel_2504\\3623398263.py:115: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  plt.colorbar()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer ran successfully\n",
      "Scramble Scoring ran successfully\n",
      "feat drop ran successfully\n",
      "shape before dropping rows (474, 14)\n",
      "shape after dropping rows (52, 14)\n",
      "shape before dropping rows (676, 3)\n",
      "shape after dropping rows (676, 3)\n",
      "final shape after melt (10191, 3)\n",
      "number of zeros in the dataset: 2066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmp95\\AppData\\Local\\Temp\\ipykernel_2504\\3623398263.py:115: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  plt.colorbar()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer ran successfully\n",
      "Scramble Scoring ran successfully\n",
      "feat drop ran successfully\n",
      "shape before dropping rows (474, 14)\n",
      "shape after dropping rows (27, 14)\n",
      "shape before dropping rows (351, 3)\n",
      "shape after dropping rows (351, 3)\n",
      "final shape after melt (6841, 3)\n",
      "number of zeros in the dataset: 752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmp95\\AppData\\Local\\Temp\\ipykernel_2504\\3623398263.py:115: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  plt.colorbar()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer ran successfully\n",
      "Scramble Scoring ran successfully\n",
      "feat drop ran successfully\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Editable Variables\n",
    "#list of test filters\n",
    "zerosperrow = [0.9, 0.5, 0.3]\n",
    "for z in zerosperrow:\n",
    "    multi_files = True  #set to false if you just want to set one  prot_abund_file\n",
    "    in_dir = \"Input_data/Proteomic data/Abundance2/\"\n",
    "    prot_abund_file = 'Input_data/Proteomic data/Abundance2/Norm_Intensity _all20230403.xlsx'\n",
    "    NP_filepath = 'Input_data/NPs/NP_Database.xlsx'\n",
    "    controls_file = 'Input_data/Proteomic data/controls_combined.xlsx'\n",
    "    uniprot_filepath = 'Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "    NSPfilePath = 'Input_data/NetSurfP_data/Combined.xlsx'\n",
    "    id = 'all_NoCon_RFE40_droprows' + str(z) + 'zeros'\n",
    "    RFE_Feats = 40\n",
    "    model = RandomForestRegressor(n_estimators=150)\n",
    "    # take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "    # melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "    files = os.listdir(in_dir)\n",
    "    if multi_files == True:\n",
    "        for i, f in enumerate(files):\n",
    "            if i == 0:\n",
    "                raw_MS_data = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = raw_MS_data.shape[1]\n",
    "                cutoff = int(z * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                # print(raw_MS_data)\n",
    "                raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp = pd.read_excel(in_dir + f, header=0)\n",
    "                cols = temp.shape[1]\n",
    "                cutoff = int(z * cols)\n",
    "                print('shape before dropping rows', raw_MS_data.shape)\n",
    "                temp = temp.drop(temp[(temp == 0).sum(axis=1) >= cutoff].index)\n",
    "                print('shape after dropping rows', raw_MS_data.shape)\n",
    "                temp = pd.melt(temp, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "                raw_MS_data = pd.concat([raw_MS_data, temp])\n",
    "                print('final shape after melt', raw_MS_data.shape)\n",
    "                print('number of zeros in the dataset:',(raw_MS_data == 0).sum().sum())\n",
    "\n",
    "    else:\n",
    "        raw_MS_data = pd.read_excel(prot_abund_file, header=0)\n",
    "        cols = raw_MS_data.shape[1]\n",
    "        cutoff = int(z * cols)\n",
    "        print('shape beofre dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = raw_MS_data.drop(raw_MS_data[(raw_MS_data == 0).sum(axis=1) >= cutoff].index)\n",
    "        print('shape after dropping rows', raw_MS_data.shape)\n",
    "        raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'], var_name='Sample_num', value_name='Abundance')\n",
    "    #remove prots that were added due to merge\n",
    "    raw_MS_data = raw_MS_data.dropna()\n",
    "    ###Bring in controls (MS data for serums)##\n",
    "    controls = pd.read_excel(controls_file, header=0)\n",
    "    MS_data_controls = pd.merge(raw_MS_data, controls, how='left', on='Entry')\n",
    "    ###Bring in Uniprot_data,NSPdata and NP data##\n",
    "    uniprot_dat = pd.read_excel(uniprot_filepath, header=0)\n",
    "    NSP_data = pd.read_excel(NSPfilePath)\n",
    "    ###Bring in NP data and merge to get complete NP dataset###\n",
    "    NPUNdata = pd.read_excel(NP_filepath, header=0, sheet_name='NPUNID')\n",
    "    NPprop = pd.read_excel(NP_filepath, header=0, sheet_name='NP_Props')\n",
    "    NPdata = pd.merge(NPUNdata, NPprop, how=\"left\", on='NPID')\n",
    "    NPdata.dropna(inplace=True)\n",
    "    #calculate Enrichment\n",
    "    #####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "    # MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "    # MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "    #keep abundance Controls\n",
    "    # MS_data=MS_data_controls.drop(columns=['Abundance'])\n",
    "    raw_prop_data = pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left', on='Entry')\n",
    "    Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']), how='left',\n",
    "                                     on='Entry')  #merges netsurfp features and biopython features\n",
    "    Protein_data_complete.fillna(0, inplace=True)\n",
    "    #creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "    for df in [Protein_data_complete]:\n",
    "        for col in ['asa_sum']:\n",
    "            df[col + '_normalized'] = df[col] / df['Mass']\n",
    "\n",
    "    data_complete = pd.merge(Protein_data_complete, NPdata, how='inner', on='Sample_num')\n",
    "    data_complete.drop(columns=['notes', 'Notes', 'NPUNID'], inplace=True)\n",
    "    data_complete.fillna(0, inplace=True)\n",
    "    data_complete = data_complete.replace([-np.inf], '-12')\n",
    "    data_complete = data_complete.replace([np.inf], '12')\n",
    "    #create ordinal variables\n",
    "    # data_complete2=pd.get_dummies(data_complete, columns=['Core Material','Surface_Ligand'])\n",
    "    le = LabelEncoder()\n",
    "    data_complete['Core Material'] = le.fit_transform(data_complete['Core Material'])\n",
    "    data_complete['Surface_Ligand'] = le.fit_transform(data_complete['Surface_Ligand'])\n",
    "\n",
    "    #set labels (what we are trying to predict) as Enrichment column\n",
    "    # labels=data_complete['Enrichment'].copy()\n",
    "    label_abund = np.ravel(data_complete['Abundance'].copy())\n",
    "    label_abund_df = pd.DataFrame(label_abund)\n",
    "    # label_enrich=np.ravel(data_complete['Enrichment'].copy())\n",
    "    #make it one dimenisional\n",
    "    #drop qualitative, not neccessary, and label columns\n",
    "    #create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "    to_drop = data_complete.filter(like='total_exposed_')\n",
    "    data_complete.drop(columns=to_drop, inplace=True)\n",
    "    df = data_complete.drop(\n",
    "        ['Entry', 'Abundance', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5',\n",
    "         'Raw_FileID'], axis=1)\n",
    "    df.drop(columns=['Abundance_Controls'], inplace=True)\n",
    "\n",
    "    # df.to_excel(\"Input_data/Save_files/df_\"+id+\".xlsx\")\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "\n",
    "    #Run PCA to seee how data differentiates#\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(df)\n",
    "    pca = PCA(n_components=5)\n",
    "    x_pca = pca.fit_transform(X_std)\n",
    "    plt.scatter(x_pca[:, 0], x_pca[:, 1], c=label_abund, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.savefig('Output_data/PCA' + id + '.png')\n",
    "    plt.close('all')\n",
    "    #use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "    step = 2\n",
    "    estimator = RandomForestRegressor(n_estimators=100)\n",
    "    selector = RFE(estimator, n_features_to_select=RFE_Feats, step=step)\n",
    "    selector = selector.fit(df, label_abund)\n",
    "    selector.support_\n",
    "    ranking = selector.ranking_\n",
    "    feat_list = selector.get_feature_names_out()\n",
    "    df = df[feat_list]\n",
    "\n",
    "    #run Recursive feature elimination with cross validation\n",
    "    id2 = 'dropped_controlAbundance'\n",
    "    step = 2\n",
    "    min_feats = 6\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    estimator = RandomForestRegressor(n_estimators=100)\n",
    "    selector = RFECV(estimator=estimator, cv=cv, scoring='neg_mean_squared_error', min_features_to_select=min_feats, step=step)\n",
    "    selector = selector.fit(df, label_abund)\n",
    "    selector.support_\n",
    "    feat_list2 = selector.get_feature_names_out()\n",
    "    selected_features = df.columns[selector.support_]\n",
    "    df = df[feat_list2]\n",
    "    # df.to_excel(\"Input_data/Save_files/df_RFECV\"+id+id2+\".xlsx\")\n",
    "    # rfecv_df=pd.DataFrame(selector.cv_results_)\n",
    "    # rfecv_df.to_excel(\"Output_data/RFECV_results\"+id+id2+\".xlsx\")\n",
    "    # label_abund_df.to_excel(\"Input_data/Save_files/label_abund_all.xlsx\")\n",
    "    n_scores = len(selector.cv_results_[\"mean_test_score\"])\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Mean test accuracy\")\n",
    "    x = range(1, n_scores + 1)\n",
    "    y = selector.cv_results_[\"mean_test_score\"]\n",
    "    err = selector.cv_results_[\"std_test_score\"]\n",
    "    plt.plot(x, y, 'k-')\n",
    "    plt.fill_between(x, y - err, y + err)\n",
    "    plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "    plt.savefig('Output_data/RFECV' + id + '.png')\n",
    "    plt.close('all')\n",
    "\n",
    "    #Quality control\n",
    "    scorer(df, label_abund, model, id, 10)\n",
    "    scram_score(df, label_abund, model, id, 0.2)\n",
    "    feat_drop(df, label_abund, model, id, 0.2)\n",
    "print('done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StratifiedKFold\n\u001B[0;32m      2\u001B[0m kfold \u001B[38;5;241m=\u001B[39m StratifiedKFold(n_splits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold_idx, (train_index, test_index) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(kfold\u001B[38;5;241m.\u001B[39msplit(df, label_abund)):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(fold_idx)\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:340\u001B[0m, in \u001B[0;36m_BaseKFold.split\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_splits \u001B[38;5;241m>\u001B[39m n_samples:\n\u001B[0;32m    333\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    334\u001B[0m         (\n\u001B[0;32m    335\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot have number of splits n_splits=\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m greater\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    336\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m than the number of samples: n_samples=\u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    337\u001B[0m         )\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_splits, n_samples)\n\u001B[0;32m    338\u001B[0m     )\n\u001B[1;32m--> 340\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msplit(X, y, groups):\n\u001B[0;32m    341\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m train, test\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:86\u001B[0m, in \u001B[0;36mBaseCrossValidator.split\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m     84\u001B[0m X, y, groups \u001B[38;5;241m=\u001B[39m indexable(X, y, groups)\n\u001B[0;32m     85\u001B[0m indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(_num_samples(X))\n\u001B[1;32m---> 86\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m test_index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_test_masks(X, y, groups):\n\u001B[0;32m     87\u001B[0m     train_index \u001B[38;5;241m=\u001B[39m indices[np\u001B[38;5;241m.\u001B[39mlogical_not(test_index)]\n\u001B[0;32m     88\u001B[0m     test_index \u001B[38;5;241m=\u001B[39m indices[test_index]\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:717\u001B[0m, in \u001B[0;36mStratifiedKFold._iter_test_masks\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m    716\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_iter_test_masks\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 717\u001B[0m     test_folds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_test_folds\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    718\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_splits):\n\u001B[0;32m    719\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m test_folds \u001B[38;5;241m==\u001B[39m i\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:660\u001B[0m, in \u001B[0;36mStratifiedKFold._make_test_folds\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    658\u001B[0m allowed_target_types \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    659\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m type_of_target_y \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m allowed_target_types:\n\u001B[1;32m--> 660\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    661\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSupported target types are: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. Got \u001B[39m\u001B[38;5;132;01m{!r}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    662\u001B[0m             allowed_target_types, type_of_target_y\n\u001B[0;32m    663\u001B[0m         )\n\u001B[0;32m    664\u001B[0m     )\n\u001B[0;32m    666\u001B[0m y \u001B[38;5;241m=\u001B[39m column_or_1d(y)\n\u001B[0;32m    668\u001B[0m _, y_idx, y_inv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(y, return_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_inverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mValueError\u001B[0m: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for fold_idx, (train_index, test_index) in enumerate(kfold.split(df, label_abund)):\n",
    "    print(fold_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'adjusted_mutual_info_score',\n 'adjusted_rand_score',\n 'average_precision',\n 'balanced_accuracy',\n 'completeness_score',\n 'explained_variance',\n 'f1',\n 'f1_macro',\n 'f1_micro',\n 'f1_samples',\n 'f1_weighted',\n 'fowlkes_mallows_score',\n 'homogeneity_score',\n 'jaccard',\n 'jaccard_macro',\n 'jaccard_micro',\n 'jaccard_samples',\n 'jaccard_weighted',\n 'matthews_corrcoef',\n 'max_error',\n 'mutual_info_score',\n 'neg_brier_score',\n 'neg_log_loss',\n 'neg_mean_absolute_error',\n 'neg_mean_absolute_percentage_error',\n 'neg_mean_gamma_deviance',\n 'neg_mean_poisson_deviance',\n 'neg_mean_squared_error',\n 'neg_mean_squared_log_error',\n 'neg_median_absolute_error',\n 'neg_root_mean_squared_error',\n 'normalized_mutual_info_score',\n 'precision',\n 'precision_macro',\n 'precision_micro',\n 'precision_samples',\n 'precision_weighted',\n 'r2',\n 'rand_score',\n 'recall',\n 'recall_macro',\n 'recall_micro',\n 'recall_samples',\n 'recall_weighted',\n 'roc_auc',\n 'roc_auc_ovo',\n 'roc_auc_ovo_weighted',\n 'roc_auc_ovr',\n 'roc_auc_ovr_weighted',\n 'top_k_accuracy',\n 'v_measure_score']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer ran successfully\n"
     ]
    }
   ],
   "source": [
    "scorer(df, label_abund, model, id, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
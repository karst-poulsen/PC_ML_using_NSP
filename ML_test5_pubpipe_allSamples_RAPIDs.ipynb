{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from cuml.ensemble import RandomForestRegressor\n",
    "from cuml.feature_extraction.text import CountVectorizer\n",
    "from cuml.feature_selection import RFECV\n",
    "from cuml.preprocessing import LabelEncoder\n",
    "from cuml.metrics.regression import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NormBALFsamples.xlsx', 'Norm_Intensity _all20230403.xlsx']\n",
      "0\n",
      "BALF (6162, 3)\n",
      "1\n",
      "1\n",
      "Bovine (31240, 3)\n",
      "merge (37402, 3)\n"
     ]
    }
   ],
   "source": [
    "#Editable Variables\n",
    "multi_files=True #set to false if you just want to set one  prot_abund_file\n",
    "in_dir=\"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file='Input_data/Proteomic data/abundance/Intensity _all20230202.xlsx'\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file='Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "# take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "files = os.listdir(in_dir)\n",
    "print(files)\n",
    "if multi_files == True:\n",
    "    for i,f in enumerate(files):\n",
    "        print(i)\n",
    "        if i==0:\n",
    "            raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "            # print(raw_MS_data)\n",
    "            raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "            print('BALF',raw_MS_data.shape)\n",
    "        else:\n",
    "            print(i)\n",
    "            temp = pd.read_excel(in_dir+f,header=0)\n",
    "            temp = pd.melt(temp, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "            print('Bovine',temp.shape)\n",
    "            # print(temp)\n",
    "            # print(temp)\n",
    "            raw_MS_data2=pd.concat([raw_MS_data,temp])\n",
    "            print('merge',raw_MS_data2.shape)\n",
    "            # print('did it')\n",
    "else:\n",
    "    raw_MS_data2=pd.read_excel(prot_abund_file,header=0)\n",
    "# raw_MS_data2\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset\n",
    "# raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "25738"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_MS_data2['Abundance']==0).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import cudf\n",
    "import os\n",
    "\n",
    "multi_files=True #set to false if you just want to set one  prot_abund_file\n",
    "in_dir=\"Input_data/Proteomic data/Abundance2/\"\n",
    "prot_abund_file='Input_data/Proteomic data/abundance/Intensity _all20230202.xlsx'\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "controls_file='Input_data/Proteomic data/controls_combined.xlsx'\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "id='allnps_meltb4join'\n",
    "\n",
    "# take files in_dir and combine then into one cuDF df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset before combining\n",
    "files = os.listdir(in_dir)\n",
    "if multi_files == True:\n",
    "    for i,f in enumerate(files):\n",
    "        if i==0:\n",
    "            raw_MS_data=cudf.read_excel(in_dir+f,header=0)\n",
    "            # print(raw_MS_data)\n",
    "            raw_MS_data = cudf.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "        else:\n",
    "\n",
    "            temp = cudf.read_excel(in_dir+f,header=0)\n",
    "            temp = cudf.melt(temp, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "\n",
    "            raw_MS_data=cudf.concat([raw_MS_data,temp])\n",
    "\n",
    "else:\n",
    "    raw_MS_data=cudf.read_excel(prot_abund_file,header=0)\n",
    "\n",
    "#remove prots that were added due to merge\n",
    "raw_MS_data=raw_MS_data.dropna()\n",
    "###Bring in controls (MS data for serums)##\n",
    "controls=cudf.read_excel(controls_file,header=0)\n",
    "MS_data_controls = cudf.merge(raw_MS_data,controls,how='left', on='Entry')\n",
    "###Bring in Uniprot_data,NSPdata and NP data##\n",
    "uniprot_dat=cudf.read_excel(uniprot_filepath,header=0)\n",
    "NSP_data=cudf.read_excel(NSPfilePath)\n",
    "###Bring in NP data and merge to get complete NP dataset###\n",
    "NPUNdata=cudf.read_excel(NP_filepath,header=0,sheet_name='NPUNID')\n",
    "NPprop=cudf.read_excel(NP_filepath,header=0,sheet_name='NP_Props')\n",
    "NPdata=cudf.merge(NPUNdata,NPprop,how=\"left\",on='NPID')\n",
    "NPdata.dropna(inplace=True)\n",
    "#calculate Enrichment\n",
    "#####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "# MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "#keep abundance Controls\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance'])\n",
    "raw_prop_data=cudf.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left',on='Entry')\n",
    "Protein_data_complete = cudf.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']),how='left', on='Entry') #merges netsurfp features and biopython features\n",
    "Protein_data_complete.fillna(0,inplace=True)\n",
    "\n",
    "#to be converted still######\n",
    "raw_prop_data=cudf.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left',on='Entry')\n",
    "Protein_data_complete = cudf.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']),how='left', on='Entry') #merges netsurfp features and biopython features\n",
    "Protein_data_complete.fillna(0,inplace=True)\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n",
    "\n",
    "# merge dataframes and drop unnecessary columns\n",
    "NPdata = cudf.read_csv('NPdata.csv')\n",
    "data_complete = Protein_data_complete.merge(NPdata, on='Sample_num', how='inner').drop(columns=['notes', 'Notes', 'NPUNID'])\n",
    "\n",
    "# replace inf values with -12 and 12\n",
    "data_complete = data_complete.replace([-np.inf], '-12')\n",
    "data_complete = data_complete.replace([np.inf], '12')\n",
    "\n",
    "# create ordinal variables\n",
    "le = LabelEncoder()\n",
    "data_complete['Core Material'] = le.fit_transform(data_complete['Core Material'])\n",
    "data_complete['Surface_Ligand'] = le.fit_transform(data_complete['Surface_Ligand'])\n",
    "\n",
    "# set labels (what we are trying to predict) as Abundance column\n",
    "label_abund = data_complete['Abundance'].values\n",
    "\n",
    "# drop qualitative, unnecessary, and label columns\n",
    "to_drop = [col for col in data_complete.columns if 'total_exposed_' in col]\n",
    "df = data_complete.drop(['Entry', 'Abundance', 'Sequence', 'NPID', 'Ligands', 'Protein Source', 'Sample_num', 'Unnamed: 5', 'Raw_FileID'] + to_drop, axis=1)\n",
    "\n",
    "# save data to files\n",
    "df.to_csv(\"Input_data/Save_files/df_\"+id+\".csv\")\n",
    "cudf.DataFrame(label_abund).to_csv(\"Input_data/Save_files/label_abund\"+id+\".csv\", index=False)\n",
    "\n",
    "data_complete= pd.merge(Protein_data_complete,NPdata,how='inner', on='Sample_num')\n",
    "data_complete.drop(columns=['notes','Notes','NPUNID'],inplace=True)\n",
    "data_complete.fillna(0,inplace=True)\n",
    "data_complete= data_complete.replace([-np.inf],'-12')\n",
    "data_complete=data_complete.replace([np.inf],'12')\n",
    "#create ordinal variables\n",
    "# data_complete2=pd.get_dummies(data_complete, columns=['Core Material','Surface_Ligand'])\n",
    "le=LabelEncoder()\n",
    "data_complete['Core Material']=le.fit_transform(data_complete['Core Material'])\n",
    "data_complete['Surface_Ligand']=le.fit_transform(data_complete['Surface_Ligand'])\n",
    "\n",
    "#set labels (what we are trying to predict) as Enrichment column\n",
    "# labels=data_complete['Enrichment'].copy()\n",
    "label_abund=np.ravel(data_complete['Abundance'].copy())\n",
    "label_abund_df=pd.DataFrame(label_abund)\n",
    "# label_enrich=np.ravel(data_complete['Enrichment'].copy())\n",
    "#make it one dimenisional\n",
    "#drop qualitative, not neccessary, and label columns\n",
    "#create df without bonus NSP columns (remove total_exposed) There are too sets of features total_exposed and exposed_exposed\n",
    "to_drop=data_complete.filter(like='total_exposed_')\n",
    "data_complete.drop(columns=to_drop,inplace=True)\n",
    "df=data_complete.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n",
    "\n",
    "df.to_excel(\"Input_data/Save_files/df_\"+id+\".xlsx\")\n",
    "label_abund_df.to_excel(\"Input_data/Save_files/label_abund\"+id+\".xlsx\",index=False)\n",
    "# label_enrich.to_excel(\"Input_data/Save_files/label_enrich_synth.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2=pd.read_excel(\"Input_data/Save_files/df_synth_RFECV\"+id+\".xlsx\")\n",
    "df_2.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "label_df=pd.read_excel(\"Input_data/Save_files/label_abund_\"+id+\".xlsx\")\n",
    "label_abund=np.ravel(label_df[0])\n",
    "print(label_abund.shape)\n",
    "print(df_2.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Run PCA to see how data differentiates#\n",
    "from sklearn.decomposition import PCA\n",
    "pca= PCA(n_components=2)\n",
    "x_pca=pca.fit_transform(df)\n",
    "\n",
    "plt.scatter(cp.asnumpy(x_pca[:, 0]), cp.asnumpy(x_pca[:, 1]), c=cp.asnumpy(label_abund_gdf), cmap='viridis')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.savefig('Output_data/PCA'+id+'.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Remove correlated features (over r2 threshold level) and output reduced dataframe (df2)# ##Maybe use in future##\n",
    "# corr_matrix = df.corr()\n",
    "# threshold = 0.8\n",
    "# correlated_features = set()\n",
    "#\n",
    "# for i in range(len(corr_matrix.columns)):\n",
    "#     for j in range(i):\n",
    "#         if threshold < abs(corr_matrix.iloc[i, j]) < 1:\n",
    "#             colname = corr_matrix.columns[i]\n",
    "#             correlated_features.add(colname)\n",
    "# correlated_features\n",
    "# df_2=df.drop(columns=correlated_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Abundance_Controls', 'Length', 'Mass', 'frac_aa_A', 'frac_aa_C',\n       'frac_aa_D', 'frac_aa_E', 'frac_aa_F', 'frac_aa_G', 'frac_aa_H',\n       'frac_aa_K', 'frac_aa_M', 'frac_aa_N', 'frac_aa_R', 'frac_aa_S',\n       'frac_aa_V', 'frac_aa_Y', 'molecular_weight', 'instability_index',\n       'flexibility_max', 'secondary_structure_fraction_sheet',\n       'fraction_exposed', 'asa_sum', 'fraction_exposed_exposed_A',\n       'fraction_exposed_exposed_D', 'fraction_exposed_exposed_G',\n       'fraction_exposed_exposed_H', 'fraction_exposed_exposed_K',\n       'fraction_exposed_exposed_L', 'fraction_exposed_exposed_N',\n       'fraction_exposed_exposed_P', 'fraction_exposed_exposed_R',\n       'fraction_exposed_exposed_S', 'fraction_exposed_exposed_T',\n       'fraction_exposed_exposed_W', 'fraction_exposed_exposed_Y',\n       'nsp_secondary_structure_coil', 'nsp_secondary_structure_sheet',\n       'nsp_secondary_structure_helix', 'nsp_disordered',\n       'asa_sum_normalized', 'Zeta Potential', 'Ligand_PVP', 'Ligand_Au',\n       'Surface_Ligand', 'Dtem', 'Dh_core', 'Dh_functionalized',\n       'Incubation Concentration (mg/ml)', 'Corona_Concentration (ug/mg)'],\n      dtype=object)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "step=2\n",
    "feats=50\n",
    "estimator=RandomForestRegressor(n_estimators=100)\n",
    "selector = RFE(estimator, n_features_to_select=feats, step=step)\n",
    "selector = selector.fit(df, label_abund)\n",
    "selector.support_\n",
    "ranking=selector.ranking_\n",
    "feat_list = selector.get_feature_names_out()\n",
    "df_rfe=df[feat_list]\n",
    "feat_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id2='dropped_controlAbundance'\n",
    "step=2\n",
    "min_feats=5\n",
    "cv= KFold(n_splits=10)\n",
    "estimator=RandomForestRegressor(n_estimators=100)\n",
    "selector = RFECV(estimator=estimator, cv=cv, scoring='neg_mean_squared_error', min_features_to_select=min_feats, step=step)\n",
    "selector = selector.fit(df_rfe, label_abund)\n",
    "\n",
    "feat_list2 = selector.get_support(indices=True)\n",
    "selected_features = df_rfe.columns[feat_list2]\n",
    "df_2 = df_rfe[selected_features]\n",
    "df_2.to_csv(\"Input_data/Save_files/df_RFECV\"+id+id2+\".csv\", index=False)\n",
    "\n",
    "rfecv_df = cudf.DataFrame.from_dict(selector.cv_results_)\n",
    "rfecv_df.to_csv(\"Output_data/RFECV_results\"+id+id2+\".csv\", index=False)\n",
    "\n",
    "n_scores = len(selector.cv_results_[\"test_score\"])\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "x=range(1, n_scores + 1)\n",
    "y=cp.abs(selector.cv_results_[\"test_score\"])\n",
    "err=selector.cv_results_[\"std_test_score\"]\n",
    "plt.plot(x,y,'k-')\n",
    "plt.fill_between(x,y-err,y+err)\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.savefig('Output_data/RFECV45'+id+id2+'.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#try using a meta model including a linear model and Random Forest Model#\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "#\n",
    "#\n",
    "#\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_rfe, label_abund, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # Create the base models\n",
    "# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# ridge = Ridge(alpha=0.1, random_state=42)\n",
    "#\n",
    "# rf_model = rf.fit(X_train,y_train)\n",
    "# ridge_model = ridge.fit(X_train,y_train)\n",
    "# # Make predictions with the base models\n",
    "# rf_pred = cross_val_predict(rf_model, X_train, y_train, cv=5)\n",
    "# ridge_pred = cross_val_predict(ridge_model, X_train, y_train, cv=5)\n",
    "#\n",
    "# # Create the input for the meta-model\n",
    "# stacking_input = np.column_stack((rf_pred, ridge_pred))\n",
    "#\n",
    "# # Train the meta-model (ridge regression)\n",
    "# meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "# meta_model.fit(stacking_input, y_train)\n",
    "#\n",
    "# # Make predictions with the meta-model\n",
    "# rf_pred_test = rf_model.predict(X_test)\n",
    "# ridge_pred_test = ridge_model.predict(X_test)\n",
    "# stacking_input_test = np.column_stack((rf_pred_test, ridge_pred_test))\n",
    "# y_pred_test = meta_model.predict(stacking_input_test)\n",
    "#\n",
    "# # Evaluate the performance of the stacked model\n",
    "# mse = mean_squared_error(y_test, y_pred_test)\n",
    "# print(\"MSE: {:.2f}\".format(mse))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#test estimator to see what is the ideal number of estimators\n",
    "# estimators=np.arange(5,500,5)\n",
    "# out_name=np.arange(5,500,5)\n",
    "# # print(out_name)\n",
    "# scores=[]\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df_abund, label_abund,\n",
    "#                                                         test_size=0.2,\n",
    "#                                                         random_state=42)\n",
    "#\n",
    "# for i in range(len(estimators)):\n",
    "#     rfg = RandomForestRegressor(n_estimators=estimators[i])\n",
    "#     rfg.fit(x_train, y_train)\n",
    "#     score=rfg.score(x_test,y_test)\n",
    "#     scores.append(score)\n",
    "# a=pd.DataFrame(list(zip(estimators,scores)), columns=['number of estimators','accuracy'])\n",
    "# a.to_excel(\"Output_data/estimators_score_abund.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#look at loss as a function of feature\n",
    "feats=[]\n",
    "scores=[]\n",
    "df=df_2\n",
    "id2='_RFECV'\n",
    "label=label_abund\n",
    "# frames=['df','df_norm','df_NSP_drop']\n",
    "# for x,i in enumerate(df_listnew):\\\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df,label, test_size = 0.2, random_state=42)\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "importances=rfr.feature_importances_*100\n",
    "scores.append(topscore)\n",
    "feats.append('allfeats')\n",
    "importances=np.insert(importances,[0],0)\n",
    "for j in x_test.columns:\n",
    "    tmp=x_test.copy()\n",
    "    # print(x_test)\n",
    "    np.random.shuffle(tmp[j].values)\n",
    "    scram_score=rfr.score(tmp,y_test)\n",
    "    scores.append(scram_score)\n",
    "    feats.append(j)\n",
    "\n",
    "a=pd.DataFrame(list(zip(feats,scores,importances)),columns=['feat','score','importances'])\n",
    "a.to_excel(\"Output_data/scram_loss_feats\"+id2+id+\".xlsx\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(feats, scores)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score as a function of feature importance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('Output_data/RFECV50_FeatScramLoss'+id+'.png')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Fit and predict after dropping the next least loss feature, drop one feature at a time\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "feats=[]\n",
    "scores=[]\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "a=list(zip(rfr.feature_importances_,rfr.feature_names_in_))\n",
    "a.sort(reverse=True)\n",
    "col_import=pd.DataFrame(a,columns=['importances','names'])\n",
    "sorted_cols=col_import['names']\n",
    "feats.append('topscore')\n",
    "scores.append(topscore)\n",
    "\n",
    "#df_3=df_2.copy()\n",
    "for i in sorted_cols:\n",
    "    df_3=df_2.copy() #remove if you only want to drop each feature instead of dropping one feature at a time\n",
    "    df_3.drop(columns=[i],inplace=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_3,label_abund, test_size = 0.2, random_state=42)\n",
    "    rfr.fit(x_train,y_train)\n",
    "    score=rfr.score(x_test,y_test)\n",
    "    scores.append(score)\n",
    "    feats.append(i)\n",
    "\n",
    "df_out=pd.DataFrame(list(zip(feats,scores)),columns=['dropped feat','score without feat'])\n",
    "df_out.to_excel(\"Output_data/scores_afterdrop\"+id+id2+\".xlsx\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(feats, scores)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score as a function of feature importance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('Output_data/RFECV50_FeatDropLoss'+id+'.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Fit and predict after dropping the next least loss feature, drop one feature at a time cumulative\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "feats=[]\n",
    "scores=[]\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "a=list(zip(rfr.feature_importances_,rfr.feature_names_in_))\n",
    "a.sort(reverse=True)\n",
    "col_import=pd.DataFrame(a,columns=['importances','names'])\n",
    "sorted_cols=col_import['names']\n",
    "feats.append('topscore')\n",
    "scores.append(topscore)\n",
    "\n",
    "df_3=df_2.copy()\n",
    "for i in sorted_cols:\n",
    "    if i == sorted_cols.iloc[-1]:\n",
    "        break\n",
    "    # df_3=df_2.copy() #remove if you only want to drop each feature instead of dropping one feature at a time\n",
    "    df_3.drop(columns=[i],inplace=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_3,label_abund, test_size = 0.2, random_state=42)\n",
    "    rfr.fit(x_train,y_train)\n",
    "    score=rfr.score(x_test,y_test)\n",
    "    scores.append(score)\n",
    "    feats.append(i)\n",
    "\n",
    "df_out=pd.DataFrame(list(zip(feats,scores)),columns=['dropped feat','score without feat'])\n",
    "df_out.to_excel(\"Output_data/scores_afterdrop_cumulative_reverse\"+id+\".xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot predictions with prediction accuracies#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#look at pearson correlation of predictions#\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "predictions=rfr.predict(x_test)\n",
    "r2=r2_score(y_test, predictions)\n",
    "corr,_ = pearsonr(y_test, predictions)\n",
    "\n",
    "print(r2)\n",
    "print(corr)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#look at pearson correlation of predictions#\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "predictions=rfr.predict(x_test)\n",
    "r2=r2_score(y_test, predictions)\n",
    "corr,_ = pearsonr(y_test, predictions)\n",
    "\n",
    "print(r2)\n",
    "print(corr)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.count_nonzero(label_abund)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_abund.size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
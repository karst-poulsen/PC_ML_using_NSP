{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from helper_functions_KP import *\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline\n",
    "Requires two inputs for training:\n",
    "    - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration)\n",
    "    - NetsurfP and Biopython data that has been precalculated\n",
    "    - X characteristics to predict\n",
    "\n",
    "pipeline\n",
    "Take mass spec spreadsheet\n",
    "Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration\n",
    "Merge with Proteome data to get file that has\n",
    "Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence\n",
    "Calculate protein features using biopython\n",
    "Merge with NSP data to get all protein features\n",
    "\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### New Data workup for RFG\n",
    "\n",
    "# Pull together Proteomic data\n",
    "in_dir=\"Input_data/Proteomic data/abundance/\"\n",
    "all_file='Input_data/Proteomic data/abundance/Intensity _all20230202.xlsx'\n",
    "#combine Mass Spec data input into one excel spreadsheet - Entry - Abundance labeled by NP Unique ID\n",
    "#Abundance as a percent\n",
    "#take files in_dir and combine then into one pandas df (raw_MS_data)\n",
    "# files = os.listdir(in_dir)\n",
    "# for i,f in enumerate(files):\n",
    "#     if i==0:\n",
    "#         raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "#     else:\n",
    "#         temp = pd.read_excel(in_dir+f,header=0)\n",
    "#         raw_MS_data=raw_MS_data.merge(temp,how='outer',on='Entry')\n",
    "raw_MS_data=pd.read_excel(all_file,header=0)\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset\n",
    "raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "#remove prots that were added due to merge\n",
    "raw_MS_data=raw_MS_data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# raw_MS_data.shape\n",
    "# print(raw_MS_data.Sample_num.unique())\n",
    "# print(NPdata.Sample_num.unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "raw_MS_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###Bring in controls (MS data for serums)##\n",
    "#Controls is the abundance of the proteins in serum that have been observed via proteomics\n",
    "#look at other sources here for human proteins, but internal sources for BALF and FBS\n",
    "controls=pd.read_excel('Input_data/Proteomic data/controls_combined.xlsx',header=0)\n",
    "MS_data_controls = pd.merge(raw_MS_data,controls,how='left', on='Entry')\n",
    "###Bring in Uniprot_data,NSPdata and NP data##\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "uniprot_dat=pd.read_excel(uniprot_filepath,header=0)\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "NSP_data=pd.read_excel(NSPfilePath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###Bring in NP data and merge to get complete NP dataset###\n",
    "NP_filepath='Input_data/NPs/NP_Database.xlsx'\n",
    "NPUNdata=pd.read_excel(NP_filepath,header=0,sheet_name='NPUNID')\n",
    "NPprop=pd.read_excel(NP_filepath,header=0,sheet_name='NP_Props')\n",
    "NPdata=pd.merge(NPUNdata,NPprop,how=\"left\",on='NPID')\n",
    "NPdata.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#calculate Enrichment\n",
    "#####MAYBE add binning here to to keep negative results and improve capapbilities######\n",
    "MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "raw_prop_data=pd.merge(MS_data, uniprot_dat.drop_duplicates(subset=['Entry']), how='left',on='Entry')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#normalize all columns by dividing by their max value\n",
    "\n",
    "Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']),how='left', on='Entry') #merges netsurfp features and biopython features\n",
    "Protein_data_complete.fillna(0,inplace=True)\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n",
    "\n",
    "data_complete= pd.merge(Protein_data_complete,NPdata,how='inner', on='Sample_num')\n",
    "data_complete.drop(columns=['notes','Notes','NPUNID'],inplace=True)\n",
    "#Optional here to drop all enrichment values that are NA - also have to deal with positive/negative infinity values\n",
    "# data_complete.dropna(subset=['Enrichment'],inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# raw_prop_data.isna().sum().sum()\n",
    "Protein_data_complete.isna().sum().sum()\n",
    "# print(Protein_data_complete.shape)\n",
    "# print(Protein_data_complete.dropna().shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_complete.fillna(0,inplace=True)\n",
    "#replace -infintiy and positive infinity with -12 and 12\n",
    "\n",
    "#####COME BACK AND FIND BETTER SOLUTION!!!##\n",
    "data_complete= data_complete.replace([-np.inf],'-12')\n",
    "data_complete=data_complete.replace([np.inf],'12')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_complete.shape\n",
    "# data_complete.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "data_complete2=pd.get_dummies(data_complete, columns=['Core Material','Surface_Ligand'])\n",
    "# print(data_complete2.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set labels (what we are trying to predict) as Enrichment column\n",
    "labels=data_complete2['Enrichment'].copy()\n",
    "#make it one dimenisional\n",
    "labels=np.ravel(labels)\n",
    "#drop qualitative, not neccessary, and label columns\n",
    "df=data_complete2.drop(['Entry','Sequence','NPID','Enrichment','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_name=[5,10,25,50,100,125,150,200,250,300,350,400,500]\n",
    "esti=[5,10,25,50,100,125,150,200,250,300,350,400,500]\n",
    "test_size=20\n",
    "scores=[]\n",
    "df2=df_norm[feat_list].copy()\n",
    "for i in range(len(out_name)):\n",
    "    scores[i]=rand_forest_reg_fit(df2,labels,out_name[i],0.20,esti[i])\n",
    "\n",
    "a = pd.DataFrame(list(zip(esti, scores.ranking_)), columns=['number of estimators', 'accuracy'])\n",
    "a.to_excel(\"Output_data/Estimator_Score_25 feats.xlsx\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Run recursive feature elimination to determine top features to select\n",
    "#currently selecting 15 although that is arbitrary\n",
    "### COME UP with better more objective way for determining number of features####\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "estimator = RandomForestRegressor(n_estimators=500)\n",
    "selector = RFE(estimator, n_features_to_select= 25, step=5)\n",
    "selector = selector.fit(df,labels)\n",
    "# selector.support_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# df_scaled=std_scaler.fit_transform(df.to_numpy())\n",
    "df_norm=df.copy()\n",
    "for col in df_norm.columns:\n",
    "    if col == 'asa_sum_normalized':\n",
    "        continue\n",
    "    df_norm[col]=df_norm[col]/df_norm[col].max()\n",
    "    print(col)\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "estimator = RandomForestRegressor(n_estimators=500)\n",
    "selector = RFE(estimator, n_features_to_select= 25, step=5)\n",
    "selector = selector.fit(df,labels)\n",
    "selector.support_\n",
    "a=pd.DataFrame(list(zip(df.columns,selector.ranking_)), columns=['feature','selector rankings'])\n",
    "a.to_excel(\"Output_data/FeatSelectionPost_norm.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selector.support_\n",
    "feat_list=selector.get_feature_names_out()\n",
    "df_norm=df_norm[feat_list]\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_norm,labels, test_size = 0.33, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "feat_importances=rfg.feature_importances_\n",
    "a=pd.DataFrame(list(zip(df_norm.columns,feat_importances*100)), columns=['feature','feat_importance'])\n",
    "a.to_excel(\"Output_data/Featimportances_postnorm.xlsx\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_drop_dupsNSP=df_norm.copy()\n",
    "to_drop=df_drop_dupsNSP.filter(like='total_exposed_')\n",
    "df_drop_dupsNSP.drop(columns=to_drop,inplace=True)\n",
    "estimator = RandomForestRegressor(n_estimators=500)\n",
    "selector = RFE(estimator, n_features_to_select= 25, step=5)\n",
    "selector = selector.fit(df_drop_dupsNSP,labels)\n",
    "selector.support_\n",
    "a=pd.DataFrame(list(zip(df_drop_dupsNSP.columns,selector.ranking_)), columns=['feature','selector rankings'])\n",
    "a.to_excel(\"Output_data/FeatSelectiondrop_total_exposed.xlsx\")\n",
    "feat_list=selector.get_feature_names_out()\n",
    "df_drop_dupsNSP=df_drop_dupsNSP[feat_list]\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_drop_dupsNSP,labels, test_size = 0.33, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "feat_importances=rfg.feature_importances_\n",
    "a=pd.DataFrame(list(zip(df_drop_dupsNSP.columns,feat_importances*100)), columns=['feature','feat_importance'])\n",
    "a.to_excel(\"Output_data/Featimportances_dropNSPdupes.xlsx\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_list=selector.get_feature_names_out()\n",
    "df_rfe=df[feat_list].copy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_rfe,labels, test_size = 0.33, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "feat_importances=rfg.feature_importances_\n",
    "a=pd.DataFrame(list(zip(df_rfe.columns,feat_importances*100)), columns=['feature','feat_importance'])\n",
    "a.to_excel(\"Output_data/Featimportances_postnorm.xlsx\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_list=selector.get_feature_names_out()\n",
    "# df_rfe.drop(['Raw_FileID'],inplace=True,axis=1)\n",
    "df_rfe\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_rfe,labels, test_size = 0.33, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "feat_importances=rfg.feature_importances_\n",
    "for i,col in enumerate(df_rfe.columns):\n",
    "    print(col,feat_importances[i]*100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(df_rfe.columns,feat_importances*100)), columns=['feature','feat_importance'])\n",
    "a.to_excel(\"Output_data/Featimportances.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#predict on every feature individually\n",
    "score_list=[]\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "for i, col in enumerate(df.columns):\n",
    "    tmp=df[col].to_numpy()\n",
    "    tmp=tmp.reshape(-1,1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(tmp,labels, test_size = 0.20)\n",
    "    rfg.fit(x_train,y_train)\n",
    "    true_score = rfg.score(x_test,y_test)\n",
    "    score_list.append(round(true_score,4))\n",
    "    # print(i)\n",
    "print(i,'columns predicted/tested')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scoremax=max(score_list)\n",
    "collist=list(df.columns)\n",
    "maxcol=collist[(score_list.index(scoremax))]\n",
    "print('max score',scoremax,maxcol)\n",
    "impcols=[]\n",
    "impscore=[]\n",
    "feats=[]\n",
    "score=[]\n",
    "for i,col in enumerate(df.columns):\n",
    "    feats.append(col)\n",
    "    score.append(score_list[i])\n",
    "    if score_list[i]> 0.45:\n",
    "        impcols.append(col)\n",
    "        impscore.append(score_list[i])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(feats,score)), columns=['feature','score'])\n",
    "a.to_excel(\"Output_data/Individual_feat_importance.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_top=df[feat_list].copy()\n",
    "df_top.drop(['Raw_FileID'],inplace=True,axis=1)\n",
    "print(df.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_top,labels, test_size = 0.10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test top 50 features as selected by Recursive Feature Elimination\n",
    "# determine how important each feature is by scrambling that specific feature and seeing how the prediction accuracy decreases as a result\n",
    "rfg=RandomForestRegressor(n_estimators=500)\n",
    "rfg.fit(x_train,y_train)\n",
    "true_score = rfg.score(x_test,y_test)\n",
    "\n",
    "feats=[]\n",
    "score=[]\n",
    "feats.append('score with no scrambling')\n",
    "score.append(true_score)\n",
    "for i, col in enumerate(x_test.columns):\n",
    "    tmp=x_test.copy()\n",
    "    tmp[col]=np.random.permutation(x_test[col].values)\n",
    "    scram_score=(true_score-rfg.score(tmp,y_test))/true_score*100\n",
    "    feats.append(col)\n",
    "    score.append(round(scram_score,4))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(feats,score)), columns=['feature','score'])\n",
    "a.to_excel(\"Output_data/Loss_duetoscramble.xlsx\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_rfe,labels, test_size = 0.33, random_state=42)\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=2016)\n",
    "# for train_index, test_index in sss.split(df, labels):\n",
    "# x_train = df.iloc[train_index]\n",
    "# X_test = df.iloc[test_index]\n",
    "# y_train = labels.iloc[train_index]\n",
    "# y_test = labels.iloc[test_index]\n",
    "\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "# print(rfg.feature_importances_)\n",
    "\n",
    "# metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfg.predict_proba(x_test)[:, 1]),\n",
    "#             'Accuracy':rfg.score(x_test, y_test), 'Recall':recall_score(y_test, rfg.predict(x_test)),\n",
    "#             'Precision':precision_score(y_test, rfg.predict(x_test), zero_division=0), 'F1':f1_score(y_test, rfg.predict(x_test))}\n",
    "# metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_importances=rfg.feature_importances_\n",
    "# print(feat_importances)\n",
    "print('feature: importance score')\n",
    "for i,col in enumerate(df_rfe.columns):\n",
    "    print(col,feat_importances[i]*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_list=selector.get_feature_names_out()\n",
    "df_rfe=df[feat_list].copy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_rfe,labels, test_size = 0.33, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "feat_importances=rfg.feature_importances_\n",
    "a=pd.DataFrame(list(zip(df_rfe.columns,feat_importances*100)), columns=['feature','feat_importance'])\n",
    "a.to_excel(\"Output_data/Featimportances_postnorm.xlsx\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_list=selector.get_feature_names_out()\n",
    "# df_rfe.drop(['Raw_FileID'],inplace=True,axis=1)\n",
    "df_rfe\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_rfe,labels, test_size = 0.33, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "feat_importances=rfg.feature_importances_\n",
    "for i,col in enumerate(df_rfe.columns):\n",
    "    print(col,feat_importances[i]*100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(df_rfe.columns,feat_importances*100)), columns=['feature','feat_importance'])\n",
    "a.to_excel(\"Output_data/Featimportances.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#predict on every feature individually\n",
    "score_list=[]\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "for i, col in enumerate(df.columns):\n",
    "    tmp=df[col].to_numpy()\n",
    "    tmp=tmp.reshape(-1,1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(tmp,labels, test_size = 0.20)\n",
    "    rfg.fit(x_train,y_train)\n",
    "    true_score = rfg.score(x_test,y_test)\n",
    "    score_list.append(round(true_score,4))\n",
    "    # print(i)\n",
    "print(i,'columns predicted/tested')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scoremax=max(score_list)\n",
    "collist=list(df.columns)\n",
    "maxcol=collist[(score_list.index(scoremax))]\n",
    "print('max score',scoremax,maxcol)\n",
    "impcols=[]\n",
    "impscore=[]\n",
    "feats=[]\n",
    "score=[]\n",
    "for i,col in enumerate(df.columns):\n",
    "    feats.append(col)\n",
    "    score.append(score_list[i])\n",
    "    if score_list[i]> 0.45:\n",
    "        impcols.append(col)\n",
    "        impscore.append(score_list[i])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(feats,score)), columns=['feature','score'])\n",
    "a.to_excel(\"Output_data/Individual_feat_importance.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_top=df[feat_list].copy()\n",
    "df_top.drop(['Raw_FileID'],inplace=True,axis=1)\n",
    "print(df.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_top,labels, test_size = 0.10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test top 50 features as selected by Recursive Feature Elimination\n",
    "# determine how important each feature is by scrambling that specific feature and seeing how the prediction accuracy decreases as a result\n",
    "rfg=RandomForestRegressor(n_estimators=500)\n",
    "rfg.fit(x_train,y_train)\n",
    "true_score = rfg.score(x_test,y_test)\n",
    "\n",
    "feats=[]\n",
    "score=[]\n",
    "feats.append('score with no scrambling')\n",
    "score.append(true_score)\n",
    "for i, col in enumerate(x_test.columns):\n",
    "    tmp=x_test.copy()\n",
    "    tmp[col]=np.random.permutation(x_test[col].values)\n",
    "    scram_score=(true_score-rfg.score(tmp,y_test))/true_score*100\n",
    "    feats.append(col)\n",
    "    score.append(round(scram_score,4))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(feats,score)), columns=['feature','score'])\n",
    "a.to_excel(\"Output_data/Loss_duetoscramble.xlsx\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_rfe,labels, test_size = 0.33, random_state=42)\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=2016)\n",
    "# for train_index, test_index in sss.split(df, labels):\n",
    "# x_train = df.iloc[train_index]\n",
    "# X_test = df.iloc[test_index]\n",
    "# y_train = labels.iloc[train_index]\n",
    "# y_test = labels.iloc[test_index]\n",
    "\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "# print(rfg.feature_importances_)\n",
    "\n",
    "# metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfg.predict_proba(x_test)[:, 1]),\n",
    "#             'Accuracy':rfg.score(x_test, y_test), 'Recall':recall_score(y_test, rfg.predict(x_test)),\n",
    "#             'Precision':precision_score(y_test, rfg.predict(x_test), zero_division=0), 'F1':f1_score(y_test, rfg.predict(x_test))}\n",
    "# metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_importances=rfg.feature_importances_\n",
    "# print(feat_importances)\n",
    "print('feature: importance score')\n",
    "for i,col in enumerate(df_rfe.columns):\n",
    "    print(col,feat_importances[i]*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6895447495328173\n",
      "frac_aa_A 2.974639083064486\n",
      "frac_aa_C 2.7178849656239232\n",
      "frac_aa_I 2.3205492592220955\n",
      "frac_aa_R 2.808479549993177\n",
      "flexibility_min 3.5805655005479524\n",
      "isoelectric_point 4.654393836586511\n",
      "gravy 2.5066496410522188\n",
      "fraction_exposed_nonpolar_exposed 2.45279599254974\n",
      "fraction_total_exposed_E 3.411109973051929\n",
      "fraction_total_exposed_K 4.295110086399459\n",
      "fraction_total_exposed_M 2.5947221725408762\n",
      "fraction_total_exposed_R 5.770905774293792\n",
      "fraction_total_exposed_Y 5.878245763326322\n",
      "fraction_exposed_exposed_K 2.5940590643218626\n",
      "fraction_exposed_exposed_L 5.081983975975696\n",
      "fraction_exposed_exposed_M 2.4302488677261342\n",
      "fraction_exposed_exposed_R 4.251238711417701\n",
      "fraction_exposed_exposed_S 3.6728440357155203\n",
      "nsp_secondary_structure_coil 3.271074662651876\n",
      "nsp_secondary_structure_helix 3.511825970017547\n",
      "asa_sum_normalized 2.9401350995789484\n",
      "Zeta Potential 14.517614793655941\n",
      "Dh_functionalized 7.225007620472766\n",
      "Incubation Concentration (mg/ml) 4.537915600213534\n"
     ]
    }
   ],
   "source": [
    "feat_list=selector.get_feature_names_out()\n",
    "# df_rfe.drop(['Raw_FileID'],inplace=True,axis=1)\n",
    "df_rfe\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_rfe,labels, test_size = 0.33, random_state=42)\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "feat_importances=rfg.feature_importances_\n",
    "for i,col in enumerate(df_rfe.columns):\n",
    "    print(col,feat_importances[i]*100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(df_rfe.columns,feat_importances*100)), columns=['feature','feat_importance'])\n",
    "a.to_excel(\"Output_data/Featimportances.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 columns predicted/tested\n"
     ]
    }
   ],
   "source": [
    "#predict on every feature individually\n",
    "score_list=[]\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "for i, col in enumerate(df.columns):\n",
    "    tmp=df[col].to_numpy()\n",
    "    tmp=tmp.reshape(-1,1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(tmp,labels, test_size = 0.20)\n",
    "    rfg.fit(x_train,y_train)\n",
    "    true_score = rfg.score(x_test,y_test)\n",
    "    score_list.append(round(true_score,4))\n",
    "    # print(i)\n",
    "print(i,'columns predicted/tested')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max score 0.5055 fraction_exposed_exposed_L\n"
     ]
    }
   ],
   "source": [
    "scoremax=max(score_list)\n",
    "collist=list(df.columns)\n",
    "maxcol=collist[(score_list.index(scoremax))]\n",
    "print('max score',scoremax,maxcol)\n",
    "impcols=[]\n",
    "impscore=[]\n",
    "feats=[]\n",
    "score=[]\n",
    "for i,col in enumerate(df.columns):\n",
    "    feats.append(col)\n",
    "    score.append(score_list[i])\n",
    "    if score_list[i]> 0.45:\n",
    "        impcols.append(col)\n",
    "        impscore.append(score_list[i])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(feats,score)), columns=['feature','score'])\n",
    "a.to_excel(\"Output_data/Individual_feat_importance.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5698, 121)\n"
     ]
    }
   ],
   "source": [
    "df_top=df[feat_list].copy()\n",
    "df_top.drop(['Raw_FileID'],inplace=True,axis=1)\n",
    "print(df.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_top,labels, test_size = 0.10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "# test top 50 features as selected by Recursive Feature Elimination\n",
    "# determine how important each feature is by scrambling that specific feature and seeing how the prediction accuracy decreases as a result\n",
    "rfg=RandomForestRegressor(n_estimators=500)\n",
    "rfg.fit(x_train,y_train)\n",
    "true_score = rfg.score(x_test,y_test)\n",
    "\n",
    "feats=[]\n",
    "score=[]\n",
    "feats.append('score with no scrambling')\n",
    "score.append(true_score)\n",
    "for i, col in enumerate(x_test.columns):\n",
    "    tmp=x_test.copy()\n",
    "    tmp[col]=np.random.permutation(x_test[col].values)\n",
    "    scram_score=(true_score-rfg.score(tmp,y_test))/true_score*100\n",
    "    feats.append(col)\n",
    "    score.append(round(scram_score,4))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "a=pd.DataFrame(list(zip(feats,score)), columns=['feature','score'])\n",
    "a.to_excel(\"Output_data/Loss_duetoscramble.xlsx\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_rfe,labels, test_size = 0.33, random_state=42)\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=2016)\n",
    "# for train_index, test_index in sss.split(df, labels):\n",
    "# x_train = df.iloc[train_index]\n",
    "# X_test = df.iloc[test_index]\n",
    "# y_train = labels.iloc[train_index]\n",
    "# y_test = labels.iloc[test_index]\n",
    "\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "# print(rfg.feature_importances_)\n",
    "\n",
    "# metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfg.predict_proba(x_test)[:, 1]),\n",
    "#             'Accuracy':rfg.score(x_test, y_test), 'Recall':recall_score(y_test, rfg.predict(x_test)),\n",
    "#             'Precision':precision_score(y_test, rfg.predict(x_test), zero_division=0), 'F1':f1_score(y_test, rfg.predict(x_test))}\n",
    "# metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_importances=rfg.feature_importances_\n",
    "# print(feat_importances)\n",
    "print('feature: importance score')\n",
    "for i,col in enumerate(df_rfe.columns):\n",
    "    print(col,feat_importances[i]*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-2.5320414762967736, -2.100316148783357, -2.1346770999965723, ...,\n       1.789022535723233, '-12', '-12'], dtype=object)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "      Length     Mass  frac_aa_A  frac_aa_C  frac_aa_D  frac_aa_E  frac_aa_F  \\\n0      607.0  69293.0   0.079077   0.057661   0.065898   0.097199   0.049423   \n1      607.0  69293.0   0.079077   0.057661   0.065898   0.097199   0.049423   \n2      607.0  69293.0   0.079077   0.057661   0.065898   0.097199   0.049423   \n3      607.0  69293.0   0.079077   0.057661   0.065898   0.097199   0.049423   \n4      607.0  69293.0   0.079077   0.057661   0.065898   0.097199   0.049423   \n...      ...      ...        ...        ...        ...        ...        ...   \n3077   412.0  45823.0   0.070388   0.007282   0.050971   0.072816   0.060680   \n3078   412.0  45823.0   0.070388   0.007282   0.050971   0.072816   0.060680   \n3079   412.0  45823.0   0.070388   0.007282   0.050971   0.072816   0.060680   \n3080   412.0  45823.0   0.070388   0.007282   0.050971   0.072816   0.060680   \n3081   412.0  45823.0   0.070388   0.007282   0.050971   0.072816   0.060680   \n\n      frac_aa_G  frac_aa_H  frac_aa_I  ...  Incubation Time (minutes)  \\\n0      0.028007   0.028007   0.024712  ...                       30.0   \n1      0.028007   0.028007   0.024712  ...                       30.0   \n2      0.028007   0.028007   0.024712  ...                       30.0   \n3      0.028007   0.028007   0.024712  ...                       30.0   \n4      0.028007   0.028007   0.024712  ...                       30.0   \n...         ...        ...        ...  ...                        ...   \n3077   0.048544   0.036408   0.048544  ...                       60.0   \n3078   0.048544   0.036408   0.048544  ...                       60.0   \n3079   0.048544   0.036408   0.048544  ...                       60.0   \n3080   0.048544   0.036408   0.048544  ...                       60.0   \n3081   0.048544   0.036408   0.048544  ...                       60.0   \n\n      Temperature  Core Material_E171  Core Material_Iron Oxide  \\\n0            25.0                   0                         1   \n1            25.0                   0                         1   \n2            25.0                   0                         1   \n3            25.0                   0                         0   \n4            25.0                   0                         0   \n...           ...                 ...                       ...   \n3077         25.0                   0                         0   \n3078         25.0                   0                         0   \n3079         25.0                   0                         0   \n3080         25.0                   0                         0   \n3081         25.0                   0                         1   \n\n      Core Material_P25  Core Material_Polystyrene  Core Material_R101  \\\n0                     0                          0                   0   \n1                     0                          0                   0   \n2                     0                          0                   0   \n3                     0                          1                   0   \n4                     0                          1                   0   \n...                 ...                        ...                 ...   \n3077                  0                          0                   1   \n3078                  1                          0                   0   \n3079                  1                          0                   0   \n3080                  1                          0                   0   \n3081                  0                          0                   0   \n\n      Ligand_Amine  Ligand_Carboxylate BSA  Ligand_none  \n0                0                       1            0  \n1                0                       1            0  \n2                0                       1            0  \n3                1                       0            0  \n4                1                       0            0  \n...            ...                     ...          ...  \n3077             0                       0            1  \n3078             0                       0            1  \n3079             0                       0            1  \n3080             0                       0            1  \n3081             0                       1            0  \n\n[1992 rows x 115 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Length</th>\n      <th>Mass</th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_H</th>\n      <th>frac_aa_I</th>\n      <th>...</th>\n      <th>Incubation Time (minutes)</th>\n      <th>Temperature</th>\n      <th>Core Material_E171</th>\n      <th>Core Material_Iron Oxide</th>\n      <th>Core Material_P25</th>\n      <th>Core Material_Polystyrene</th>\n      <th>Core Material_R101</th>\n      <th>Ligand_Amine</th>\n      <th>Ligand_Carboxylate BSA</th>\n      <th>Ligand_none</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>607.0</td>\n      <td>69293.0</td>\n      <td>0.079077</td>\n      <td>0.057661</td>\n      <td>0.065898</td>\n      <td>0.097199</td>\n      <td>0.049423</td>\n      <td>0.028007</td>\n      <td>0.028007</td>\n      <td>0.024712</td>\n      <td>...</td>\n      <td>30.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>607.0</td>\n      <td>69293.0</td>\n      <td>0.079077</td>\n      <td>0.057661</td>\n      <td>0.065898</td>\n      <td>0.097199</td>\n      <td>0.049423</td>\n      <td>0.028007</td>\n      <td>0.028007</td>\n      <td>0.024712</td>\n      <td>...</td>\n      <td>30.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>607.0</td>\n      <td>69293.0</td>\n      <td>0.079077</td>\n      <td>0.057661</td>\n      <td>0.065898</td>\n      <td>0.097199</td>\n      <td>0.049423</td>\n      <td>0.028007</td>\n      <td>0.028007</td>\n      <td>0.024712</td>\n      <td>...</td>\n      <td>30.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>607.0</td>\n      <td>69293.0</td>\n      <td>0.079077</td>\n      <td>0.057661</td>\n      <td>0.065898</td>\n      <td>0.097199</td>\n      <td>0.049423</td>\n      <td>0.028007</td>\n      <td>0.028007</td>\n      <td>0.024712</td>\n      <td>...</td>\n      <td>30.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>607.0</td>\n      <td>69293.0</td>\n      <td>0.079077</td>\n      <td>0.057661</td>\n      <td>0.065898</td>\n      <td>0.097199</td>\n      <td>0.049423</td>\n      <td>0.028007</td>\n      <td>0.028007</td>\n      <td>0.024712</td>\n      <td>...</td>\n      <td>30.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3077</th>\n      <td>412.0</td>\n      <td>45823.0</td>\n      <td>0.070388</td>\n      <td>0.007282</td>\n      <td>0.050971</td>\n      <td>0.072816</td>\n      <td>0.060680</td>\n      <td>0.048544</td>\n      <td>0.036408</td>\n      <td>0.048544</td>\n      <td>...</td>\n      <td>60.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3078</th>\n      <td>412.0</td>\n      <td>45823.0</td>\n      <td>0.070388</td>\n      <td>0.007282</td>\n      <td>0.050971</td>\n      <td>0.072816</td>\n      <td>0.060680</td>\n      <td>0.048544</td>\n      <td>0.036408</td>\n      <td>0.048544</td>\n      <td>...</td>\n      <td>60.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3079</th>\n      <td>412.0</td>\n      <td>45823.0</td>\n      <td>0.070388</td>\n      <td>0.007282</td>\n      <td>0.050971</td>\n      <td>0.072816</td>\n      <td>0.060680</td>\n      <td>0.048544</td>\n      <td>0.036408</td>\n      <td>0.048544</td>\n      <td>...</td>\n      <td>60.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3080</th>\n      <td>412.0</td>\n      <td>45823.0</td>\n      <td>0.070388</td>\n      <td>0.007282</td>\n      <td>0.050971</td>\n      <td>0.072816</td>\n      <td>0.060680</td>\n      <td>0.048544</td>\n      <td>0.036408</td>\n      <td>0.048544</td>\n      <td>...</td>\n      <td>60.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3081</th>\n      <td>412.0</td>\n      <td>45823.0</td>\n      <td>0.070388</td>\n      <td>0.007282</td>\n      <td>0.050971</td>\n      <td>0.072816</td>\n      <td>0.060680</td>\n      <td>0.048544</td>\n      <td>0.036408</td>\n      <td>0.048544</td>\n      <td>...</td>\n      <td>60.0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1992 rows Ã— 115 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [53]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m estimator \u001B[38;5;241m=\u001B[39m RandomForestRegressor(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[0;32m      7\u001B[0m selector \u001B[38;5;241m=\u001B[39m RFE(estimator, n_features_to_select\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m15\u001B[39m, step\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m selector \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m selector\u001B[38;5;241m.\u001B[39msupport_\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:235\u001B[0m, in \u001B[0;36mRFE.fit\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001B[39;00m\n\u001B[0;32m    217\u001B[0m \n\u001B[0;32m    218\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:296\u001B[0m, in \u001B[0;36mRFE._fit\u001B[1;34m(self, X, y, step_score, **fit_params)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting estimator with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m features.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m np\u001B[38;5;241m.\u001B[39msum(support_))\n\u001B[1;32m--> 296\u001B[0m estimator\u001B[38;5;241m.\u001B[39mfit(X[:, features], y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    298\u001B[0m \u001B[38;5;66;03m# Get importance and rank them\u001B[39;00m\n\u001B[0;32m    299\u001B[0m importances \u001B[38;5;241m=\u001B[39m _get_feature_importances(\n\u001B[0;32m    300\u001B[0m     estimator,\n\u001B[0;32m    301\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimportance_getter,\n\u001B[0;32m    302\u001B[0m     transform_func\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msquare\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    303\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    465\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    466\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    468\u001B[0m ]\n\u001B[0;32m    470\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    475\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 476\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    486\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1046\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1047\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1050\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 861\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    778\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 779\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 117\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    187\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 189\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    191\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1342\u001B[0m, in \u001B[0;36mDecisionTreeRegressor.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m   1313\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1314\u001B[0m     \u001B[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[0;32m   1315\u001B[0m \n\u001B[0;32m   1316\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1339\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m   1340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1342\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1344\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1345\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1346\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1347\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1348\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001B[0m, in \u001B[0;36mBaseDecisionTree.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    447\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    448\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    449\u001B[0m         splitter,\n\u001B[0;32m    450\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    455\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    456\u001B[0m     )\n\u001B[1;32m--> 458\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    461\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "data_complete"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "array([False,  True, False,  True, False, False, False, False, False,\n       False, False, False, False, False, False,  True, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False,  True, False,  True, False, False,\n       False, False, False, False, False,  True, False, False, False,\n       False, False,  True,  True, False, False,  True, False, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False, False, False, False, False,  True, False,\n       False, False, False, False, False, False, False,  True,  True,\n       False, False,  True,  True, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run recursive feature elimination to determine top features to select\n",
    "#currently selecting 15 although that is arbitrary\n",
    "### COME UP with better more objective way for determining number of features####\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "estimator = RandomForestRegressor(n_estimators=100)\n",
    "selector = RFE(estimator, n_features_to_select= 15, step=5)\n",
    "selector = selector.fit(df,labels)\n",
    "selector.support_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mass' 'frac_aa_I' 'frac_aa_Q' 'frac_aa_W' 'flexibility_std'\n",
      " 'flexibility_min' 'flexibility_median' 'rsa_std'\n",
      " 'fraction_total_exposed_A' 'fraction_total_exposed_Y'\n",
      " 'fraction_exposed_exposed_H' 'fraction_exposed_exposed_L'\n",
      " 'fraction_exposed_exposed_M' 'Sample_num' 'Raw_FileID']\n"
     ]
    }
   ],
   "source": [
    "selector.ranking_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mass' 'frac_aa_C' 'frac_aa_Q' 'fraction_exposed_nonpolar_exposed'\n",
      " 'fraction_exposed_polar_exposed' 'fraction_total_exposed_E'\n",
      " 'fraction_total_exposed_L' 'fraction_total_exposed_M'\n",
      " 'fraction_total_exposed_Q' 'fraction_exposed_exposed_A'\n",
      " 'fraction_exposed_exposed_P' 'nsp_secondary_structure_sheet'\n",
      " 'nsp_secondary_structure_helix' 'Raw_FileID' 'Zeta Potential']\n"
     ]
    }
   ],
   "source": [
    "feat_list=selector.get_feature_names_out()\n",
    "print(feat_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "          Mass  frac_aa_C  frac_aa_Q  fraction_exposed_nonpolar_exposed  \\\n0      69293.0   0.057661   0.032949                           0.257396   \n1      38419.0   0.038997   0.047354                           0.510490   \n2      46104.0   0.014423   0.031250                           0.288793   \n3      52347.0   0.019355   0.034409                           0.308824   \n4     140374.0   0.066343   0.045307                           0.327566   \n...        ...        ...        ...                                ...   \n5693       0.0   0.000000   0.000000                           0.410853   \n5694   60682.0   0.001873   0.037453                           0.368098   \n5695   51907.0   0.032854   0.032854                           0.424920   \n5696   20746.0   0.005525   0.044199                           0.281250   \n5697   36724.0   0.014970   0.032934                           0.341317   \n\n      fraction_exposed_polar_exposed  fraction_total_exposed_E  \\\n0                           0.742604                  0.088962   \n1                           0.489510                  0.047354   \n2                           0.711207                  0.060096   \n3                           0.691176                  0.068817   \n4                           0.672434                  0.080097   \n...                              ...                       ...   \n5693                        0.589147                  0.075221   \n5694                        0.631902                  0.063670   \n5695                        0.575080                  0.057495   \n5696                        0.718750                  0.066298   \n5697                        0.658683                  0.059880   \n\n      fraction_total_exposed_L  fraction_total_exposed_M  \\\n0                     0.021417                  0.003295   \n1                     0.061281                  0.002786   \n2                     0.031250                  0.009615   \n3                     0.030108                  0.004301   \n4                     0.025890                  0.004854   \n...                        ...                       ...   \n5693                  0.030973                  0.013274   \n5694                  0.018727                  0.013109   \n5695                  0.034908                  0.004107   \n5696                  0.016575                  0.011050   \n5697                  0.020958                  0.017964   \n\n      fraction_total_exposed_Q  fraction_exposed_exposed_A  \\\n0                     0.026359                    0.082840   \n1                     0.044568                    0.087413   \n2                     0.031250                    0.060345   \n3                     0.027957                    0.040441   \n4                     0.045307                    0.034602   \n...                        ...                         ...   \n5693                  0.022124                    0.069767   \n5694                  0.033708                    0.061350   \n5695                  0.030801                    0.083067   \n5696                  0.038674                    0.031250   \n5697                  0.026946                    0.047904   \n\n      fraction_exposed_exposed_P  nsp_secondary_structure_sheet  \\\n0                       0.062130                          0.005   \n1                       0.118881                          0.256   \n2                       0.043103                          0.286   \n3                       0.051471                          0.267   \n4                       0.062284                          0.391   \n...                          ...                            ...   \n5693                    0.077519                          0.177   \n5694                    0.085890                          0.090   \n5695                    0.105431                          0.257   \n5696                    0.031250                          0.204   \n5697                    0.041916                          0.198   \n\n      nsp_secondary_structure_helix  Raw_FileID  Zeta Potential  \n0                             0.710       65136           -38.0  \n1                             0.120       65136           -38.0  \n2                             0.257       65136           -38.0  \n3                             0.241       65136           -38.0  \n4                             0.005       65136           -38.0  \n...                             ...         ...             ...  \n5693                          0.279       65138           -38.0  \n5694                          0.442       65138           -38.0  \n5695                          0.140       65138           -38.0  \n5696                          0.337       65138           -38.0  \n5697                          0.425       65138           -38.0  \n\n[5698 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mass</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_Q</th>\n      <th>fraction_exposed_nonpolar_exposed</th>\n      <th>fraction_exposed_polar_exposed</th>\n      <th>fraction_total_exposed_E</th>\n      <th>fraction_total_exposed_L</th>\n      <th>fraction_total_exposed_M</th>\n      <th>fraction_total_exposed_Q</th>\n      <th>fraction_exposed_exposed_A</th>\n      <th>fraction_exposed_exposed_P</th>\n      <th>nsp_secondary_structure_sheet</th>\n      <th>nsp_secondary_structure_helix</th>\n      <th>Raw_FileID</th>\n      <th>Zeta Potential</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69293.0</td>\n      <td>0.057661</td>\n      <td>0.032949</td>\n      <td>0.257396</td>\n      <td>0.742604</td>\n      <td>0.088962</td>\n      <td>0.021417</td>\n      <td>0.003295</td>\n      <td>0.026359</td>\n      <td>0.082840</td>\n      <td>0.062130</td>\n      <td>0.005</td>\n      <td>0.710</td>\n      <td>65136</td>\n      <td>-38.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38419.0</td>\n      <td>0.038997</td>\n      <td>0.047354</td>\n      <td>0.510490</td>\n      <td>0.489510</td>\n      <td>0.047354</td>\n      <td>0.061281</td>\n      <td>0.002786</td>\n      <td>0.044568</td>\n      <td>0.087413</td>\n      <td>0.118881</td>\n      <td>0.256</td>\n      <td>0.120</td>\n      <td>65136</td>\n      <td>-38.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>46104.0</td>\n      <td>0.014423</td>\n      <td>0.031250</td>\n      <td>0.288793</td>\n      <td>0.711207</td>\n      <td>0.060096</td>\n      <td>0.031250</td>\n      <td>0.009615</td>\n      <td>0.031250</td>\n      <td>0.060345</td>\n      <td>0.043103</td>\n      <td>0.286</td>\n      <td>0.257</td>\n      <td>65136</td>\n      <td>-38.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>52347.0</td>\n      <td>0.019355</td>\n      <td>0.034409</td>\n      <td>0.308824</td>\n      <td>0.691176</td>\n      <td>0.068817</td>\n      <td>0.030108</td>\n      <td>0.004301</td>\n      <td>0.027957</td>\n      <td>0.040441</td>\n      <td>0.051471</td>\n      <td>0.267</td>\n      <td>0.241</td>\n      <td>65136</td>\n      <td>-38.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>140374.0</td>\n      <td>0.066343</td>\n      <td>0.045307</td>\n      <td>0.327566</td>\n      <td>0.672434</td>\n      <td>0.080097</td>\n      <td>0.025890</td>\n      <td>0.004854</td>\n      <td>0.045307</td>\n      <td>0.034602</td>\n      <td>0.062284</td>\n      <td>0.391</td>\n      <td>0.005</td>\n      <td>65136</td>\n      <td>-38.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5693</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.410853</td>\n      <td>0.589147</td>\n      <td>0.075221</td>\n      <td>0.030973</td>\n      <td>0.013274</td>\n      <td>0.022124</td>\n      <td>0.069767</td>\n      <td>0.077519</td>\n      <td>0.177</td>\n      <td>0.279</td>\n      <td>65138</td>\n      <td>-38.0</td>\n    </tr>\n    <tr>\n      <th>5694</th>\n      <td>60682.0</td>\n      <td>0.001873</td>\n      <td>0.037453</td>\n      <td>0.368098</td>\n      <td>0.631902</td>\n      <td>0.063670</td>\n      <td>0.018727</td>\n      <td>0.013109</td>\n      <td>0.033708</td>\n      <td>0.061350</td>\n      <td>0.085890</td>\n      <td>0.090</td>\n      <td>0.442</td>\n      <td>65138</td>\n      <td>-38.0</td>\n    </tr>\n    <tr>\n      <th>5695</th>\n      <td>51907.0</td>\n      <td>0.032854</td>\n      <td>0.032854</td>\n      <td>0.424920</td>\n      <td>0.575080</td>\n      <td>0.057495</td>\n      <td>0.034908</td>\n      <td>0.004107</td>\n      <td>0.030801</td>\n      <td>0.083067</td>\n      <td>0.105431</td>\n      <td>0.257</td>\n      <td>0.140</td>\n      <td>65138</td>\n      <td>-38.0</td>\n    </tr>\n    <tr>\n      <th>5696</th>\n      <td>20746.0</td>\n      <td>0.005525</td>\n      <td>0.044199</td>\n      <td>0.281250</td>\n      <td>0.718750</td>\n      <td>0.066298</td>\n      <td>0.016575</td>\n      <td>0.011050</td>\n      <td>0.038674</td>\n      <td>0.031250</td>\n      <td>0.031250</td>\n      <td>0.204</td>\n      <td>0.337</td>\n      <td>65138</td>\n      <td>-38.0</td>\n    </tr>\n    <tr>\n      <th>5697</th>\n      <td>36724.0</td>\n      <td>0.014970</td>\n      <td>0.032934</td>\n      <td>0.341317</td>\n      <td>0.658683</td>\n      <td>0.059880</td>\n      <td>0.020958</td>\n      <td>0.017964</td>\n      <td>0.026946</td>\n      <td>0.047904</td>\n      <td>0.041916</td>\n      <td>0.198</td>\n      <td>0.425</td>\n      <td>65138</td>\n      <td>-38.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5698 rows Ã— 15 columns</p>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfe=df[feat_list].copy()\n",
    "df_rfe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4570470777946193\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[col],labels, test_size = 0.33, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6465667833499533\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_rfe,labels, test_size = 0.33, random_state=42)\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=2016)\n",
    "# for train_index, test_index in sss.split(df, labels):\n",
    "# x_train = df.iloc[train_index]\n",
    "# X_test = df.iloc[test_index]\n",
    "# y_train = labels.iloc[train_index]\n",
    "# y_test = labels.iloc[test_index]\n",
    "\n",
    "rfg=RandomForestRegressor(n_estimators=100)\n",
    "rfg.fit(x_train,y_train)\n",
    "print(rfg.score(x_test,y_test))\n",
    "# print(rfg.feature_importances_)\n",
    "\n",
    "# metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfg.predict_proba(x_test)[:, 1]),\n",
    "#             'Accuracy':rfg.score(x_test, y_test), 'Recall':recall_score(y_test, rfg.predict(x_test)),\n",
    "#             'Precision':precision_score(y_test, rfg.predict(x_test), zero_division=0), 'F1':f1_score(y_test, rfg.predict(x_test))}\n",
    "# metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43730337548323595\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: importance score\n",
      "Mass 6.219611866425997\n",
      "frac_aa_C 3.7906416395642863\n",
      "frac_aa_Q 5.353379181900639\n",
      "fraction_exposed_nonpolar_exposed 6.581614366243876\n",
      "fraction_exposed_polar_exposed 5.513655378393571\n",
      "fraction_total_exposed_E 3.613280819316462\n",
      "fraction_total_exposed_L 5.922764031610669\n",
      "fraction_total_exposed_M 5.817975298836434\n",
      "fraction_total_exposed_Q 5.086106650778767\n",
      "fraction_exposed_exposed_A 5.4386754748774395\n",
      "fraction_exposed_exposed_P 5.198639653426508\n",
      "nsp_secondary_structure_sheet 7.66734410793521\n",
      "nsp_secondary_structure_helix 4.031894860062419\n",
      "Raw_FileID 22.10910602196721\n",
      "Zeta Potential 7.655310648660528\n"
     ]
    }
   ],
   "source": [
    "feat_importances=rfg.feature_importances_\n",
    "# print(feat_importances)\n",
    "print('feature: importance score')\n",
    "for i,col in enumerate(df_rfe.columns):\n",
    "    print(col,feat_importances[i]*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

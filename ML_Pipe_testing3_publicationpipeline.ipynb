{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import _gradient_boosting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions_pipe_testing import *\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions_KP import *\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Instructions for the pipeline Requires two inputs for training: - Mass spec data with corresponding NP surface characteristics and experimental conditions (time, concentration) - NetsurfP and Biopython data that has been precalculated - X characteristics to predict\n",
    "pipeline Take mass spec spreadsheet Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration Merge with Proteome data to get file that has Accession,Enrichment,Dh,TEM,Zp,BET,Composition,Ligand,Shape,IncubationTime,IncubationConcentration,Mass,Length,Sequence Calculate protein features using biopython Merge with NSP data to get all protein features\n",
    "Split into X and Y dataset with Entries as labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 32, 33, 34, 35, 36, 37, 38, 44, 45, 46, 47, 48, 49, 50, 'Unnamed: 26']\n"
     ]
    }
   ],
   "source": [
    "all_file='Input_data/Proteomic data/abundance/Norm_Intensity_Bovsynth022123.xlsx'\n",
    "raw_MS_data=pd.read_excel(all_file,header=0)\n",
    "col_length=raw_MS_data.shape[0]\n",
    "to_drop=[]\n",
    "for i in raw_MS_data.columns:\n",
    "    column=raw_MS_data[i]\n",
    "    zeros=(column==0).sum()\n",
    "    # print(zeros)\n",
    "    # print(zeros)\n",
    "    # print(raw_MS_data[i])\n",
    "    if zeros>int(col_length/2):\n",
    "        to_drop.append(i)\n",
    "        # print(raw_MS_data.shape[1])\n",
    "# print(raw_MS_data.shape[1])\n",
    "print(to_drop)\n",
    "raw_MS_data.drop(columns=to_drop,inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pull together Proteomic data\n",
    "in_dir=\"Input_data/Proteomic data/abundance/\"\n",
    "all_file='Input_data/Proteomic data/abundance/Norm_Intensity_Bovsynth022123.xlsx'\n",
    "#combine Mass Spec data input into one excel spreadsheet - Entry - Abundance labeled by NP Unique ID\n",
    "#Abundance as a percent\n",
    "#take files in_dir and combine then into one pandas df (raw_MS_data) ###USE when combining multiple datasets####\n",
    "# files = os.listdir(in_dir)\n",
    "# for i,f in enumerate(files):\n",
    "#     if i==0:\n",
    "#         raw_MS_data=pd.read_excel(in_dir+f,header=0)\n",
    "#     else:\n",
    "#         temp = pd.read_excel(in_dir+f,header=0)\n",
    "#         raw_MS_data=raw_MS_data.merge(temp,how='outer',on='Entry')\n",
    "raw_MS_data=pd.read_excel(all_file,header=0)\n",
    "#drop samples that are missing more than half of the proteins in their file, dont use this section currently\n",
    "# col_length=raw_MS_data.shape[0]\n",
    "# to_drop=[]\n",
    "# for i in raw_MS_data.columns:\n",
    "#     column=raw_MS_data[i]\n",
    "#     zeros=(column==0).sum()\n",
    "#     if zeros>int(col_length/2):\n",
    "#         to_drop.append(i)\n",
    "# raw_MS_data.drop(columns=to_drop,inplace=True)\n",
    "# melt the df to make it an accession number, NPUNID, Abundance dataset\n",
    "raw_MS_data = pd.melt(raw_MS_data, id_vars=['Entry'],var_name='Sample_num', value_name='Abundance')\n",
    "#remove prots that were added due to merge\n",
    "raw_MS_data=raw_MS_data.dropna()\n",
    "###Bring in controls (MS data for serums)##\n",
    "controls=pd.read_excel('Input_data/Proteomic data/controls_combined.xlsx',header=0)\n",
    "MS_data_controls = pd.merge(raw_MS_data,controls,how='left', on='Entry')\n",
    "###Bring in Uniprot_data,NSPdata and NP data##\n",
    "uniprot_filepath='Input_data/BioPython_data/Combined_biopyCalcs.xlsx'\n",
    "uniprot_dat=pd.read_excel(uniprot_filepath,header=0)\n",
    "NSPfilePath='Input_data/NetSurfP_data/Combined.xlsx'\n",
    "NSP_data=pd.read_excel(NSPfilePath)\n",
    "###Bring in NP data and merge to get complete NP dataset###\n",
    "NP_filepath='Input_data/NPs/NP_Database_2.xlsx'\n",
    "NPUNdata=pd.read_excel(NP_filepath,header=0,sheet_name='NPUNID')\n",
    "NPprop=pd.read_excel(NP_filepath,header=0,sheet_name='NP_Props')\n",
    "NPdata=pd.merge(NPUNdata,NPprop,how=\"left\",on='NPID')\n",
    "NPdata.dropna(inplace=True)\n",
    "#calculate Enrichment\n",
    "#####MAYBE add binning here to keep negative results and improve capapbilities######\n",
    "# MS_data_controls['Enrichment']= np.log2(MS_data_controls['Abundance']/MS_data_controls['Abundance_Controls'])\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance','Abundance_Controls'])\n",
    "#keep abundance Controls\n",
    "# MS_data=MS_data_controls.drop(columns=['Abundance'])\n",
    "raw_prop_data=pd.merge(MS_data_controls, uniprot_dat.drop_duplicates(subset=['Entry']), how='left',on='Entry')\n",
    "Protein_data_complete = pd.merge(raw_prop_data, NSP_data.drop_duplicates(subset=['Entry']),how='left', on='Entry') #merges netsurfp features and biopython features\n",
    "Protein_data_complete.fillna(0,inplace=True)\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n",
    "\n",
    "data_complete= pd.merge(Protein_data_complete,NPdata,how='inner', on='Sample_num')\n",
    "data_complete.drop(columns=['notes','Notes','NPUNID'],inplace=True)\n",
    "data_complete.fillna(0,inplace=True)\n",
    "data_complete= data_complete.replace([-np.inf],'-12')\n",
    "data_complete=data_complete.replace([np.inf],'12')\n",
    "#create ordinal variables\n",
    "# data_complete2=pd.get_dummies(data_complete, columns=['Core Material','Surface_Ligand'])\n",
    "le=LabelEncoder()\n",
    "data_complete['Core Material']=le.fit_transform(data_complete['Core Material'])\n",
    "data_complete['Surface_Ligand']=le.fit_transform(data_complete['Surface_Ligand'])\n",
    "\n",
    "#set labels (what we are trying to predict) as Enrichment column\n",
    "# labels=data_complete['Enrichment'].copy()\n",
    "label_abund=np.ravel(data_complete['Abundance'].copy())\n",
    "label_abund_df=pd.DataFrame(label_abund)\n",
    "# label_enrich=np.ravel(data_complete['Enrichment'].copy())\n",
    "#make it one dimenisional\n",
    "#drop qualitative, not neccessary, and label columns\n",
    "#create df without bonus NSP columns (remove total_exposed)\n",
    "to_drop=data_complete.filter(like='total_exposed_')\n",
    "data_complete.drop(columns=to_drop,inplace=True)\n",
    "df=data_complete.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n",
    "# df_enrich=data_complete.drop(['Entry','Abundance','Sequence','NPID','Ligands','Protein Source','Sample_num','Unnamed: 5','Raw_FileID'],axis=1)\n",
    "df.to_excel(\"Input_data/Save_files/df_synth.xlsx\")\n",
    "label_abund_df.to_excel(\"Input_data/Save_files/label_abund_synth.xlsx\")\n",
    "# label_enrich.to_excel(\"Input_data/Save_files/label_enrich_synth.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7562,)\n",
      "(7562, 10)\n"
     ]
    }
   ],
   "source": [
    "df_2=pd.read_excel(\"Input_data/Save_files/df_synth_RFECV.xlsx\")\n",
    "label_df=pd.read_excel(\"Input_data/Save_files/label_abund_synth.xlsx\")\n",
    "label_abund=np.ravel(label_df[0])\n",
    "print(label_abund.shape)\n",
    "print(df_2.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Run PCA to seee how data differentiates#\n",
    "from sklearn.decomposition import PCA\n",
    "pca= PCA(n_components=2)\n",
    "x_pca=pca.fit_transform(df)\n",
    "\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c=label_abund, cmap='viridis')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.savefig('Output_data/PCA.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Remove correlated features (over r2 threshold level) and output reduced dataframe (df2)# ##Maybe use in future##\n",
    "# corr_matrix = df.corr()\n",
    "# threshold = 0.8\n",
    "# correlated_features = set()\n",
    "#\n",
    "# for i in range(len(corr_matrix.columns)):\n",
    "#     for j in range(i):\n",
    "#         if threshold < abs(corr_matrix.iloc[i, j]) < 1:\n",
    "#             colname = corr_matrix.columns[i]\n",
    "#             correlated_features.add(colname)\n",
    "# correlated_features\n",
    "# df_2=df.drop(columns=correlated_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#use recursive feature elimination with Random Forest Regression as the estimator to select top 45 important features\n",
    "step=2\n",
    "feats=45\n",
    "estimator=RandomForestRegressor(n_estimators=100)\n",
    "selector = RFE(estimator, n_features_to_select=feats, step=step)\n",
    "selector = selector.fit(df, label_abund)\n",
    "selector.support_\n",
    "ranking=selector.ranking_\n",
    "feat_list = selector.get_feature_names_out()\n",
    "df_rfe=df[feat_list]\n",
    "feat_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#run Recursive feature elimination with cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "step=1\n",
    "min_feats=1\n",
    "cv= KFold(n_splits=10)\n",
    "estimator=RandomForestRegressor(n_estimators=100)\n",
    "# estimator=Lasso(alpha=.05)\n",
    "selector = RFECV(estimator=estimator, cv=cv, scoring='neg_mean_squared_error', min_features_to_select=min_feats, step=step)\n",
    "selector = selector.fit(df_rfe, label_abund)\n",
    "selector.support_\n",
    "feat_list2 = selector.get_feature_names_out()\n",
    "selected_features= df_rfe.columns[selector.support_]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2=df[feat_list2]\n",
    "n_scores = len(selector.cv_results_[\"mean_test_score\"])\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "x=range(1, n_scores + 1)\n",
    "y=selector.cv_results_[\"mean_test_score\"]\n",
    "err=selector.cv_results_[\"std_test_score\"]\n",
    "plt.plot(x,y,'k-')\n",
    "plt.fill_between(x,y-err,y+err)\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.savefig('Output_data/RFECV45.png')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfecv_df=pd.DataFrame(selector.cv_results_)\n",
    "rfecv_df.to_excel(\"Output_data/RFECV_data.xlsx\")\n",
    "feat_list2\n",
    "df_2.to_excel(\"Input_data/Save_files/df_synth_RFECV.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#try using a meta model including a linear model and Random Forest Model#\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "#\n",
    "#\n",
    "#\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_rfe, label_abund, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # Create the base models\n",
    "# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# ridge = Ridge(alpha=0.1, random_state=42)\n",
    "#\n",
    "# rf_model = rf.fit(X_train,y_train)\n",
    "# ridge_model = ridge.fit(X_train,y_train)\n",
    "# # Make predictions with the base models\n",
    "# rf_pred = cross_val_predict(rf_model, X_train, y_train, cv=5)\n",
    "# ridge_pred = cross_val_predict(ridge_model, X_train, y_train, cv=5)\n",
    "#\n",
    "# # Create the input for the meta-model\n",
    "# stacking_input = np.column_stack((rf_pred, ridge_pred))\n",
    "#\n",
    "# # Train the meta-model (ridge regression)\n",
    "# meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "# meta_model.fit(stacking_input, y_train)\n",
    "#\n",
    "# # Make predictions with the meta-model\n",
    "# rf_pred_test = rf_model.predict(X_test)\n",
    "# ridge_pred_test = ridge_model.predict(X_test)\n",
    "# stacking_input_test = np.column_stack((rf_pred_test, ridge_pred_test))\n",
    "# y_pred_test = meta_model.predict(stacking_input_test)\n",
    "#\n",
    "# # Evaluate the performance of the stacked model\n",
    "# mse = mean_squared_error(y_test, y_pred_test)\n",
    "# print(\"MSE: {:.2f}\".format(mse))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#test estimator to see what is the ideal number of estimators\n",
    "# estimators=np.arange(5,500,5)\n",
    "# out_name=np.arange(5,500,5)\n",
    "# # print(out_name)\n",
    "# scores=[]\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df_abund, label_abund,\n",
    "#                                                         test_size=0.2,\n",
    "#                                                         random_state=42)\n",
    "#\n",
    "# for i in range(len(estimators)):\n",
    "#     rfg = RandomForestRegressor(n_estimators=estimators[i])\n",
    "#     rfg.fit(x_train, y_train)\n",
    "#     score=rfg.score(x_test,y_test)\n",
    "#     scores.append(score)\n",
    "# a=pd.DataFrame(list(zip(estimators,scores)), columns=['number of estimators','accuracy'])\n",
    "# a.to_excel(\"Output_data/estimators_score_abund.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#look at loss as a function of feature\n",
    "feats=[]\n",
    "scores=[]\n",
    "df=df_2\n",
    "id='_RFECV'\n",
    "label=label_abund\n",
    "# frames=['df','df_norm','df_NSP_drop']\n",
    "# for x,i in enumerate(df_listnew):\\\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df,label, test_size = 0.2, random_state=42)\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "importances=rfr.feature_importances_*100\n",
    "scores.append(topscore)\n",
    "feats.append('allfeats')\n",
    "for j in x_test.columns:\n",
    "    tmp=x_test.copy()\n",
    "    # print(x_test)\n",
    "    np.random.shuffle(tmp[j].values)\n",
    "    scram_score=rfr.score(tmp,y_test)\n",
    "    scores.append(scram_score)\n",
    "    feats.append(j)\n",
    "\n",
    "a=pd.DataFrame(list(zip(feats,scores,importances)),columns=['feat','score','importances'])\n",
    "a.to_excel(\"Output_data/scram_loss_feats\"+id+\".xlsx\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Fit and predict after dropping the next least loss feature, drop one feature at a time\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "feats=[]\n",
    "scores=[]\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "a=list(zip(rfr.feature_importances_,rfr.feature_names_in_))\n",
    "a.sort(reverse=True)\n",
    "col_import=pd.DataFrame(a,columns=['importances','names'])\n",
    "sorted_cols=col_import['names']\n",
    "feats.append('topscore')\n",
    "scores.append(topscore)\n",
    "\n",
    "#df_3=df_2.copy()\n",
    "for i in sorted_cols:\n",
    "    df_3=df_2.copy() #remove if you only want to drop each feature instead of dropping one feature at a time\n",
    "    df_3.drop(columns=[i],inplace=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_3,label_abund, test_size = 0.2, random_state=42)\n",
    "    rfr.fit(x_train,y_train)\n",
    "    score=rfr.score(x_test,y_test)\n",
    "    scores.append(score)\n",
    "    feats.append(i)\n",
    "\n",
    "df_out=pd.DataFrame(list(zip(feats,scores)),columns=['dropped feat','score without feat'])\n",
    "df_out.to_excel(\"Output_data/scores_afterdrop.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Fit and predict after dropping the next least loss feature, drop one feature at a time cumulative\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "feats=[]\n",
    "scores=[]\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "topscore=rfr.score(x_test,y_test)\n",
    "a=list(zip(rfr.feature_importances_,rfr.feature_names_in_))\n",
    "a.sort(reverse=True)\n",
    "col_import=pd.DataFrame(a,columns=['importances','names'])\n",
    "sorted_cols=col_import['names']\n",
    "feats.append('topscore')\n",
    "scores.append(topscore)\n",
    "\n",
    "df_3=df_2.copy()\n",
    "for i in sorted_cols:\n",
    "    if i == sorted_cols.iloc[-1]:\n",
    "        break\n",
    "    # df_3=df_2.copy() #remove if you only want to drop each feature instead of dropping one feature at a time\n",
    "    df_3.drop(columns=[i],inplace=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_3,label_abund, test_size = 0.2, random_state=42)\n",
    "    rfr.fit(x_train,y_train)\n",
    "    score=rfr.score(x_test,y_test)\n",
    "    scores.append(score)\n",
    "    feats.append(i)\n",
    "\n",
    "df_out=pd.DataFrame(list(zip(feats,scores)),columns=['dropped feat','score without feat'])\n",
    "df_out.to_excel(\"Output_data/scores_afterdrop_cumulative_reverse.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot predictions with prediction accuracies#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6064262306745349\n",
      "0.7791496130816059\n"
     ]
    }
   ],
   "source": [
    "#look at pearson correlation of predictions#\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_2,label_abund, test_size = 0.2, random_state=42)\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "rfr.fit(x_train,y_train)\n",
    "predictions=rfr.predict(x_test)\n",
    "r2=r2_score(y_test, predictions)\n",
    "corr,_ = pearsonr(y_test, predictions)\n",
    "\n",
    "print(r2)\n",
    "print(corr)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
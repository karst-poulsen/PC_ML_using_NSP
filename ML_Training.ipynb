{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "\n",
    "# Homemade functions required\n",
    "from data_prep_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Entry  NPID  Enrichment  \\\n",
      "0   P02769     1   -2.768846   \n",
      "1   P41361     1    4.335954   \n",
      "2   P00735     1    6.352583   \n",
      "3   Q9N2I2     1    6.606206   \n",
      "4   P12763     1   -1.479128   \n",
      "..     ...   ...         ...   \n",
      "67  O02659     1    3.694238   \n",
      "68  Q29RQ1     1    0.610974   \n",
      "69  P56651     1    1.687743   \n",
      "70  P07224     1    0.241398   \n",
      "71  Q32PJ2     1    1.511005   \n",
      "\n",
      "                                        Protein names  Length   Mass  \\\n",
      "0                    Albumin (BSA) (allergen Bos d 6)     607  69293   \n",
      "1                 Antithrombin-III, ATIII (Serpin C1)     465  52347   \n",
      "2   Prothrombin, EC 3.4.21.5 (Coagulation factor I...     625  70506   \n",
      "3   Plasma serine protease inhibitor (Protein C in...     404  45297   \n",
      "4   Alpha-2-HS-glycoprotein (Asialofetuin) (Fetuin-A)     359  38419   \n",
      "..                                                ...     ...    ...   \n",
      "67  Mannose-binding protein C, MBP-C (Mannan-bindi...     249  26471   \n",
      "68                            Complement component C7     843  93090   \n",
      "69  Inter-alpha-trypsin inhibitor heavy chain H2, ...      50   5984   \n",
      "70                      Vitamin K-dependent protein S     675  75133   \n",
      "71  Apolipoprotein A-IV, Apo-AIV, ApoA-IV (Apolipo...     380  43018   \n",
      "\n",
      "                                             Sequence  \\\n",
      "0   MKWVTFISLLLLFSSAYSRGVFRRDTHKSEIAHRFKDLGEEHFKGL...   \n",
      "1   MISNGIGTVTAGKRSICLLPLLLIGLWGCVTCHRSPVEDVCTAKPR...   \n",
      "2   MARVRGPRLPGCLALAALFSLVHSQHVFLAHQQASSLLQRARRANK...   \n",
      "3   MRLCLFLCLVLLGPRMATLRRSQKKKIQEVPPAVTTAPPGSRDFVF...   \n",
      "4   MKSFVLLFCLAQLWGCHSIPLDPVAGYKEPACDDPDTEQAALAAVD...   \n",
      "..                                                ...   \n",
      "67  MSLFTSLPFLLLTAVTASCADTETENCENIRKTCPVIACGPPGING...   \n",
      "68  MKAITLLFLVGFIGEFQVFSSASSPINCQWGSYAPWSECNGCTKTQ...   \n",
      "69  LHYQEVKWRKLGSYEHRLHLKPGRLAKHELEVFNGYFVHFPAPENM...   \n",
      "70  MRVLGGRTGTLLACLALVLPVLEANFLSRQHASQVLIRRRRANTLL...   \n",
      "71  MFLKAVVLSLALVAVTGAEAEVNADQVATVIWDYFSQLGNNAKKAV...   \n",
      "\n",
      "                                   Gene Ontology (GO)  \n",
      "0   extracellular region [GO:0005576]; extracellul...  \n",
      "1   extracellular space [GO:0005615]; heparin bind...  \n",
      "2   collagen-containing extracellular matrix [GO:0...  \n",
      "3   acrosomal membrane [GO:0002080]; extracellular...  \n",
      "4   extracellular matrix [GO:0031012]; extracellul...  \n",
      "..                                                ...  \n",
      "67  collagen trimer [GO:0005581]; extracellular sp...  \n",
      "68  extracellular space [GO:0005615]; membrane att...  \n",
      "69  extracellular region [GO:0005576]; serine-type...  \n",
      "70  extracellular space [GO:0005615]; calcium ion ...  \n",
      "71  chylomicron [GO:0042627]; high-density lipopro...  \n",
      "\n",
      "[72 rows x 8 columns]\n",
      "Index(['Entry', 'NPID', 'Protein names', 'Sequence', 'Length', 'Mass',\n",
      "       'frac_aa_A', 'frac_aa_C', 'frac_aa_D', 'frac_aa_E',\n",
      "       ...\n",
      "       'Core Material', 'Ligand', 'Dtem', 'Dh', 'Shaken', 'Centrifuged',\n",
      "       'Incubation Concentration (mg/ml)', 'Incubation Time (minutes)',\n",
      "       'Temperature', 'notes'],\n",
      "      dtype='object', length=110)\n",
      "Index(['Entry', 'Protein names', 'Sequence', 'Length', 'Mass', 'frac_aa_A',\n",
      "       'frac_aa_C', 'frac_aa_D', 'frac_aa_E', 'frac_aa_F',\n",
      "       ...\n",
      "       'Zeta Potential', 'Core Material', 'Ligand', 'Dtem', 'Dh', 'Shaken',\n",
      "       'Centrifuged', 'Incubation Concentration (mg/ml)',\n",
      "       'Incubation Time (minutes)', 'Temperature'],\n",
      "      dtype='object', length=108)\n"
     ]
    }
   ],
   "source": [
    "in_dir=\"Input_data/Proteomic data/\"\n",
    "out_dir=\"Output_data/\"\n",
    "NSP_dir=\"NetsurfP_Proteomes/\"\n",
    "uniprot_dir=\"UniProt/\"\n",
    "\n",
    "#Mass Spec data input in one excel spreadsheet\n",
    "datfile='test_withNPID.xlsx'\n",
    "uniprot_filepath=uniprot_dir+'Bovine_Proteome_082322.xlsx'\n",
    "uniprot_dat=pd.read_excel(uniprot_filepath,header=0)\n",
    "# Protein data input in one excel spreadsheet with two sheets\n",
    "\n",
    "datfile='test_withNPID.xlsx'\n",
    "# prot_prop = 'Protein Properties'\n",
    "# mass_spec = 'Mass Sped Details'\n",
    "\n",
    "\n",
    "\n",
    "#netsurfp data file\n",
    "NSPfilePath=NSP_dir+'Bovine_Proteome.xlsx'\n",
    "\n",
    "#NP data file\n",
    "NPdata=pd.read_excel(in_dir+\"NP_Database.xlsx\",header=0)\n",
    "\n",
    "raw_MS_data=pd.read_excel(in_dir+datfile,header=0)\n",
    "raw_prop_data=pd.merge(raw_MS_data, uniprot_dat, left_on='Entry', right_on='Entry')\n",
    "print(raw_prop_data)\n",
    "NSP_data=pd.read_excel(NSPfilePath)\n",
    "#\n",
    "# #clean up and calculate % protein abundance and enrichment, function found in data_prep_functions line 470\n",
    "#\n",
    "MS_data_clean = raw_MS_data.copy()\n",
    "Accesions_IDs = MS_data_clean[\"Entry\"].to_frame()\n",
    "\n",
    "\n",
    "# clean up protein data, function found in data_prep_functions.py line 367\n",
    "\n",
    "PROT_cleaned_data = clean_up_data_biopy(raw_prop_data, Accesions_IDs) #calculates biopython features from protein sequences, and removes proteins removed during mass spec clean up\n",
    "\n",
    "PROT_cleaned_data = normalize_mass_length_1DF(PROT_cleaned_data) #function found in data_prep_functions line 167, normalizes mass, length and mw by dividing all values by the max in the column\n",
    "\n",
    "Protein_data_complete = pd.merge(PROT_cleaned_data, NSP_data, left_on='Entry', right_on='Entry') #merges netsurfp features and biopython features\n",
    "\n",
    "\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n",
    "# print(Protein_data_complete.columns)\n",
    "# print(NPdata)\n",
    "\n",
    "data_complete= pd.merge(Protein_data_complete,NPdata,how='left', on='NPID')\n",
    "print(data_complete.columns)\n",
    "data_complete.drop(labels=['notes','NPID'],inplace=True,axis=1)\n",
    "\n",
    "print(data_complete.columns)\n",
    "# print(data_complete)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "bov=pd.read_excel(\"UniProt/Bovine_Proteome_082322.xlsx\")\n",
    "mos=pd.read_excel(\"UniProt/Mouse_Proteome_082322.xlsx\")\n",
    "Bovine_Mouse=pd.concat([bov,mos])\n",
    "Bovine_Mouse.to_excel('UniProt/Bovine_Mouse_proteome.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set Data files for prediction and directory for outputs and inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Entry', 'Enrichment', 'Protein names', 'Length', 'Mass', 'Sequence',\n",
      "       'Gene Ontology (GO)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "in_dir=\"Input_data/Proteomic data/\"\n",
    "out_dir=\"Output_data/\"\n",
    "NSP_dir=\"NetsurfP_Proteomes/\"\n",
    "uniprot_dir=\"UniProt/\"\n",
    "\n",
    "#Mass Spec data input in one excel spreadsheet\n",
    "datfile='test1.xlsx'\n",
    "uniprot_filepath=uniprot_dir+'Bovine_Proteome_082322.xlsx'\n",
    "uniprot_dat=pd.read_excel(uniprot_filepath,header=0)\n",
    "# Protein data input in one excel spreadsheet with two sheets\n",
    "\n",
    "datfile='test1a.xlsx'\n",
    "# prot_prop = 'Protein Properties'\n",
    "# mass_spec = 'Mass Sped Details'\n",
    "\n",
    "\n",
    "\n",
    "#netsurfp data file\n",
    "NSPfilePath=NSP_dir+'Bovine_Proteome.xlsx'\n",
    "\n",
    "\n",
    "#load data\n",
    "raw_MS_data=pd.read_excel(in_dir+datfile,header=0)\n",
    "raw_prop_data=pd.merge(raw_MS_data, uniprot_dat, left_on='Entry', right_on='Entry')\n",
    "print(raw_prop_data.columns)\n",
    "\n",
    "# raw_prop_data=pd.read_excel(in_dir+datfile, sheet_name=prot_prop, thousands=',')\n",
    "# raw_MS_data=pd.read_excel(in_dir+datfile, sheet_name=mass_spec, header=0)\n",
    "NSP_data=pd.read_excel(NSPfilePath)\n",
    "#\n",
    "# #clean up and calculate % protein abundance and enrichment, function found in data_prep_functions line 470\n",
    "#\n",
    "MS_data_clean = raw_MS_data.copy()\n",
    "Accesions_IDs = MS_data_clean[\"Entry\"].to_frame()\n",
    "\n",
    "\n",
    "# clean up protein data, function found in data_prep_functions.py line 367\n",
    "\n",
    "PROT_cleaned_data = clean_up_data_biopy(raw_prop_data, Accesions_IDs) #calculates biopython features from protein sequences, and removes proteins removed during mass spec clean up\n",
    "\n",
    "PROT_cleaned_data = normalize_mass_length_1DF(PROT_cleaned_data) #function found in data_prep_functions line 167, normalizes mass, length and mw by dividing all values by the max in the column\n",
    "\n",
    "Protein_data_complete = pd.merge(PROT_cleaned_data, NSP_data, left_on='Entry', right_on='Entry') #merges netsurfp features and biopython features\n",
    "\n",
    "\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Training Models and Classification\n",
    "there are three ways to classify in corona versus out of corona here. 1) landry classifer 2) split classifer and 3) psudeo abundance thresholding for 1 and 2 run graphing script at end, 3 has graphing already integrated\n",
    "anything that says ignore is left over code/useless code from when they use to run 2 serums at a time, needs to be further cleaned (only commented one classifer since they all share same code except for classification part"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power  1.0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Accession', 'NP_%_Abundance', 'FBS Relative Abundance'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPower \u001B[39m\u001B[38;5;124m'\u001B[39m, thresh_power)\n\u001B[0;32m     11\u001B[0m fluids_type_list\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCMNP200\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;66;03m#ignore\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m CMNP200_labels \u001B[38;5;241m=\u001B[39m \u001B[43mMS_data_clean\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mAccession\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mNP_\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m_Abundance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mEnrichment\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mFBS Relative Abundance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;66;03m#makes copy of mass spec data\u001B[39;00m\n\u001B[0;32m     13\u001B[0m data_sheets \u001B[38;5;241m=\u001B[39m [CMNP200_labels]\u001B[38;5;66;03m#ignore\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m#fit data to distribution, ignore for loop part\u001B[39;00m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3509\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[0;32m   3510\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 3511\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   3513\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[0;32m   3514\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5782\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   5779\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   5780\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[1;32m-> 5782\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5784\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   5785\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   5786\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5845\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[1;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[0;32m   5842\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   5844\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[1;32m-> 5845\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['Accession', 'NP_%_Abundance', 'FBS Relative Abundance'] not in index\""
     ]
    }
   ],
   "source": [
    "### Landry Classifer\n",
    "\n",
    "trials = 100 #number of models per threshold power\n",
    "first_frame = True #starting dataframe for saving metrics\n",
    "correctness_frame = pd.DataFrame()\n",
    "metrics_frame = pd.DataFrame()\n",
    "print_metrics = 0 #0, doesn't show metrics while runnning for each model, 1 does show metrics\n",
    "\n",
    "for thresh_power in np.arange(1,3.6,.25): #loop for training 100 models for each power\n",
    "    print('Power ', thresh_power)\n",
    "    fluids_type_list=['CMNP200'] #ignore\n",
    "    CMNP200_labels = MS_data_clean[['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']].copy() #makes copy of mass spec data\n",
    "    data_sheets = [CMNP200_labels]#ignore\n",
    "    #fit data to distribution, ignore for loop part\n",
    "    x = np.arange(0,1,.01)\n",
    "    for k in range(1):\n",
    "        sheet = data_sheets[k] #ignore\n",
    "        fluid_type = fluids_type_list[k]#ignore\n",
    "\n",
    "        print(f'\\n{fluid_type.capitalize()} running')\n",
    "\n",
    "        num_proteins = np.zeros(100)\n",
    "        for i in np.arange(0,100,1):\n",
    "            thresh = i/100\n",
    "            index = (sheet['NP_%_Abundance']>thresh)\n",
    "            num_proteins[i] = (np.count_nonzero(index))\n",
    "\n",
    "\n",
    "        num_proteins_above_1_abundance = min(num_proteins)\n",
    "        biexponent_dist = num_proteins - num_proteins_above_1_abundance\n",
    "        num_protein_thresh = np.max(biexponent_dist)/pow(np.e,thresh_power)\n",
    "        cutoff_thresh_value = min(x[(biexponent_dist<num_protein_thresh)&(biexponent_dist>0)], default=0)\n",
    "        #classifer\n",
    "        sheet['Corona'] = ((sheet['NP_%_Abundance']>cutoff_thresh_value) | (sheet['Enrichment']>1)).astype(int)\n",
    "        #percent of protein classifed as in corona\n",
    "        print(sheet['Corona'].sum()/sheet['Corona'].shape[0])\n",
    "    #drop unneeded columns\n",
    "    CMNP200_labels = CMNP200_labels.drop(['NP_%_Abundance', 'Enrichment', 'FBS Relative Abundance'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # function found in data prep functions.py line 827, if there are multiple accession numbers associated with single protein expands that into multiple row entries\n",
    "    CMNP200_labels = accession_expansion(CMNP200_labels)\n",
    "\n",
    "    #merge in/out of corona labels and features\n",
    "    CMNP200_total_data_names = pd.merge(CMNP200_labels, Protein_data_complete.copy(), left_on='Accession', right_on='Entry')\n",
    "    #drop unneeded columns\n",
    "    CMNP200_total_data_names =CMNP200_total_data_names.drop(['Accession','Entry', 'entry', 'Sequence', 'Length', 'Mass'], axis=1)\n",
    "\n",
    "    #make a copy of just features\n",
    "    features_CMNP200 = CMNP200_total_data_names.copy()\n",
    "    features_CMNP200 = features_CMNP200.drop(['Corona'], axis=1)\n",
    "    #make copy of just labels\n",
    "    names_CMNP200 = CMNP200_total_data_names['Corona'].copy()\n",
    "    #ignore\n",
    "    features_merged_naive = features_CMNP200\n",
    "    names_merged = names_CMNP200\n",
    "\n",
    "    #normalize dataset with min/max scaling\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    #make copy of features\n",
    "    total_data = features_merged_naive.copy()\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data = total_data.drop(['Protein names'], axis=1)\n",
    "\n",
    "    #fit scaler to data\n",
    "    scaler = scaler.fit(total_data)\n",
    "    scaled_df = pd.DataFrame(scaler.transform(total_data), columns=total_data.columns)\n",
    "\n",
    "    #ignore\n",
    "    names = names_merged.copy()\n",
    "\n",
    "\n",
    "    #ignore\n",
    "    df = scaled_df.copy()\n",
    "    labels = names\n",
    "    #make data frame for feature importances\n",
    "    feature_imp = pd.DataFrame(columns=list(scaled_df.columns))\n",
    "\n",
    "    i = 0\n",
    "    #split data into text and train n times\n",
    "    sss = StratifiedShuffleSplit(n_splits=trials, test_size=0.1, random_state=2016)\n",
    "    for train_index, test_index in sss.split(df, labels):\n",
    "        X_train = df.iloc[train_index]\n",
    "        X_test = df.iloc[test_index]\n",
    "        y_train = labels.iloc[train_index]\n",
    "        y_test = labels.iloc[test_index]\n",
    "        k = i\n",
    "        #syntheic oversampling\n",
    "        sme = SMOTE(sampling_strategy=1, random_state=i, n_jobs=-1, k_neighbors=12)\n",
    "        X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Create and Train\n",
    "        rfc=RandomForestClassifier(criterion='entropy', n_jobs=-1, random_state=i, n_estimators=100)\n",
    "        rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "\n",
    "        #everyting below is for saving meteric of each model trained\n",
    "        if first_frame:  # Initialize\n",
    "            first_frame = False  # Don't Come back Here\n",
    "\n",
    "            datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "\n",
    "            correctness_frame = pd.DataFrame(data=datadict)\n",
    "            correctness_frame['round'] = i\n",
    "\n",
    "            metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "            'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "            'Precision':precision_score(y_test, rfc.predict(X_test), zero_division=0), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "\n",
    "            metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()\n",
    "            metrics_frame['round'] = i\n",
    "            metrics_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "            if print_metrics == 1:\n",
    "                print('Round:', i)\n",
    "                print('Correctness Frame')\n",
    "                print(correctness_frame)\n",
    "                print('Metrics Frame')\n",
    "                print(metrics_frame)\n",
    "\n",
    "\n",
    "        else:\n",
    "            datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "            revolve_frame = pd.DataFrame(data=datadict)\n",
    "            revolve_frame['round'] = i\n",
    "            correctness_frame = pd.concat([correctness_frame,revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "            metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "            'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "            'Precision':precision_score(y_test, rfc.predict(X_test), zero_division=0), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "            metrics_revolve_frame = pd.DataFrame.from_dict(data=metrics_dict, orient='index').transpose()\n",
    "            metrics_revolve_frame['round'] = i\n",
    "            metrics_revolve_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "            metrics_frame = pd.concat([metrics_frame,metrics_revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "            if print_metrics == 1:\n",
    "                print(\"Round:\", i)\n",
    "                print(\"Correctness Frame\")\n",
    "                print(revolve_frame)\n",
    "                print('Metrics')\n",
    "                print(metrics_revolve_frame)\n",
    "\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "        feature_imp.loc[i] = pd.Series(rfc.feature_importances_,index=list(df.columns))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "## Split Classifer\n",
    "\n",
    "\n",
    "trials = 100\n",
    "first_frame = True\n",
    "correctness_frame = pd.DataFrame()\n",
    "metrics_frame = pd.DataFrame()\n",
    "print_metrics = 0\n",
    "\n",
    "for thresh_power in np.arange(1,3.6,.25):\n",
    "    print('Power ', thresh_power)\n",
    "    fluids_type_list=['CMNP200']\n",
    "    CMNP200_labels = MS_data_clean[['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']].copy()\n",
    "    data_sheets = [CMNP200_labels]\n",
    "    x = np.arange(0,1,.01)\n",
    "    for k in range(1):\n",
    "        sheet = data_sheets[k]\n",
    "        fluid_type = fluids_type_list[k]\n",
    "\n",
    "        print(f'\\n{fluid_type.capitalize()} running')\n",
    "\n",
    "        num_proteins = np.zeros(100)\n",
    "        for i in np.arange(0,100,1):\n",
    "            thresh = i/100\n",
    "            index = (sheet['NP_%_Abundance']>thresh)\n",
    "            num_proteins[i] = (np.count_nonzero(index))\n",
    "\n",
    "\n",
    "        num_proteins_above_1_abundance = min(num_proteins)\n",
    "        biexponent_dist = num_proteins - num_proteins_above_1_abundance\n",
    "        num_protein_thresh = np.max(biexponent_dist)/pow(np.e,thresh_power)\n",
    "        cutoff_thresh_value = min(x[(biexponent_dist<num_protein_thresh)&(biexponent_dist>0)], default=0)\n",
    "        psudeo_enriched_proteins = sheet.loc[sheet[\"FBS Relative Abundance\"] < 0.044, ['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']]\n",
    "        non_psudeo_enriched_proteins = sheet.loc[sheet[\"FBS Relative Abundance\"] > 0.044, ['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']]\n",
    "        psudeo_enriched_proteins['Corona'] = (psudeo_enriched_proteins['NP_%_Abundance']>cutoff_thresh_value).astype(int)\n",
    "        non_psudeo_enriched_proteins['Corona'] = ((non_psudeo_enriched_proteins['NP_%_Abundance']>cutoff_thresh_value) | (non_psudeo_enriched_proteins['Enrichment']>1)).astype(int)\n",
    "        psudeo_enriched_prot_labels = psudeo_enriched_proteins[[\"Accession\", \"Corona\"]]\n",
    "        non_psudeo_enriched_pro_labels = non_psudeo_enriched_proteins[[\"Accession\", \"Corona\"]]\n",
    "        remerged_protein_labels = pd.concat([non_psudeo_enriched_pro_labels,psudeo_enriched_prot_labels], ignore_index=True, axis=0)\n",
    "        sheet = pd.merge(sheet,remerged_protein_labels, how=\"inner\", on='Accession')\n",
    "        print(sheet['Corona'].sum()/sheet['Corona'].shape[0])\n",
    "        CMNP200_labels = sheet\n",
    "    CMNP200_labels = CMNP200_labels.drop(['NP_%_Abundance', 'Enrichment', 'FBS Relative Abundance'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # checks for any accession lists\n",
    "    CMNP200_labels = accession_expansion(CMNP200_labels)\n",
    "\n",
    "\n",
    "    CMNP200_total_data_names = pd.merge(CMNP200_labels, Protein_data_complete.copy(), left_on='Accession', right_on='Entry')\n",
    "\n",
    "    CMNP200_total_data_names =CMNP200_total_data_names.drop(['Accession','Entry', 'entry', 'Sequence', 'Length', 'Mass'], axis=1)\n",
    "\n",
    "\n",
    "    features_CMNP200 = CMNP200_total_data_names.copy()\n",
    "    features_CMNP200 = features_CMNP200.drop(['Corona'], axis=1)\n",
    "    names_CMNP200 = CMNP200_total_data_names['Corona'].copy()\n",
    "\n",
    "    #rename database to match combined database name\n",
    "\n",
    "    features_merged_naive = features_CMNP200\n",
    "    names_merged = names_CMNP200\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    total_data = features_merged_naive.copy()\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data = total_data.drop(['Protein names'], axis=1)\n",
    "    scaler = scaler.fit(total_data)\n",
    "    scaled_df = pd.DataFrame(scaler.transform(total_data), columns=total_data.columns)\n",
    "\n",
    "    names = names_merged.copy()\n",
    "\n",
    "\n",
    "\n",
    "    df = scaled_df.copy()\n",
    "    labels = names\n",
    "    feature_imp = pd.DataFrame(columns=list(scaled_df.columns))\n",
    "\n",
    "    i = 0\n",
    "    sss = StratifiedShuffleSplit(n_splits=trials, test_size=0.1, random_state=2016)\n",
    "    for train_index, test_index in sss.split(df, labels):\n",
    "        X_train = df.iloc[train_index]\n",
    "        X_test = df.iloc[test_index]\n",
    "        y_train = labels.iloc[train_index]\n",
    "        y_test = labels.iloc[test_index]\n",
    "        k = i\n",
    "        sme = SMOTE(sampling_strategy=1, random_state=i, n_jobs=-1, k_neighbors=12)\n",
    "        X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Create and Train\n",
    "        rfc=RandomForestClassifier(criterion='entropy', n_jobs=-1, random_state=i, n_estimators=100)\n",
    "\n",
    "        rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "        if first_frame:  # Initialize\n",
    "            first_frame = False  # Don't Come back Here\n",
    "\n",
    "            datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "\n",
    "            correctness_frame = pd.DataFrame(data=datadict)\n",
    "            correctness_frame['round'] = i\n",
    "\n",
    "            metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "            'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "            'Precision':precision_score(y_test, rfc.predict(X_test), zero_division=0), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "\n",
    "            metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()\n",
    "            metrics_frame['round'] = i\n",
    "            metrics_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "            if print_metrics == 1:\n",
    "                print('Round:', i)\n",
    "                print('Correctness Frame')\n",
    "                print(correctness_frame)\n",
    "                print('Metrics Frame')\n",
    "                print(metrics_frame)\n",
    "\n",
    "\n",
    "        else:\n",
    "            datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "            revolve_frame = pd.DataFrame(data=datadict)\n",
    "            revolve_frame['round'] = i\n",
    "            correctness_frame = pd.concat([correctness_frame,revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "            metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "            'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "            'Precision':precision_score(y_test, rfc.predict(X_test), zero_division=0), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "            metrics_revolve_frame = pd.DataFrame.from_dict(data=metrics_dict, orient='index').transpose()\n",
    "            metrics_revolve_frame['round'] = i\n",
    "            metrics_revolve_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "            metrics_frame = pd.concat([metrics_frame,metrics_revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "            if print_metrics == 1:\n",
    "                print(\"Round:\", i)\n",
    "                print(\"Correctness Frame\")\n",
    "                print(revolve_frame)\n",
    "                print('Metrics')\n",
    "                print(metrics_revolve_frame)\n",
    "\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "        feature_imp.loc[i] = pd.Series(rfc.feature_importances_,index=list(df.columns))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###psuedo abundance thresholding classifer\n",
    "\n",
    "trials = 100\n",
    "first_frame = True\n",
    "correctness_frame = pd.DataFrame()\n",
    "metrics_frame = pd.DataFrame()\n",
    "print_metrics = 0\n",
    "\n",
    "##generate thresholding list for psuedo abudances\n",
    "\n",
    "abudance_list = []\n",
    "temp_df = raw_MS_data.copy()\n",
    "temp_df.loc[temp_df['FBS Relative Abundance'] == 0.0431998478815122, 'FBS Relative Abundance'] = np.nan\n",
    "min_abundance = temp_df[\"FBS Relative Abundance\"].min()\n",
    "\n",
    "for i in range(1,9,1):\n",
    "    i = i*0.1\n",
    "    abudance_list.append(min_abundance*i)\n",
    "\n",
    "First_Pass = True\n",
    "y = 0\n",
    "\n",
    "for j in abudance_list:\n",
    "\n",
    "    if First_Pass == True:\n",
    "        temp_df['FBS Relative Abundance'] = temp_df['FBS Relative Abundance'].replace(np.nan, j)\n",
    "        CMNP200_mass_spec_data_clean = clean_up_data_mass_spec(temp_df)\n",
    "        First_Pass = False\n",
    "    else:\n",
    "        temp_df['FBS Relative Abundance'] = temp_df['FBS Relative Abundance'].replace(abudance_list[y], j)\n",
    "        CMNP200_mass_spec_data_clean = clean_up_data_mass_spec(temp_df)\n",
    "        y = y + 1\n",
    "\n",
    "    print('Abundance Threshold', round(j,3))\n",
    "\n",
    "    for thresh_power in np.arange(1,3.6,.25):\n",
    "        print('Power ', thresh_power)\n",
    "        fluids_type_list=['CMNP200']\n",
    "        CMNP200_labels = CMNP200_mass_spec_data_clean[['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']].copy()\n",
    "        data_sheets = [CMNP200_labels]\n",
    "        x = np.arange(0,1,.01)\n",
    "        for k in range(1):\n",
    "            sheet = data_sheets[k]\n",
    "            fluid_type = fluids_type_list[k]\n",
    "\n",
    "            print(f'\\n{fluid_type.capitalize()} running')\n",
    "\n",
    "            num_proteins = np.zeros(100)\n",
    "            for i in np.arange(0,100,1):\n",
    "                thresh = i/100\n",
    "                index = (sheet['NP_%_Abundance']>thresh)\n",
    "                num_proteins[i] = (np.count_nonzero(index))\n",
    "\n",
    "\n",
    "            num_proteins_above_1_abundance = min(num_proteins)\n",
    "            biexponent_dist = num_proteins - num_proteins_above_1_abundance\n",
    "            num_protein_thresh = np.max(biexponent_dist)/pow(np.e,thresh_power)\n",
    "            cutoff_thresh_value = min(x[(biexponent_dist<num_protein_thresh)&(biexponent_dist>0)], default=0)\n",
    "            sheet['Corona'] = ((sheet['NP_%_Abundance']>cutoff_thresh_value) | (sheet['Enrichment']>1)).astype(int)\n",
    "            print(sheet['Corona'].sum()/sheet['Corona'].shape[0])\n",
    "        CMNP200_labels = CMNP200_labels.drop(['NP_%_Abundance', 'Enrichment', 'FBS Relative Abundance'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        # checks for any accession lists\n",
    "        CMNP200_labels = accession_expansion(CMNP200_labels)\n",
    "\n",
    "\n",
    "        CMNP200_total_data_names = pd.merge(CMNP200_labels, Protein_data_complete.copy(), left_on='Accession', right_on='Entry')\n",
    "\n",
    "        CMNP200_total_data_names =CMNP200_total_data_names.drop(['Accession','Entry', 'entry', 'Sequence', 'Length', 'Mass'], axis=1)\n",
    "\n",
    "\n",
    "        features_CMNP200 = CMNP200_total_data_names.copy()\n",
    "        features_CMNP200 = features_CMNP200.drop(['Corona'], axis=1)\n",
    "        names_CMNP200 = CMNP200_total_data_names['Corona'].copy()\n",
    "\n",
    "        #rename database to match combined database name\n",
    "\n",
    "        features_merged_naive = features_CMNP200\n",
    "        names_merged = names_CMNP200\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        total_data = features_merged_naive.copy()\n",
    "        total_data = total_data.fillna(0)\n",
    "        total_data = total_data.drop(['Protein names'], axis=1)\n",
    "        scaler = scaler.fit(total_data)\n",
    "        scaled_df = pd.DataFrame(scaler.transform(total_data), columns=total_data.columns)\n",
    "\n",
    "        names = names_merged.copy()\n",
    "\n",
    "\n",
    "\n",
    "        df = scaled_df.copy()\n",
    "        labels = names\n",
    "        feature_imp = pd.DataFrame(columns=list(scaled_df.columns))\n",
    "\n",
    "        i = 0\n",
    "        sss = StratifiedShuffleSplit(n_splits=trials, test_size=0.1, random_state=2016)\n",
    "        for train_index, test_index in sss.split(df, labels):\n",
    "            X_train = df.iloc[train_index]\n",
    "            X_test = df.iloc[test_index]\n",
    "            y_train = labels.iloc[train_index]\n",
    "            y_test = labels.iloc[test_index]\n",
    "            k = i\n",
    "            sme = SMOTE(sampling_strategy=1, random_state=i, n_jobs=-1, k_neighbors=12)\n",
    "            X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "\n",
    "            # Create and Train\n",
    "            rfc=RandomForestClassifier(criterion='entropy', n_jobs=-1, random_state=i, n_estimators=100)\n",
    "\n",
    "            rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "            if first_frame:  # Initialize\n",
    "                first_frame = False  # Don't Come back Here\n",
    "\n",
    "                datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "\n",
    "                correctness_frame = pd.DataFrame(data=datadict)\n",
    "                correctness_frame['round'] = i\n",
    "\n",
    "                metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "                'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "                'Precision':precision_score(y_test, rfc.predict(X_test)), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "\n",
    "                metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()\n",
    "                metrics_frame['round'] = i\n",
    "                metrics_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "                if print_metrics == 1:\n",
    "                    print('Round:', i)\n",
    "                    print('Correctness Frame')\n",
    "                    print(correctness_frame)\n",
    "                    print('Metrics Frame')\n",
    "                    print(metrics_frame)\n",
    "\n",
    "\n",
    "            else:\n",
    "                datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "                revolve_frame = pd.DataFrame(data=datadict)\n",
    "                revolve_frame['round'] = i\n",
    "                correctness_frame = pd.concat([correctness_frame,revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "                metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "                'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "                'Precision':precision_score(y_test, rfc.predict(X_test)), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "                metrics_revolve_frame = pd.DataFrame.from_dict(data=metrics_dict, orient='index').transpose()\n",
    "                metrics_revolve_frame['round'] = i\n",
    "                metrics_revolve_frame['Threshold Power'] = thresh_power\n",
    "                metrics_frame = pd.concat([metrics_frame,metrics_revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "                if print_metrics == 1:\n",
    "                    print(\"Round:\", i)\n",
    "                    print(\"Correctness Frame\")\n",
    "                    print(revolve_frame)\n",
    "                    print('Metrics')\n",
    "                    print(metrics_revolve_frame)\n",
    "\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "            feature_imp.loc[i] = pd.Series(rfc.feature_importances_,index=list(df.columns))\n",
    "\n",
    "    plt.close(\"all\")\n",
    "    short_description = \"PsuedoAbundanceThresholding\" + str(round(j,3))\n",
    "    png_filepath = log_metrics_data(metrics_frame,short_description)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    total_metrics_df_melted = pd.melt(metrics_frame, id_vars=['Threshold Power', 'round'], value_vars=['Accuracy', 'AUC', 'Precision', 'Recall'],\n",
    "                                  var_name='Metric', value_name='Mean')\n",
    "    # total_metrics_df_melted.head()\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    ax = sns.lineplot(data=total_metrics_df_melted, x=\"Threshold Power\", y=\"Mean\", hue=\"Metric\", palette=['#4448FB', '#3DA5E3', \"#50FAD0\", '#3DE34E'])\n",
    "\n",
    "    ax.set_ylabel('Mean', fontsize=22)\n",
    "    ax.set_xlabel('Threshold Power', fontsize=22)\n",
    "    plt.savefig(png_filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save data with time, date and description\n",
    "plt.close(\"all\")\n",
    "short_description = \"FBS_int_raz\"\n",
    "png_filepath = log_metrics_data(metrics_frame,short_description)\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "total_metrics_df_melted = pd.melt(metrics_frame, id_vars=['Threshold Power', 'round'], value_vars=['Accuracy', 'AUC', 'Precision', 'Recall'],\n",
    "                                  var_name='Metric', value_name='Mean')\n",
    "# total_metrics_df_melted.head()\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = sns.lineplot(data=total_metrics_df_melted, x=\"Threshold Power\", y=\"Mean\", hue=\"Metric\", palette=['#4448FB', '#3DA5E3', \"#50FAD0\", '#3DE34E'])\n",
    "\n",
    "ax.set_ylabel('Mean', fontsize=22)\n",
    "ax.set_xlabel('Threshold Power', fontsize=22)\n",
    "plt.savefig(png_filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'interpro_scraping'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimblearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mover_sampling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Homemade functions required\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdata_prep_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PC_ML_using NetSurfP\\data_prep_functions.py:5\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01minterpro_scraping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m interpro_scraping_pandas\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mBio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSeqUtils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mProtParam\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ProteinAnalysis\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'interpro_scraping'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import  f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "\n",
    "# Homemade functions required\n",
    "from data_prep_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set Data files for prediction and directory for outputs and inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_dir=\"Input_data/\"\n",
    "out_dir=\"Output_data/\"\n",
    "proteom_dir=\"NetsurfP_Proteomes/\"\n",
    "\n",
    "# Protein data input in one excel spreadsheet with two sheets\n",
    "datfile='test1.xlsx'\n",
    "prot_prop = 'Protein Properties'\n",
    "mass_spec = 'Mass Sped Details'\n",
    "\n",
    "#netsurfp data file\n",
    "NSPfilePath=proteom_dir+'Bovine_Proteome.xlsx'\n",
    "\n",
    "\n",
    "#load data\n",
    "raw_prop_data=pd.read_excel(in_dir+datfile, sheet_name=prot_prop, thousands=',')\n",
    "raw_MS_data=pd.read_excel(in_dir+datfile, sheet_name=mass_spec, header=0)\n",
    "NSP_data=pd.read_excel(NSPfilePath)\n",
    "\n",
    "#clean up and calculate % protein abundance and enrichment, function found in data_prep_functions line 470\n",
    "\n",
    "MS_data_clean = clean_up_data_mass_spec(raw_MS_data)\n",
    "Accesions_IDs = MS_data_clean[\"Accession\"].to_frame()\n",
    "\n",
    "\n",
    "# clean up protein data, function found in data_prep_functions.py line 367\n",
    "\n",
    "PROT_cleaned_data = clean_up_data_biopy(raw_prop_data, Accesions_IDs) #calculates biopython features from protein sequences, and removes proteins removed during mass spec clean up\n",
    "\n",
    "PROT_cleaned_data = normalize_mass_length_1DF(PROT_cleaned_data) #function found in data_prep_functions line 167, normalizes mass, length and mw by dividing all values by the max in the column\n",
    "\n",
    "Protein_data_complete = pd.merge(PROT_cleaned_data, NSP_data, left_on='Entry', right_on='entry') #merges netsurfp features and biopython features\n",
    "\n",
    "\n",
    "#creates new column called asa_sum_normalized which is the asa_sum value divide by the mass of the protein\n",
    "for df in [Protein_data_complete]:\n",
    "    for col in ['asa_sum']:\n",
    "        df[col+'_normalized'] = df[col] / df['Mass']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Training Models and Classification\n",
    "there are three ways to classify in corona versus out of corona here. 1) landry classifer 2) split classifer and 3) psudeo abundance thresholding for 1 and 2 run graphing script at end, 3 has graphing already integrated\n",
    "anything that says ignore is left over code/useless code from when they use to run 2 serums at a time, needs to be further cleaned (only commented one classifer since they all share same code except for classification part"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Landry Classifer\n",
    "\n",
    "trials = 100 #number of models per threshold power\n",
    "first_frame = True #starting dataframe for saving metrics\n",
    "correctness_frame = pd.DataFrame()\n",
    "metrics_frame = pd.DataFrame()\n",
    "print_metrics = 0 #0, doesn't show metrics while runnning for each model, 1 does show metrics\n",
    "\n",
    "for thresh_power in np.arange(1,3.6,.25): #loop for training 100 models for each power\n",
    "    print('Power ', thresh_power)\n",
    "    fluids_type_list=['CMNP200'] #ignore\n",
    "    CMNP200_labels = MS_data_clean[['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']].copy() #makes copy of mass spec data\n",
    "    data_sheets = [CMNP200_labels]#ignore\n",
    "    #fit data to distribution, ignore for loop part\n",
    "    x = np.arange(0,1,.01)\n",
    "    for k in range(1):\n",
    "        sheet = data_sheets[k] #ignore\n",
    "        fluid_type = fluids_type_list[k]#ignore\n",
    "\n",
    "        print(f'\\n{fluid_type.capitalize()} running')\n",
    "\n",
    "        num_proteins = np.zeros(100)\n",
    "        for i in np.arange(0,100,1):\n",
    "            thresh = i/100\n",
    "            index = (sheet['NP_%_Abundance']>thresh)\n",
    "            num_proteins[i] = (np.count_nonzero(index))\n",
    "\n",
    "\n",
    "        num_proteins_above_1_abundance = min(num_proteins)\n",
    "        biexponent_dist = num_proteins - num_proteins_above_1_abundance\n",
    "        num_protein_thresh = np.max(biexponent_dist)/pow(np.e,thresh_power)\n",
    "        cutoff_thresh_value = min(x[(biexponent_dist<num_protein_thresh)&(biexponent_dist>0)], default=0)\n",
    "        #classifer\n",
    "        sheet['Corona'] = ((sheet['NP_%_Abundance']>cutoff_thresh_value) | (sheet['Enrichment']>1)).astype(int)\n",
    "        #percent of protein classifed as in corona\n",
    "        print(sheet['Corona'].sum()/sheet['Corona'].shape[0])\n",
    "    #drop unneeded columns\n",
    "    CMNP200_labels = CMNP200_labels.drop(['NP_%_Abundance', 'Enrichment', 'FBS Relative Abundance'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # function found in data prep functions.py line 827, if there are multiple accession numbers associated with single protein expands that into multiple row entries\n",
    "    CMNP200_labels = accession_expansion(CMNP200_labels)\n",
    "\n",
    "    #merge in/out of corona labels and features\n",
    "    CMNP200_total_data_names = pd.merge(CMNP200_labels, Protein_data_complete.copy(), left_on='Accession', right_on='Entry')\n",
    "    #drop unneeded columns\n",
    "    CMNP200_total_data_names =CMNP200_total_data_names.drop(['Accession','Entry', 'entry', 'Sequence', 'Length', 'Mass'], axis=1)\n",
    "\n",
    "    #make a copy of just features\n",
    "    features_CMNP200 = CMNP200_total_data_names.copy()\n",
    "    features_CMNP200 = features_CMNP200.drop(['Corona'], axis=1)\n",
    "    #make copy of just labels\n",
    "    names_CMNP200 = CMNP200_total_data_names['Corona'].copy()\n",
    "    #ignore\n",
    "    features_merged_naive = features_CMNP200\n",
    "    names_merged = names_CMNP200\n",
    "\n",
    "    #normalize dataset with min/max scaling\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    #make copy of features\n",
    "    total_data = features_merged_naive.copy()\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data = total_data.drop(['Protein names'], axis=1)\n",
    "\n",
    "    #fit scaler to data\n",
    "    scaler = scaler.fit(total_data)\n",
    "    scaled_df = pd.DataFrame(scaler.transform(total_data), columns=total_data.columns)\n",
    "\n",
    "    #ignore\n",
    "    names = names_merged.copy()\n",
    "\n",
    "\n",
    "    #ignore\n",
    "    df = scaled_df.copy()\n",
    "    labels = names\n",
    "    #make data frame for feature importances\n",
    "    feature_imp = pd.DataFrame(columns=list(scaled_df.columns))\n",
    "\n",
    "    i = 0\n",
    "    #split data into text and train n times\n",
    "    sss = StratifiedShuffleSplit(n_splits=trials, test_size=0.1, random_state=2016)\n",
    "    for train_index, test_index in sss.split(df, labels):\n",
    "        X_train = df.iloc[train_index]\n",
    "        X_test = df.iloc[test_index]\n",
    "        y_train = labels.iloc[train_index]\n",
    "        y_test = labels.iloc[test_index]\n",
    "        k = i\n",
    "        #syntheic oversampling\n",
    "        sme = SMOTE(sampling_strategy=1, random_state=i, n_jobs=-1, k_neighbors=12)\n",
    "        X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Create and Train\n",
    "        rfc=RandomForestClassifier(criterion='entropy', n_jobs=-1, random_state=i, n_estimators=100)\n",
    "        rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "\n",
    "        #everyting below is for saving meteric of each model trained\n",
    "        if first_frame:  # Initialize\n",
    "            first_frame = False  # Don't Come back Here\n",
    "\n",
    "            datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "\n",
    "            correctness_frame = pd.DataFrame(data=datadict)\n",
    "            correctness_frame['round'] = i\n",
    "\n",
    "            metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "            'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "            'Precision':precision_score(y_test, rfc.predict(X_test), zero_division=0), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "\n",
    "            metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()\n",
    "            metrics_frame['round'] = i\n",
    "            metrics_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "            if print_metrics == 1:\n",
    "                print('Round:', i)\n",
    "                print('Correctness Frame')\n",
    "                print(correctness_frame)\n",
    "                print('Metrics Frame')\n",
    "                print(metrics_frame)\n",
    "\n",
    "\n",
    "        else:\n",
    "            datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "            revolve_frame = pd.DataFrame(data=datadict)\n",
    "            revolve_frame['round'] = i\n",
    "            correctness_frame = pd.concat([correctness_frame,revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "            metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "            'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "            'Precision':precision_score(y_test, rfc.predict(X_test), zero_division=0), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "            metrics_revolve_frame = pd.DataFrame.from_dict(data=metrics_dict, orient='index').transpose()\n",
    "            metrics_revolve_frame['round'] = i\n",
    "            metrics_revolve_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "            metrics_frame = pd.concat([metrics_frame,metrics_revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "            if print_metrics == 1:\n",
    "                print(\"Round:\", i)\n",
    "                print(\"Correctness Frame\")\n",
    "                print(revolve_frame)\n",
    "                print('Metrics')\n",
    "                print(metrics_revolve_frame)\n",
    "\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "        feature_imp.loc[i] = pd.Series(rfc.feature_importances_,index=list(df.columns))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "## Split Classifer\n",
    "\n",
    "\n",
    "trials = 100\n",
    "first_frame = True\n",
    "correctness_frame = pd.DataFrame()\n",
    "metrics_frame = pd.DataFrame()\n",
    "print_metrics = 0\n",
    "\n",
    "for thresh_power in np.arange(1,3.6,.25):\n",
    "    print('Power ', thresh_power)\n",
    "    fluids_type_list=['CMNP200']\n",
    "    CMNP200_labels = MS_data_clean[['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']].copy()\n",
    "    data_sheets = [CMNP200_labels]\n",
    "    x = np.arange(0,1,.01)\n",
    "    for k in range(1):\n",
    "        sheet = data_sheets[k]\n",
    "        fluid_type = fluids_type_list[k]\n",
    "\n",
    "        print(f'\\n{fluid_type.capitalize()} running')\n",
    "\n",
    "        num_proteins = np.zeros(100)\n",
    "        for i in np.arange(0,100,1):\n",
    "            thresh = i/100\n",
    "            index = (sheet['NP_%_Abundance']>thresh)\n",
    "            num_proteins[i] = (np.count_nonzero(index))\n",
    "\n",
    "\n",
    "        num_proteins_above_1_abundance = min(num_proteins)\n",
    "        biexponent_dist = num_proteins - num_proteins_above_1_abundance\n",
    "        num_protein_thresh = np.max(biexponent_dist)/pow(np.e,thresh_power)\n",
    "        cutoff_thresh_value = min(x[(biexponent_dist<num_protein_thresh)&(biexponent_dist>0)], default=0)\n",
    "        psudeo_enriched_proteins = sheet.loc[sheet[\"FBS Relative Abundance\"] < 0.044, ['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']]\n",
    "        non_psudeo_enriched_proteins = sheet.loc[sheet[\"FBS Relative Abundance\"] > 0.044, ['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']]\n",
    "        psudeo_enriched_proteins['Corona'] = (psudeo_enriched_proteins['NP_%_Abundance']>cutoff_thresh_value).astype(int)\n",
    "        non_psudeo_enriched_proteins['Corona'] = ((non_psudeo_enriched_proteins['NP_%_Abundance']>cutoff_thresh_value) | (non_psudeo_enriched_proteins['Enrichment']>1)).astype(int)\n",
    "        psudeo_enriched_prot_labels = psudeo_enriched_proteins[[\"Accession\", \"Corona\"]]\n",
    "        non_psudeo_enriched_pro_labels = non_psudeo_enriched_proteins[[\"Accession\", \"Corona\"]]\n",
    "        remerged_protein_labels = pd.concat([non_psudeo_enriched_pro_labels,psudeo_enriched_prot_labels], ignore_index=True, axis=0)\n",
    "        sheet = pd.merge(sheet,remerged_protein_labels, how=\"inner\", on='Accession')\n",
    "        print(sheet['Corona'].sum()/sheet['Corona'].shape[0])\n",
    "        CMNP200_labels = sheet\n",
    "    CMNP200_labels = CMNP200_labels.drop(['NP_%_Abundance', 'Enrichment', 'FBS Relative Abundance'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # checks for any accession lists\n",
    "    CMNP200_labels = accession_expansion(CMNP200_labels)\n",
    "\n",
    "\n",
    "    CMNP200_total_data_names = pd.merge(CMNP200_labels, Protein_data_complete.copy(), left_on='Accession', right_on='Entry')\n",
    "\n",
    "    CMNP200_total_data_names =CMNP200_total_data_names.drop(['Accession','Entry', 'entry', 'Sequence', 'Length', 'Mass'], axis=1)\n",
    "\n",
    "\n",
    "    features_CMNP200 = CMNP200_total_data_names.copy()\n",
    "    features_CMNP200 = features_CMNP200.drop(['Corona'], axis=1)\n",
    "    names_CMNP200 = CMNP200_total_data_names['Corona'].copy()\n",
    "\n",
    "    #rename database to match combined database name\n",
    "\n",
    "    features_merged_naive = features_CMNP200\n",
    "    names_merged = names_CMNP200\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    total_data = features_merged_naive.copy()\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data = total_data.drop(['Protein names'], axis=1)\n",
    "    scaler = scaler.fit(total_data)\n",
    "    scaled_df = pd.DataFrame(scaler.transform(total_data), columns=total_data.columns)\n",
    "\n",
    "    names = names_merged.copy()\n",
    "\n",
    "\n",
    "\n",
    "    df = scaled_df.copy()\n",
    "    labels = names\n",
    "    feature_imp = pd.DataFrame(columns=list(scaled_df.columns))\n",
    "\n",
    "    i = 0\n",
    "    sss = StratifiedShuffleSplit(n_splits=trials, test_size=0.1, random_state=2016)\n",
    "    for train_index, test_index in sss.split(df, labels):\n",
    "        X_train = df.iloc[train_index]\n",
    "        X_test = df.iloc[test_index]\n",
    "        y_train = labels.iloc[train_index]\n",
    "        y_test = labels.iloc[test_index]\n",
    "        k = i\n",
    "        sme = SMOTE(sampling_strategy=1, random_state=i, n_jobs=-1, k_neighbors=12)\n",
    "        X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Create and Train\n",
    "        rfc=RandomForestClassifier(criterion='entropy', n_jobs=-1, random_state=i, n_estimators=100)\n",
    "\n",
    "        rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "        if first_frame:  # Initialize\n",
    "            first_frame = False  # Don't Come back Here\n",
    "\n",
    "            datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "\n",
    "            correctness_frame = pd.DataFrame(data=datadict)\n",
    "            correctness_frame['round'] = i\n",
    "\n",
    "            metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "            'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "            'Precision':precision_score(y_test, rfc.predict(X_test), zero_division=0), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "\n",
    "            metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()\n",
    "            metrics_frame['round'] = i\n",
    "            metrics_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "            if print_metrics == 1:\n",
    "                print('Round:', i)\n",
    "                print('Correctness Frame')\n",
    "                print(correctness_frame)\n",
    "                print('Metrics Frame')\n",
    "                print(metrics_frame)\n",
    "\n",
    "\n",
    "        else:\n",
    "            datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "            revolve_frame = pd.DataFrame(data=datadict)\n",
    "            revolve_frame['round'] = i\n",
    "            correctness_frame = pd.concat([correctness_frame,revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "            metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "            'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "            'Precision':precision_score(y_test, rfc.predict(X_test), zero_division=0), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "            metrics_revolve_frame = pd.DataFrame.from_dict(data=metrics_dict, orient='index').transpose()\n",
    "            metrics_revolve_frame['round'] = i\n",
    "            metrics_revolve_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "            metrics_frame = pd.concat([metrics_frame,metrics_revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "            if print_metrics == 1:\n",
    "                print(\"Round:\", i)\n",
    "                print(\"Correctness Frame\")\n",
    "                print(revolve_frame)\n",
    "                print('Metrics')\n",
    "                print(metrics_revolve_frame)\n",
    "\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "        feature_imp.loc[i] = pd.Series(rfc.feature_importances_,index=list(df.columns))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###psuedo abundance thresholding classifer\n",
    "\n",
    "trials = 100\n",
    "first_frame = True\n",
    "correctness_frame = pd.DataFrame()\n",
    "metrics_frame = pd.DataFrame()\n",
    "print_metrics = 0\n",
    "\n",
    "##generate thresholding list for psuedo abudances\n",
    "\n",
    "abudance_list = []\n",
    "temp_df = raw_MS_data.copy()\n",
    "temp_df.loc[temp_df['FBS Relative Abundance'] == 0.0431998478815122, 'FBS Relative Abundance'] = np.nan\n",
    "min_abundance = temp_df[\"FBS Relative Abundance\"].min()\n",
    "\n",
    "for i in range(1,9,1):\n",
    "    i = i*0.1\n",
    "    abudance_list.append(min_abundance*i)\n",
    "\n",
    "First_Pass = True\n",
    "y = 0\n",
    "\n",
    "for j in abudance_list:\n",
    "\n",
    "    if First_Pass == True:\n",
    "        temp_df['FBS Relative Abundance'] = temp_df['FBS Relative Abundance'].replace(np.nan, j)\n",
    "        CMNP200_mass_spec_data_clean = clean_up_data_mass_spec(temp_df)\n",
    "        First_Pass = False\n",
    "    else:\n",
    "        temp_df['FBS Relative Abundance'] = temp_df['FBS Relative Abundance'].replace(abudance_list[y], j)\n",
    "        CMNP200_mass_spec_data_clean = clean_up_data_mass_spec(temp_df)\n",
    "        y = y + 1\n",
    "\n",
    "    print('Abundance Threshold', round(j,3))\n",
    "\n",
    "    for thresh_power in np.arange(1,3.6,.25):\n",
    "        print('Power ', thresh_power)\n",
    "        fluids_type_list=['CMNP200']\n",
    "        CMNP200_labels = CMNP200_mass_spec_data_clean[['Accession', 'NP_%_Abundance','Enrichment','FBS Relative Abundance']].copy()\n",
    "        data_sheets = [CMNP200_labels]\n",
    "        x = np.arange(0,1,.01)\n",
    "        for k in range(1):\n",
    "            sheet = data_sheets[k]\n",
    "            fluid_type = fluids_type_list[k]\n",
    "\n",
    "            print(f'\\n{fluid_type.capitalize()} running')\n",
    "\n",
    "            num_proteins = np.zeros(100)\n",
    "            for i in np.arange(0,100,1):\n",
    "                thresh = i/100\n",
    "                index = (sheet['NP_%_Abundance']>thresh)\n",
    "                num_proteins[i] = (np.count_nonzero(index))\n",
    "\n",
    "\n",
    "            num_proteins_above_1_abundance = min(num_proteins)\n",
    "            biexponent_dist = num_proteins - num_proteins_above_1_abundance\n",
    "            num_protein_thresh = np.max(biexponent_dist)/pow(np.e,thresh_power)\n",
    "            cutoff_thresh_value = min(x[(biexponent_dist<num_protein_thresh)&(biexponent_dist>0)], default=0)\n",
    "            sheet['Corona'] = ((sheet['NP_%_Abundance']>cutoff_thresh_value) | (sheet['Enrichment']>1)).astype(int)\n",
    "            print(sheet['Corona'].sum()/sheet['Corona'].shape[0])\n",
    "        CMNP200_labels = CMNP200_labels.drop(['NP_%_Abundance', 'Enrichment', 'FBS Relative Abundance'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        # checks for any accession lists\n",
    "        CMNP200_labels = accession_expansion(CMNP200_labels)\n",
    "\n",
    "\n",
    "        CMNP200_total_data_names = pd.merge(CMNP200_labels, Protein_data_complete.copy(), left_on='Accession', right_on='Entry')\n",
    "\n",
    "        CMNP200_total_data_names =CMNP200_total_data_names.drop(['Accession','Entry', 'entry', 'Sequence', 'Length', 'Mass'], axis=1)\n",
    "\n",
    "\n",
    "        features_CMNP200 = CMNP200_total_data_names.copy()\n",
    "        features_CMNP200 = features_CMNP200.drop(['Corona'], axis=1)\n",
    "        names_CMNP200 = CMNP200_total_data_names['Corona'].copy()\n",
    "\n",
    "        #rename database to match combined database name\n",
    "\n",
    "        features_merged_naive = features_CMNP200\n",
    "        names_merged = names_CMNP200\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        total_data = features_merged_naive.copy()\n",
    "        total_data = total_data.fillna(0)\n",
    "        total_data = total_data.drop(['Protein names'], axis=1)\n",
    "        scaler = scaler.fit(total_data)\n",
    "        scaled_df = pd.DataFrame(scaler.transform(total_data), columns=total_data.columns)\n",
    "\n",
    "        names = names_merged.copy()\n",
    "\n",
    "\n",
    "\n",
    "        df = scaled_df.copy()\n",
    "        labels = names\n",
    "        feature_imp = pd.DataFrame(columns=list(scaled_df.columns))\n",
    "\n",
    "        i = 0\n",
    "        sss = StratifiedShuffleSplit(n_splits=trials, test_size=0.1, random_state=2016)\n",
    "        for train_index, test_index in sss.split(df, labels):\n",
    "            X_train = df.iloc[train_index]\n",
    "            X_test = df.iloc[test_index]\n",
    "            y_train = labels.iloc[train_index]\n",
    "            y_test = labels.iloc[test_index]\n",
    "            k = i\n",
    "            sme = SMOTE(sampling_strategy=1, random_state=i, n_jobs=-1, k_neighbors=12)\n",
    "            X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "\n",
    "            # Create and Train\n",
    "            rfc=RandomForestClassifier(criterion='entropy', n_jobs=-1, random_state=i, n_estimators=100)\n",
    "\n",
    "            rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "            if first_frame:  # Initialize\n",
    "                first_frame = False  # Don't Come back Here\n",
    "\n",
    "                datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "\n",
    "                correctness_frame = pd.DataFrame(data=datadict)\n",
    "                correctness_frame['round'] = i\n",
    "\n",
    "                metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "                'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "                'Precision':precision_score(y_test, rfc.predict(X_test)), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "\n",
    "                metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()\n",
    "                metrics_frame['round'] = i\n",
    "                metrics_frame['Threshold Power'] = thresh_power\n",
    "\n",
    "                if print_metrics == 1:\n",
    "                    print('Round:', i)\n",
    "                    print('Correctness Frame')\n",
    "                    print(correctness_frame)\n",
    "                    print('Metrics Frame')\n",
    "                    print(metrics_frame)\n",
    "\n",
    "\n",
    "            else:\n",
    "                datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "                revolve_frame = pd.DataFrame(data=datadict)\n",
    "                revolve_frame['round'] = i\n",
    "                correctness_frame = pd.concat([correctness_frame,revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "                metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "                'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)),\n",
    "                'Precision':precision_score(y_test, rfc.predict(X_test)), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "                metrics_revolve_frame = pd.DataFrame.from_dict(data=metrics_dict, orient='index').transpose()\n",
    "                metrics_revolve_frame['round'] = i\n",
    "                metrics_revolve_frame['Threshold Power'] = thresh_power\n",
    "                metrics_frame = pd.concat([metrics_frame,metrics_revolve_frame], ignore_index=True, axis=0)\n",
    "\n",
    "                if print_metrics == 1:\n",
    "                    print(\"Round:\", i)\n",
    "                    print(\"Correctness Frame\")\n",
    "                    print(revolve_frame)\n",
    "                    print('Metrics')\n",
    "                    print(metrics_revolve_frame)\n",
    "\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "            feature_imp.loc[i] = pd.Series(rfc.feature_importances_,index=list(df.columns))\n",
    "\n",
    "    plt.close(\"all\")\n",
    "    short_description = \"PsuedoAbundanceThresholding\" + str(round(j,3))\n",
    "    png_filepath = log_metrics_data(metrics_frame,short_description)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    total_metrics_df_melted = pd.melt(metrics_frame, id_vars=['Threshold Power', 'round'], value_vars=['Accuracy', 'AUC', 'Precision', 'Recall'],\n",
    "                                  var_name='Metric', value_name='Mean')\n",
    "    # total_metrics_df_melted.head()\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    ax = sns.lineplot(data=total_metrics_df_melted, x=\"Threshold Power\", y=\"Mean\", hue=\"Metric\", palette=['#4448FB', '#3DA5E3', \"#50FAD0\", '#3DE34E'])\n",
    "\n",
    "    ax.set_ylabel('Mean', fontsize=22)\n",
    "    ax.set_xlabel('Threshold Power', fontsize=22)\n",
    "    plt.savefig(png_filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save data with time, date and description\n",
    "plt.close(\"all\")\n",
    "short_description = \"FBS_int_raz\"\n",
    "png_filepath = log_metrics_data(metrics_frame,short_description)\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "total_metrics_df_melted = pd.melt(metrics_frame, id_vars=['Threshold Power', 'round'], value_vars=['Accuracy', 'AUC', 'Precision', 'Recall'],\n",
    "                                  var_name='Metric', value_name='Mean')\n",
    "# total_metrics_df_melted.head()\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = sns.lineplot(data=total_metrics_df_melted, x=\"Threshold Power\", y=\"Mean\", hue=\"Metric\", palette=['#4448FB', '#3DA5E3', \"#50FAD0\", '#3DE34E'])\n",
    "\n",
    "ax.set_ylabel('Mean', fontsize=22)\n",
    "ax.set_xlabel('Threshold Power', fontsize=22)\n",
    "plt.savefig(png_filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}